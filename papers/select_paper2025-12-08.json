[
  {
    "date": "2025-12-08",
    "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
    "authors": "JV Roig",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07497v1",
    "source": "arXiv",
    "abstract": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
    "title_zh": "大语言模型在代理场景中如何失败？对多种大语言模型在代理模拟中的成功与失败场景的定性分析",
    "abstract_zh": "我们研究了大型语言模型（LLMs）在作为具备工具使用能力的自主代理时的失效机制。基于Kamiwaza Agentic Merit Index（KAMI）v0.1基准测试，我们对三种代表性模型——Granite 4 Small、Llama 4 Maverick 和 DeepSeek V3.1——在文件系统操作、文本提取、CSV分析和SQL查询等场景下的900次执行轨迹进行了分析。与关注总体得分不同，我们采用细粒度的逐次行为分析，揭示实现多步骤工具执行成功的关键策略，以及反复出现的导致可靠性下降的失败模式。\n\n研究发现，模型规模本身并不能预测其作为代理的鲁棒性：在某些受不确定性驱动的任务中，Llama 4 Maverick（400B）的表现仅略优于Granite 4 Small（32B）；而DeepSeek V3.1之所以表现出更优的可靠性，主要归因于后训练阶段的强化学习，而非模型架构或规模优势。在各类模型中，我们识别出四种反复出现的失败原型：在缺乏依据的情况下过早采取行动、过度“热心”而用虚构实体替代缺失信息、易受干扰信息引发的上下文污染，以及在高负载下执行过程的脆弱性。这些模式凸显了对代理评估方法的迫切需求——应更加注重交互式接地、故障恢复行为以及环境感知适应能力。这表明，要实现可靠的企事业级部署，不仅需要更强的模型，更需通过有意识的训练与设计选择，强化验证机制、约束发现能力，并严格遵循真实数据源。"
  },
  {
    "date": "2025-12-08",
    "title": "Do Generalisation Results Generalise?",
    "authors": "Matteo Boglioni, Andrea Sgobbi, Gabriel Tavernini, Francesco Rita, Marius Mosbach, Tiago Pimentel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07832v1",
    "source": "arXiv",
    "abstract": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.",
    "title_zh": "泛化结果能泛化吗？",
    "abstract_zh": "大型语言模型（LLM）的分布外（OOD）泛化能力对其实际部署至关重要。然而，以往评估 LLM 泛化性能的研究通常仅关注单一的分布外数据集，这种方法可能无法准确衡量模型的真实能力，因为模型在实际部署时所遇到的数据分布变化远比单一数据集复杂得多。本文旨在探究 OOD 泛化结果是否具有可推广性。更具体地说，我们在微调过程中对模型在多个 OOD 测试集上的表现进行评估，并进一步分析这些测试集之间性能的偏相关性，同时控制域内性能的影响。这一方法使我们能够评估在排除域内性能影响后，不同 OOD 泛化表现之间的相关性。通过对 OLMo2 和 OPT 模型的分析，我们发现泛化结果并无普遍趋势：任意两个 OOD 测试集之间是否存在正相关或负相关，强烈依赖于所分析的具体模型。"
  },
  {
    "date": "2025-12-08",
    "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
    "authors": "Sijia Li, Yuchen Huang, Zifan Liu, Zijian Li, Jingjing fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07287v1",
    "source": "arXiv",
    "abstract": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
    "title_zh": "SIT-Graph：用于多轮智能体的状态集成工具图",
    "abstract_zh": "尽管智能体系统取得了令人瞩目的进展，多轮工具使用场景仍然充满挑战。这主要是因为用户意图是逐步明确的，且环境会随着每一次工具调用而动态演变。虽然复用过往经验是自然的行为，但当前的大语言模型（LLM）智能体要么将完整轨迹或预定义的子任务视为不可分割的整体单元，要么仅依赖工具之间的依赖关系，难以适应状态和信息随对话轮次不断演进的情况。在本文中，我们提出了一种**状态集成工具图**（State Integrated Tool Graph, SIT-Graph），通过利用部分重叠的经验来增强多轮工具使用能力。受人类决策过程中情景记忆与程序性记忆相结合的启发，SIT-Graph 从历史交互轨迹中同时捕捉两种关键信息：紧凑的状态表示（类似情景记忆的片段）以及工具间的依赖关系（类似程序性记忆的流程）。具体而言，我们首先基于累积的工具使用序列构建一个工具图，并在每条边中附加一个关于对话与工具历史的紧凑状态摘要，这些摘要可能影响下一步行动的选择。在推理阶段，SIT-Graph 实现了类人的平衡：当下一步决策需要回溯先前上下文时，智能体从相关边中检索存储的状态摘要，以此指导其行为；当步骤属于常规流程时，则直接遵循高置信度的工具依赖关系，无需显式回忆。在多个具有状态感知能力的多轮工具使用基准测试中，SIT-Graph 均显著优于现有的强大多记忆与图结构基线方法，在工具选择的鲁棒性以及经验迁移的有效性方面均表现出色。"
  },
  {
    "date": "2025-12-08",
    "title": "Property Testing of Computational Networks",
    "authors": "Artur Czumaj, Christian Sohler",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07577v1",
    "source": "arXiv",
    "abstract": "In this paper we initiate the study of \\emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\\frac23$) any network that is \\emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \\emph{$(ε,δ)$-far}, which means that one needs to change an $ε$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $δ$-fraction of inputs from the desired function (or any function with a given property). To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model.",
    "title_zh": "计算网络的属性测试",
    "abstract_zh": "在本文中，我们首次开展了对**作为计算设备的加权计算网络的性质测试**的研究。我们的目标是设计一种性质测试算法，该算法在给定一个具有权重 oracle 访问权限的计算网络时，能够以至少 $\\frac{2}{3}$ 的概率接受任何能够计算某个特定函数（或具有某种性质的函数）的网络，并以至少 $\\frac{2}{3}$ 的概率拒绝任何与该函数“相距甚远”的网络。我们对“距离”这一概念进行了参数化，要求拒绝那些处于 **$(ε,δ)$-远** 状态的网络，这意味着：必须修改网络描述中的 $ε$-比例部分，才能得到一个能计算出在至多 $δ$-比例输入上与目标函数不同的函数（或具有给定性质的任意函数）的网络。\n\n为了展示我们框架的有效性，我们以具有 ReLU 激活函数的简单神经布尔网络为例进行案例研究。其中一个重要成果是：对于这类网络，任何接近常数的函数都可以在与网络规模无关的查询复杂度下被测试。此外，我们还证明了，在将分布无关模型自然推广到我们设定的情形时，无法获得类似的结果；同样地，在一个相关的基础测试模型中，也无法实现这一结果。"
  },
  {
    "date": "2025-12-08",
    "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings",
    "authors": "Sebastian Sztwiertnia, Felix Friedrich, Kristian Kersting, Patrick Schramowski, Björn Deiseroth",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07522v1",
    "source": "arXiv",
    "abstract": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.",
    "title_zh": "LIME：通过语言学元数据嵌入提升大语言模型数据的效率",
    "abstract_zh": "预训练仅解码器语言模型依赖于海量高质量数据，然而此类数据的可用性正日益接近其极限。尽管元数据常被用于数据集的构建与筛选，但其作为直接训练信号的潜力仍鲜有探索。我们挑战这一现状，提出LIME（语言学元数据嵌入）方法，通过引入捕捉语法、语义及上下文特性的元数据来增强词元嵌入。LIME显著提升了预训练效率：在适应训练数据分布方面，速度最快可提升56%，同时仅增加0.01%的参数量，计算开销几乎可以忽略不计。除了效率提升外，LIME还优化了分词效果，显著增强了语言建模能力以及生成任务的表现。这些优势在不同模型规模（从5亿到20亿参数）下均保持稳定。此外，我们还开发了一种元数据偏移版本——LIME+1，能够引导词元生成。在已知下一个词元的先验元数据条件下，LIME+1可使推理性能最高提升38%，算术准确率最高提升35%。"
  },
  {
    "date": "2025-12-08",
    "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
    "authors": "Hua Yang, Alejandro Velasco, Sen Fang, Bowen Xu, Denys Poshyvanyk",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07814v1",
    "source": "arXiv",
    "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
    "title_zh": "通过训练动态理解代码模型中的隐私风险：一种因果方法",
    "abstract_zh": "代码大语言模型（LLM4Code）在显著提升开发者生产力的同时，也因依赖包含大量个人身份信息（PII）的开源代码库而引发隐私担忧。已有研究指出，商业模型可能重现敏感的PII，但现有工作大多将PII视为单一类别，忽视了不同类型PII之间存在的异质性风险。本文探究不同类型的PII在被LLM4Code学习和泄露的可能性上是否存在差异，以及这种关系是否具有因果性。我们的方法包括构建涵盖多种PII类型的语料数据集，对不同规模的代表性模型进行微调，在真实PII数据上计算训练动态，并建立结构化因果模型以估计可学习性对泄露的因果影响。结果表明，不同PII类型之间的泄露风险存在显著差异，且与训练动态密切相关：易于学习的实例（如IP地址）更易泄露，而较难学习的类型（如密钥和密码）泄露频率较低，模糊类型则表现出混合行为。本研究首次提供了泄露风险依赖于PII类型的因果证据，为开发面向类型和可学习性的LLM4Code防护机制提供了重要指导。"
  },
  {
    "date": "2025-12-08",
    "title": "Large Causal Models from Large Language Models",
    "authors": "Sridhar Mahadevan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07796v1",
    "source": "arXiv",
    "abstract": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.",
    "title_zh": "从大型语言模型中构建大型因果模型",
    "abstract_zh": "我们提出了一种构建大型因果模型（LCMs）的新范式，充分利用了当今大型语言模型（LLMs）所蕴含的巨大潜在能力。本文介绍了我们正在进行的实验，其核心是一个名为DEMOCRITUS（去中心化因果关系本体族提取系统，集成拓扑通用切片）的已实现系统，旨在从针对LLM的精心设计的文本查询中提取跨领域的信息，进而构建、组织并可视化大型因果模型。与传统以特定领域和假设为中心、依赖数值实验数据进行因果推断的方法不同，DEMOCRITUS在方法论上具有显著差异。该系统利用高质量的LLM来提出主题、生成因果问题，并从多种多样的领域中提取合理的因果陈述。其技术挑战在于：如何将这些孤立、碎片化、可能模糊甚至相互冲突的因果主张整合成一个连贯的整体，将其转化为关系型因果三元组，并嵌入到大型因果模型中。为应对这一挑战，我们开发了全新的范畴机器学习方法，尽管在本文中仅能简要概述，因本文重点更侧重于DEMOCRITUS系统的实现层面。我们详细描述了DEMOCRITUS的实现流程，包含六个模块，并分析其计算成本分布，以识别当前在扩展至更大规模模型时的主要性能瓶颈。此外，我们展示了DEMOCRITUS在考古学、生物学、气候变化、经济学、医学和技术等多个广泛领域中的应用成果。最后，我们讨论了当前DEMOCRITUS系统的局限性，并提出了未来扩展其功能的方向。"
  },
  {
    "date": "2025-12-08",
    "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration",
    "authors": "Jucheng Shen, Gaurav Sarkar, Yeonju Ro, Sharath Nittur Sridhar, Zhangyang Wang, Aditya Akella, Souvik Kundu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07173v1",
    "source": "arXiv",
    "abstract": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.",
    "title_zh": "通过一种无需训练的置信度感知校准提升基于扩散的大语言模型的吞吐量",
    "abstract_zh": "我们提出了一种名为CadLLM的无需训练的方法，用于加速基于扩散模型的大型语言模型（dLLMs）的推理吞吐量。我们首先研究了在不同块和步骤中token去掩码置信度的动态特性。基于这一观察，我们设计了一种轻量级自适应方法，根据未掩码token的平均置信度动态调整生成块大小、步长以及阈值。此外，通过动态利用词汇表的一个子集来调控采样范围，进一步降低了softmax计算开销。CadLLM是一种即插即用、与模型无关的方法，可兼容基于KV缓存的dLLMs。在四个主流任务上的大量实验表明，与当前最先进的基线相比，CadLLM在保持良好准确率的同时，最高可实现2.28倍的吞吐量提升。"
  },
  {
    "date": "2025-12-08",
    "title": "Reliable agent engineering should integrate machine-compatible organizational principles",
    "authors": "R. Patrick Xian, Garry A. Gabison, Ahmed Alaa, Christoph Riedl, Grigorios G. Chrysos",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07665v1",
    "source": "arXiv",
    "abstract": "As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.",
    "title_zh": "可靠的代理工程应整合机器可兼容的组织原则。",
    "abstract_zh": "随着基于大语言模型（LLMs）的AI代理在社会中日益普及，协调、控制、委派与问责等问题与它们的可靠性担忧交织在一起。为了围绕可靠操作设计和实施LLM代理，我们应考虑应用环境中的任务复杂性，在努力减少其局限性的同时，最大限度降低代理失败风险，并优化资源利用效率。高度运作的人类组织也曾面临类似的权衡挑战，这促使人们发展出基于证据的理论，以理解其运行策略。本文探讨了LLM代理与组织科学中相关框架之间的相似之处，重点分析组织的设计、扩展与管理如何为提升代理系统的可靠性提供启示。我们提出了三条关于AI代理工程的组织原则，以实现可靠性和有效性：在代理设计中平衡自主性与能力，在代理扩展中权衡资源约束与性能收益，在代理管理中协调内部与外部机制。我们的研究拓展了AI系统与社会系统在运作与治理原则之间的持续交流，旨在促进系统的深度融合与集成。"
  },
  {
    "date": "2025-12-08",
    "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds",
    "authors": "Shahar Lutati",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07631v1",
    "source": "arXiv",
    "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\",
    "title_zh": "代理能力问题：通过信息论界限预测可解性",
    "abstract_zh": "自主代理应在何时投入资源执行任务？我们提出了“代理能力问题”（Agent Capability Problem, ACP）这一框架，用于在资源受限条件下预测代理能否解决特定问题。与依赖经验启发式方法不同，ACP将问题求解建模为信息获取过程：代理需要获取 $\\Itotal$ 比特的信息才能识别出解决方案，而每一步行动可获得 $\\Istep$ 比特信息，代价为 $\\Cstep$，由此得出有效成本 $\\Ceff = (\\Itotal/\\Istep) \\cdot \\Cstep$，该指标可在搜索开始前准确预测资源需求。我们证明了 $\\Ceff$ 为预期成本的下界，并提供了紧致的概率上界。实验验证表明，ACP的预测结果与实际代理性能高度吻合，在持续控制搜索开销的同时，显著提升了效率，优于贪婪策略和随机策略。该框架适用于基于大语言模型（LLM）及各类代理工作流，通过统一的信息论视角，将主动学习、贝叶斯优化和强化学习中的核心原理有机联系起来。"
  },
  {
    "date": "2025-12-08",
    "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection",
    "authors": "Yuzhou Nie, Hongwei Li, Chengquan Guo, Ruizhe Jiang, Zhun Wang, Bo Li, Dawn Song, Wenbo Guo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07533v1",
    "source": "arXiv",
    "abstract": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.",
    "title_zh": "VulnLLM-R：基于智能体框架的专用推理大模型在漏洞检测中的应用",
    "abstract_zh": "我们提出了VulnLLM-R，这是首个专注于漏洞检测的推理型大语言模型。我们的核心洞察是：大语言模型能够对程序状态进行推理，并分析潜在漏洞，而不仅仅是进行简单的模式匹配。这种方法可以提升模型的泛化能力，避免学习到捷径。然而，当前最先进的推理型大语言模型通常规模巨大、为闭源，或在漏洞检测任务上表现有限。为此，我们提出了一种新颖的训练方法，包括专用数据选择、推理数据生成、推理数据过滤与修正，以及测试阶段优化。基于该方法，我们训练了一个拥有七亿参数的推理模型。在Python、C/C++和Java等多个主流SOTA数据集上的大量实验表明，VulnLLM-R在有效性和效率方面均优于现有的静态分析工具，以及各类开源和商业的大规模推理模型。我们还进行了详尽的消融实验，验证了训练方案中各项关键设计的有效性。最后，我们在模型基础上构建了一个智能体框架，并在真实项目中证明其性能超越CodeQL和AFL++。该智能体进一步在持续维护的代码仓库中发现了多个零日漏洞。本工作开创性地实现了基于专用推理模型驱动的AI智能体在真实世界、项目级漏洞检测中的应用。代码已公开于~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{GitHub}。"
  },
  {
    "date": "2025-12-08",
    "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models",
    "authors": "Haidong Kang, Jun Du, Lihong Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07419v1",
    "source": "arXiv",
    "abstract": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.",
    "title_zh": "通过大型语言模型实现无需训练的自动代理发现，革新混合精度量化",
    "abstract_zh": "混合精度量化（Mixed-Precision Quantization, MPQ）有效缓解了深度神经网络（DNNs）面临的内存溢出（Out-Of-Memory, OOM）瓶颈，因而受到越来越多研究者的关注。然而，传统方法要么依赖于代价高昂的可微分优化搜索，效率低下且缺乏灵活性；要么依赖人工专家设计的代理模型（如HAWQ）来学习量化后的DNN，不仅耗时费力，还高度依赖领域专业知识。我们能否在无需任何人工干预和训练的情况下设计出一个代理模型？本文给出了肯定的回答：我们提出了一种全新的、由大语言模型（Large Language Models, LLMs）驱动的无训练自动代理（Training-free Automatic Proxy，简称TAP）发现框架。该框架通过利用LLMs自动探索并生成针对MPQ任务的优质TAP，彻底革新了MPQ的代理设计范式。此外，为弥合黑箱LLMs与复杂MPQ任务之间的鸿沟，我们巧妙地引入基于直接策略优化（Direct Policy Optimization, DPO）的强化学习方法，通过优化提示词（prompts）来增强LLMs的推理能力，从而在LLM与MPQ任务之间建立起正向反馈循环，使LLM能够在后续迭代中生成更优的TAP。在主流基准上的大量实验表明，TAP实现了当前最优性能。最后，我们坚信，TAP将为MPQ领域带来深远影响，为LLM驱动的设计算法开辟全新视角。"
  },
  {
    "date": "2025-12-08",
    "title": "Challenges in Developing Secure Software -- Results of an Interview Study in the German Software Industry",
    "authors": "Alex R. Mattukat, Timo Langstrof, Horst Lichter",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07368v1",
    "source": "arXiv",
    "abstract": "The damage caused by cybercrime makes the development of secure software inevitable. Although many tools and frameworks exist to support the development of secure software, statistics on cybercrime show no improvement in recent years. To understand the challenges software companies face in developing secure software, we conducted an interview study with 19 industry experts from 12 cross-industry companies. The results of our study show that the challenges are mainly due to high complexity, a lack of security awareness, and unsuitable processes, which are further exacerbated by an immediate lack of skilled personnel. This article presents our study and the challenges we identified, and derives potential research directions from them.",
    "title_zh": "开发安全软件的挑战——德国软件行业访谈研究结果",
    "abstract_zh": "网络犯罪造成的损害使得开发安全软件成为不可避免的趋势。尽管已有许多工具和框架可用于支持安全软件的开发，但近年来网络犯罪的统计数据并未显示出明显改善。为了了解软件公司面临的安全软件开发挑战，我们对来自12家跨行业公司的19位行业专家进行了访谈研究。研究结果表明，这些挑战主要源于高复杂性、安全意识不足以及不合适的流程，而当前熟练人才的严重短缺进一步加剧了这些问题。本文介绍了我们的研究发现及所识别的挑战，并由此推导出潜在的研究方向。"
  },
  {
    "date": "2025-12-08",
    "title": "Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection",
    "authors": "Mengqi Wang, Jianwei Wang, Qing Liu, Xiwei Xu, Zhenchang Xing, Liming Zhu, Wenjie Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07246v1",
    "source": "arXiv",
    "abstract": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.",
    "title_zh": "用于可解释且鲁棒的错误检测的大型语言模型诱导决策树集成",
    "abstract_zh": "错误检测（Error Detection, ED）旨在识别表格数据中不正确或不一致的单元格值，对于保障数据质量至关重要。近年来，最先进的ED方法利用大型语言模型（LLMs）中嵌入的预训练知识和语义理解能力，直接对单元格是否出错进行标注。然而，这种“以LLM为标签器”的范式存在两个主要缺陷：（1）依赖于黑箱式的、隐式的决策过程，无法提供检测结果的可解释性；（2）对提示词（prompt）高度敏感，由于模型本身的随机性，容易产生不一致的输出，导致鲁棒性不足。\n\n为解决上述问题，我们提出一种“以LLM为诱导者”（LLM-as-an-inducer）的框架，该框架利用LLM来诱导构建用于ED的决策树（称为TreeED），并进一步集成多棵此类决策树以实现共识检测（称为ForestED），从而显著提升方法的可解释性与鲁棒性。具体而言，基于从数据上下文、决策树结构规范及输出要求中提取的提示信息，TreeED调用LLM生成决策树的骨架，其从根节点到叶节点的决策路径明确指出了评估给定样本的逐步推理流程。每棵决策树包含三类节点：（1）规则节点，执行简单的验证检查（如格式或取值范围）；（2）图神经网络（GNN）节点，用于捕捉复杂的数据模式（如函数依赖关系）；（3）叶节点，输出最终的判断结果（错误或正常）。\n\n此外，ForestED采用基于不确定性的采样策略，获取多个不同的行子集，并使用TreeED为每个子集构建一棵决策树。随后，通过一种基于期望最大化（Expectation-Maximization）的算法，联合估计各棵树的可靠性，并优化最终的共识式ED预测结果。\n\n大量实验表明，所提方法在准确性、可解释性和鲁棒性方面均表现优异，相较于最优基线方法，平均F1分数提升了16.1%。"
  },
  {
    "date": "2025-12-08",
    "title": "VIGIL: A Reflective Runtime for Self-Healing Agents",
    "authors": "Christopher Cruz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07094v1",
    "source": "arXiv",
    "abstract": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.",
    "title_zh": "VIGIL：一种用于自愈代理的反思式运行时",
    "abstract_zh": "代理型大语言模型框架承诺通过任务分解、工具使用和迭代规划实现自主行为，但大多数已部署的系统依然脆弱。它们缺乏运行时自我反思能力，无法诊断自身的失败模式，也无法在无人干预的情况下持续改进。实际上，许多代理架构退化为经过装饰的LLM调用链，缺乏保障可靠性的结构性机制。我们提出VIGIL（可验证的检查与受控迭代学习），一种具备反思能力的运行时系统，它监督一个同级代理，并执行自主维护而非直接的任务执行。VIGIL接收行为日志，将每个事件解析为结构化的“情感表征”，维护一个具有衰减机制和上下文策略的持久化EmoBank，并生成RBT诊断，将近期行为归类为优势、改进机会和失败点。基于此分析，VIGIL生成受保护的提示更新（保持核心身份语义不变），以及由策略引擎根据日志证据和代码热点生成的只读代码建议。VIGIL作为一个状态门控流水线运行：非法状态转换会触发明确错误，而非允许LLM随意发挥。在一个提醒延迟的案例研究中，VIGIL识别出延迟升高问题，提出了提示和代码修复方案；当其自身诊断工具因模式冲突失效时，它主动暴露内部错误，生成备用诊断，并输出修复计划。这展示了在实际部署的代理运行时中实现元层级自我修复的能力。"
  },
  {
    "date": "2025-12-08",
    "title": "Studying the Role of Reusing Crowdsourcing Knowledge in Software Development",
    "authors": "Rabe Abdalkareem",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07824v1",
    "source": "arXiv",
    "abstract": "Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered. Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.",
    "title_zh": "研究重用众包知识在软件开发中的作用",
    "abstract_zh": "众包平台（如 Stack Overflow）已深刻改变了软件开发实践。在这些平台上，开发者分享并重用彼此的软件开发与编程经验。因此，大量研究聚焦于软件工程中的众包现象，并表明，众包开发有助于提升开发者的生产效率、缩短上市时间。然而，在众包实践中，关于软件质量的实证研究仍显不足，一些基本问题尚未解答，例如：开发者究竟如何使用众包知识？为此，我们的研究重点在于探讨重用众包知识对软件项目的影响。为此，我们对若干知名的众包平台（包括 Stack Overflow 和 npm）开展了多项大规模实证研究。研究结果表明，从这些众包平台中重用知识确实能够助力软件开发实践，尤其体现在代码的复用方面。然而，这种知识重用也对软件质量带来了多方面的负面影响，例如增加项目依赖开销、提高维护成本。基于上述发现，我们利用所获得的知识，做出科学的数据驱动决策，进一步评估软件质量保障方法，以降低在软件开发中依赖众包知识所带来的风险。我们特别考察了持续集成（CI）的应用。分析表明，通过优化 CI 流程，可以有效提升开发者的生产力，同时节省其时间和资源。"
  },
  {
    "date": "2025-12-08",
    "title": "Provable Long-Range Benefits of Next-Token Prediction",
    "authors": "Xinyuan Cao, Santosh S. Vempala",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07818v1",
    "source": "arXiv",
    "abstract": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.",
    "title_zh": "下一令牌预测的可证明远期收益",
    "abstract_zh": "为什么现代语言模型在经过以“下一个词预测”为目标的训练后，仍能生成连贯的文档并捕捉到长距离结构？本文表明，对更长范围结构的学习而言，“下一个标记预测”任务具有可证明的强大能力，即使使用常见的神经网络架构也是如此。具体而言，我们证明：在循环神经网络（RNN）上优化下一个标记预测，将得到一个能够紧密逼近训练数据分布的模型。对于从训练分布中采样的未见文档，任何仅能观察接下来 $k$ 个标记、且描述长度有界的算法，都无法区分这些文档中的 $k$ 个连续标记与由学习到的语言模型在相同前缀下生成的 $k$ 个标记。我们给出了实现这种 $k$-标记不可区分性的模型规模的多项式界（关于 $k$ 的多项式，与文档长度无关），从而为实践中观察到的长距离连贯性提供了一个复杂性理论层面的解释。"
  },
  {
    "date": "2025-12-08",
    "title": "Automating High Energy Physics Data Analysis with LLM-Powered Agents",
    "authors": "Eli Gendreau-Distler, Joshua Ho, Dongwon Kim, Luc Tomas Le Pottier, Haichen Wang, Chengxi Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07785v1",
    "source": "arXiv",
    "abstract": "We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.",
    "title_zh": "利用大语言模型驱动的智能体实现高能物理数据自动分析",
    "abstract_zh": "我们提出了一项原理验证研究，展示了大型语言模型（LLM）代理在自动化高能物理（HEP）分析中的应用。以ATLAS开放数据中的希格斯玻色子双光子衰变截面测量为例，我们设计了一个混合系统，该系统结合了基于LLM的监督-编码代理与Snakemake工作流管理器。在此架构中，工作流管理器确保了分析过程的可复现性和确定性，而代理则能够自主生成、执行并根据用户指令迭代修正分析代码。我们定义了一系列定量评估指标，包括成功率、错误分布、每项特定任务的成本以及平均API调用次数，用以评估代理在多阶段工作流中的表现。为表征不同架构间的差异，我们对一系列前沿LLM进行了基准测试，涵盖Gemini和GPT-5系列、Claude家族以及主流开源权重模型。尽管工作流管理器保证了所有分析步骤的确定性执行，最终输出仍表现出一定的随机性。虽然我们将温度参数设为零，但其他采样参数（如top-p、top-k）仍保持默认值，且部分以推理为导向的模型会内部调整这些设置，因此模型无法产生完全确定的结果。本研究首次建立了基于LLM代理的自动化数据分析框架，应用于高能物理领域，实现了对模型能力、稳定性及局限性的系统性基准测试，适用于真实世界科学计算环境。本文所用的基础代码已公开于 https://huggingface.co/HWresearch/LLM4HEP。本工作已被NeurIPS 2025年“机器学习与物理科学”（ML4PS）研讨会接受为海报展示，初稿提交日期为2025年8月30日。"
  },
  {
    "date": "2025-12-08",
    "title": "Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model",
    "authors": "Yusei Ishimizu, Takuto Yamauchi, Sinan Chen, Jinyu Cai, Jialong Li, Kenji Tei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07261v1",
    "source": "arXiv",
    "abstract": "Discrete Controller Synthesis (DCS) is a powerful formal method for automatically generating specifications of discrete event systems. However, its practical adoption is often hindered by the highly specialized nature of formal models written in languages such as FSP and FLTL. In practice, syntax errors in modeling frequently become an important bottleneck for developers-not only disrupting the workflow and reducing productivity, but also diverting attention from higher-level semantic design. To this end, this paper presents an automated approach that leverages Large Language Models (LLMs) to repair syntax errors in DCS models using a well-designed, knowledge-informed prompting strategy. Specifically, the prompting is derived from a systematic empirical study of common error patterns, identified through expert interviews and student workshops. It equips the LLM with DCS-specific domain knowledge, including formal grammar rules and illustrative examples, to guide accurate corrections. To evaluate our method, we constructed a new benchmark by systematically injecting realistic syntax errors into validated DCS models. The quantitative evaluation demonstrates the high effectiveness of the proposed approach in terms of repair accuracy and its practical utility regarding time, achieving a speedup of 3.46 times compared to human developers. The experimental replication suite, including the benchmark and prompts, is available at https://github.com/Uuusay1432/DCSModelRepair.git",
    "title_zh": "基于大语言模型的离散控制器综合中的自动语法错误修复",
    "abstract_zh": "离散控制器综合（Discrete Controller Synthesis, DCS）是一种强大的形式化方法，可自动生成离散事件系统的规范。然而，其实际应用常因使用FSP和FLTL等语言编写的正式模型具有高度专业性而受到阻碍。在实践中，建模中的语法错误常常成为开发者的重大瓶颈——不仅打断工作流程、降低生产效率，还使开发者偏离更高层次的语义设计。为此，本文提出一种基于大型语言模型（LLM）的自动化方法，通过精心设计、融入领域知识的提示策略，修复DCS模型中的语法错误。具体而言，该提示策略源于对常见错误模式的系统性实证研究，这些模式通过专家访谈和学生工作坊被识别出来。该策略为LLM提供了DCS领域的特定知识，包括形式化语法规则和示范性示例，以引导准确修正。为评估本方法，我们通过在经过验证的DCS模型中系统性地注入真实存在的语法错误，构建了一个新的基准测试集。定量评估结果表明，所提方法在修复准确率方面表现出色，并在时间效率上展现出显著的实际价值，相较于人工开发者的修复速度提升了3.46倍。实验复现套件（包括基准测试数据和提示模板）已公开于 https://github.com/Uuusay1432/DCSModelRepair.git"
  },
  {
    "date": "2025-12-08",
    "title": "Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method",
    "authors": "Manthan Shenoy, Andreas Rausch",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07193v1",
    "source": "arXiv",
    "abstract": "This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.",
    "title_zh": "面向在混淆条件下基准设计模式检测：重现与评估基于注意力的检测方法",
    "abstract_zh": "本文研究了基于注意力机制的分类器在设计模式检测中的语义鲁棒性，特别关注其对结构语义和行为语义的依赖程度。我们复现了DPDAtt——一种基于学习型分类器的注意力机制设计模式检测方法，并在代码混淆条件下评估其性能。为此，我们构建了一个经过混淆的DPDAtt语料库版本，其中代码中的名称标识符（如类名、方法名等）以及字符串字面量（如打印语句、注释块等）被替换，同时保留了控制流、继承关系和逻辑结构。研究结果表明，DPDAtt中训练好的分类器严重依赖表面的语法特征，当这些线索通过混淆被移除时，会导致显著的误分类。本工作凸显了开发更具鲁棒性的检测工具的必要性，以真正捕捉源代码中的深层语义。我们提出所构建的混淆语料库（包含34个Java源文件）作为一个可重复使用的概念验证基准，用于评估当前最先进的设计模式检测工具在真实语义泛化能力方面的表现。"
  },
  {
    "date": "2025-12-08",
    "title": "aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \\& Software Formal Verification",
    "authors": "Noé Amiot, Quentin L. Meunier, Karine Heydemann, Emmanuelle Encrenaz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07520v1",
    "source": "arXiv",
    "abstract": "Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs",
    "title_zh": "aLEAKator：用于掩码硬件与软件形式化验证的HDL混合域仿真",
    "abstract_zh": "在高级泄露模型下，验证掩码化硬件与软件实现的安全性仍是一项重大挑战，尤其是在考虑毛刺、信号跳变以及CPU微架构细节的情况下。现有的验证方法要么仅限于小型硬件电路或CPU上的小型程序（如S盒），要么局限于有限的泄露模型，或者需要针对特定硬件的先验知识。本文提出aLEAKator，一个开源框架，用于从硬件描述语言（HDL）描述出发，对掩码化密码加速器及运行在CPU上的软件进行自动化形式化验证。我们的方法引入了混合域仿真技术，能够在多种（包括鲁棒型和宽松型）1探针泄露模型下实现精确建模与验证，并支持可变信号粒度，无需局限于1比特导线。aLEAKator还支持在存在查找表情况下的验证，且不依赖目标CPU架构的先验知识。该方法已通过与现有工具对比及真实世界测量数据进行了验证，并取得了创新成果，例如成功验证了在多种CPU上运行的完整首阶掩码AES算法。"
  },
  {
    "date": "2025-12-08",
    "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution",
    "authors": "Weilin Luo, Xueyi Liang, Haotian Deng, Yanan Liu, Hai Wan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07501v1",
    "source": "arXiv",
    "abstract": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.",
    "title_zh": "AutoICE：通过LLM驱动的进化自动生成可验证的C语言代码",
    "abstract_zh": "从自然语言需求自动生成可验证代码，能够确保软件的正确性和可靠性，同时显著降低采用形式化方法的技术门槛。随着大型语言模型（LLMs）的兴起，长期致力于自动形式化的工作迎来了新的发展动力。然而，现有方法由于缺乏领域特定的预训练语料库，常存在严重的语法和语义错误，且难以有效形式化隐含知识。本文提出AutoICE，一种基于大语言模型驱动的进化搜索方法，用于合成可验证的C语言代码。该方法引入多样化的个体初始化与协同交叉机制，实现多样性的迭代更新，从而缓解单智能体迭代中固有的误差传播问题。此外，通过自省式变异机制，有效促进对隐含知识的发现。实验结果表明，AutoICE具有显著成效：在标准测试集上成功验证了90.36%的代码，优于当前最先进的方法；在面向开发者的数据集变体上，其验证成功率高达88.33%，远超SOTA方法65%的成功率。"
  },
  {
    "date": "2025-12-08",
    "title": "Training Language Models to Use Prolog as a Tool",
    "authors": "Niklas Mellgren, Peter Schneider-Kamp, Lukas Galke Poech",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07407v1",
    "source": "arXiv",
    "abstract": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",
    "title_zh": "训练语言模型以使用Prolog作为工具",
    "abstract_zh": "确保可靠的工具使用对于安全的智能体式AI系统至关重要。语言模型经常产生看似合理但实际错误的推理结果，且这些结果难以验证。为解决这一问题，我们研究了通过微调模型以使用Prolog作为外部工具进行可验证计算的方法。采用组相对策略优化（GRPO），我们在清理后的GSM8K-Prolog-Prover数据集上对Qwen2.5-3B-Instruct模型进行微调，并在以下三个方面进行变化：(i) 提示结构，(ii) 奖励构成（执行、语法、语义、结构），以及(iii) 推理协议：单次生成、最佳N选一，以及两种智能体模式——Prolog被内部调用或独立调用。我们的强化学习方法优于监督微调，我们的3B模型在零样本MMLU任务上的表现达到了7B模型在少样本情况下的水平。研究发现表明：1）提示、奖励与推理机制的联合调优能够显著影响程序的语法和逻辑结构；2）采用外部Prolog验证的最佳N选一推理策略在GSM8K任务上实现了最高的准确率；3）在内部修复机制支持下的智能体推理模式，在MMLU-Stem和MMLU-Pro任务上展现出更优的零样本泛化能力。这些结果表明，将模型推理过程建立在形式化验证系统之上，能显著提升其可靠性与可审计性，适用于安全关键型应用场景。实验复现的源代码已公开于 https://github.com/niklasmellgren/grpo-prolog-inference"
  },
  {
    "date": "2025-12-08",
    "title": "Do LLMs Trust the Code They Write?",
    "authors": "Francisco Ribeiro, Claudio Spiess, Prem Devanbu, Sarah Nadi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07404v1",
    "source": "arXiv",
    "abstract": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
    "title_zh": "大语言模型会信任自己写的代码吗？",
    "abstract_zh": "尽管大型语言模型（LLMs）在代码生成方面表现出色，但它们常常生成错误的代码。其中一个原因是模型输出的概率与代码正确性往往缺乏良好相关性，仅反映了生成过程的最终结果。受启发于发现LLMs内部会编码诸如“真实性”等概念，本文探讨了LLMs是否同样内含代码正确性的表征。具体而言，我们通过对比同一编程任务中正确与错误代码对应的隐藏状态，识别出LLMs内部存在的正确性表征。在四个LLM上的实验表明，利用这一提取出的正确性表征，其性能优于标准的对数似然排序方法以及模型自述的信心评分。此外，我们还探索了如何利用这种内部正确性信号来选择质量更高的代码样本，而无需实际执行测试。最终，本研究展示了如何通过挖掘模型内部表征来提升代码生成系统，增强LLMs的可靠性，从而提高对自动生成代码的信任度。"
  },
  {
    "date": "2025-12-08",
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "authors": "Tong Wu, Yang Liu, Jun Bai, Zixia Jia, Shuyi Zhang, Ziyong Lin, Yanting Wang, Song-Chun Zhu, Zilong Zheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07461v1",
    "source": "arXiv",
    "abstract": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
    "title_zh": "原生并行推理者：通过自蒸馏强化学习实现并行推理",
    "abstract_zh": "我们提出原生并行推理器（Native Parallel Reasoner, NPR），这是一种无需教师模型的框架，使大型语言模型（LLMs）能够自主演化出真正的并行推理能力。NPR通过三项关键创新，将模型从传统的顺序模拟转变为原生的并行认知：1）一种自蒸馏的渐进式训练范式，实现了从“冷启动”格式发现到严格拓扑约束的过渡，全程无需外部监督；2）一种新颖的并行感知策略优化（Parallel-Aware Policy Optimization, PAPO）算法，能够在执行图内部直接优化分支策略，使模型通过试错学习自适应的任务分解；3）一个强大的NPR引擎，重构了SGLang的内存管理和流程控制机制，从而支持稳定、大规模的并行强化学习训练。在八个推理基准测试中，基于Qwen3-4B训练的NPR实现了最高达24.5%的性能提升，以及最高4.6倍的推理加速。与以往依赖自回归解码的基线方法不同，NPR展现出100%真正意义上的并行执行，为自我演进、高效且可扩展的智能体推理树立了新标准。"
  },
  {
    "date": "2025-12-08",
    "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning",
    "authors": "Nearchos Potamitis, Lars Klein, Akhil Arora",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07795v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .",
    "title_zh": "ReasonBENCH：评估大语言模型推理（不）稳定性的基准测试",
    "abstract_zh": "大型语言模型（LLMs）在需要推理能力的场景中日益广泛应用，例如多步问题求解和思维链（chain-of-thought）推理。然而，当前的评估实践普遍仅报告单次运行的准确率，忽略了由随机解码自然引发的内在不确定性。这种忽略造成了一个盲点：从业者无法可靠判断某种方法所报告的性能是否稳定、可复现或成本一致。为此，我们提出了 ReasonBENCH——首个旨在量化 LLM 推理内在不稳定的基准测试。ReasonBENCH 提供了三项核心功能：(i) 一个模块化的评估库，用于标准化推理框架、模型与任务；(ii) 多次运行协议，能够报告质量与成本方面具有统计可靠性的指标；(iii) 公开排行榜，以鼓励对方差敏感的报告方式。在来自不同领域的多个任务上，我们发现绝大多数推理策略和模型均表现出高度不稳定性。值得注意的是，即使平均性能相近的策略，其置信区间也可能相差高达四倍；而表现最佳的方法往往伴随着更高且更不稳定的成本。这种不稳定性严重损害了不同运行之间的可复现性，进而影响了所报告性能的可靠性。为进一步理解这些动态特性，我们还深入分析了提示设计、模型家族以及模型规模对求解率与稳定性之间权衡的影响。我们的研究结果强调了可复现性作为可靠 LLM 推理的关键维度，并为未来推理方法及不确定性量化技术的发展奠定了基础。ReasonBENCH 已公开发布，访问地址为 https://github.com/au-clan/ReasonBench。"
  },
  {
    "date": "2025-12-08",
    "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding",
    "authors": "Zeqi Chen, Zhaoyang Chu, Yi Gui, Feng Guo, Yao Wan, Chuan Shi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07666v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.",
    "title_zh": "代码图与大型语言模型的融合以实现更优的代码理解",
    "abstract_zh": "大型语言模型（LLMs）在代码智能任务中表现出色，如代码生成、摘要和翻译。然而，它们依赖于线性化的标记序列，限制了对程序结构语义的理解能力。尽管先前的研究探索了图增强提示和结构感知预训练方法，但这些方法要么受提示长度的限制，要么需要针对特定任务进行架构修改，这与大规模指令跟随型LLM不兼容。为解决上述局限性，本文提出了一种新颖的即插即用方法——CGBridge，通过一个外部可训练的“桥接”模块，将代码图信息引入LLM中以增强其理解能力。CGBridge首先在一个包含27万代码图的大规模数据集上，通过自监督学习预训练一个代码图编码器，以学习代码的结构语义；接着，训练一个外部模块，利用跨模态注意力机制对齐代码、图和文本之间的语义，弥合模态差异；最后，该桥接模块生成富含结构信息的提示，并注入到冻结的LLM中，同时针对下游代码智能任务进行微调。实验结果表明，CGBridge在原始模型及图增强提示方法的基础上均取得了显著提升：在代码摘要任务中，LLM作为评判者（LLM-as-a-Judge）的性能分别提升了16.19%和9.12%；在代码翻译任务中，执行准确率（Execution Accuracy）分别提高了9.84%和38.87%。此外，CGBridge的推理速度比LoRA微调模型快逾4倍，充分展示了其在结构感知代码理解方面的高效性与有效性。"
  },
  {
    "date": "2025-12-08",
    "title": "PCMind-2.1-Kaiyuan-2B Technical Report",
    "authors": "Kairong Luo, Zhenbo Sun, Xinyu Shi, Shengqi Chen, Bowen Yu, Yunyi Chen, Chenyi Dang, Hengtao Tao, Hui Wang, Fangming Liu, Kaifeng Lyu, Wenguang Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07612v1",
    "source": "arXiv",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.",
    "title_zh": "PCMind-2.1-Kaiyuan-2B 技术报告",
    "abstract_zh": "大型语言模型（LLMs）的迅猛发展，导致开源社区与产业界之间出现了显著的知识鸿沟，主要原因在于产业界依赖封闭源代码的高质量数据和训练方案。为解决这一问题，我们推出了PCMind-2.1-Kaiyuan-2B——一个完全开源的20亿参数模型，专注于在资源受限条件下提升训练效率与效果。我们的方法包含三项关键创新：一种分位数数据基准评估方法，可系统性地比较异构的开源数据集，并为数据混合策略提供洞察；一种基于多阶段范式的战略选择性重复机制，以高效利用稀疏但高质量的数据；以及一种多领域课程训练策略，按数据质量对样本进行排序。得益于高度优化的数据预处理流程及针对FP16精度稳定性的架构改进，Kaiyuan-2B在性能上达到与当前顶尖全开源模型相当的水平，展示了适用于资源受限场景的实用且可扩展的预训练解决方案。我们已将所有相关资产（包括模型权重、数据和代码）以Apache 2.0许可证发布，可在 https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B 获取。"
  },
  {
    "date": "2025-12-08",
    "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations",
    "authors": "Liping Han, Tingting Nie, Le Yu, Mingzhe Hu, Tao Yue",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07122v1",
    "source": "arXiv",
    "abstract": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.",
    "title_zh": "RisConFix：基于大模型的无人机风险配置自动修复",
    "abstract_zh": "飞行控制软件通常设计有大量可配置参数，用于管理多种功能，从而实现对任务多样性和环境不确定性的灵活适应。尽管开发者和制造商通常会为这些参数提供推荐值以确保安全稳定的运行，但某些采用推荐值的参数组合仍可能导致飞行行为不稳定，进而降低无人机的鲁棒性。为此，我们提出一种基于大语言模型（LLM）的实时风险配置修复方法（命名为RisConFix），用于修复那些削弱无人机鲁棒性的高风险配置。RisConFix持续监控无人机的运行状态，一旦检测到异常飞行行为，便会自动触发修复机制。该修复机制利用大语言模型分析配置参数与飞行状态之间的关系，并生成修正后的参数更新，以恢复飞行稳定性。为确保更新后配置的有效性，RisConFix采用迭代式工作流程：持续监测无人机的飞行状态，若在应用一次更新后异常仍存在，则自动启动下一轮修复循环。我们在ArduPilot系统上开展了一项案例研究（包含1,421组错误配置），实验结果表明，RisConFix实现了最高97%的修复成功率，平均修复次数仅为1.17次，充分证明了其在实时场景下高效、有效修复高风险配置的能力。"
  },
  {
    "date": "2025-12-08",
    "title": "PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs",
    "authors": "Håvard Rognebakke Krogstie, Helge Bahmann, Magnus Själander, Nico Reissmann",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.07299v1",
    "source": "arXiv",
    "abstract": "Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.",
    "title_zh": "PIP：使安德森的指针分析在不完整C程序中既可靠又实用",
    "abstract_zh": "单独编译文件有利于并行化，但会迫使编译器在不完整程序上进行操作。当前最先进的指针分析方法仅对完整程序能保证结果的正确性，因此需要使用摘要函数来描述程序中缺失的部分。然而，在实际的生产级编译器中，摘要函数很少可用，因为正确性和效率是不可妥协的。本文提出了一种安德森风格（Andersen-style）的指针分析方法，能够高效地为不完整的C程序生成正确的分析结果。该分析通过追踪从外部模块可访问的内存位置和指针来实现正确性，并通过在约束图中隐式地执行这种追踪来提升效率。我们证明，隐式指向对象追踪使约束求解器的速度比任何五种不同先进技术组合所采用的显式指向对象追踪快15倍。此外，我们还提出了“优先隐式指向对象”（Prefer Implicit Pointees, PIP）技术，进一步减少了对显式指向对象的依赖。与未使用PIP的最快求解器配置相比，PIP带来了额外1.9倍的性能提升。在以别名分析客户端为评估基准时，该分析的精度也得到了验证：相比仅使用LLVM的BasicAA传递，其将MayAlias响应的数量减少了40%。最后，我们展示了该分析在内存使用上的可扩展性，使其在实际优化编译器中具有应用价值。"
  }
]