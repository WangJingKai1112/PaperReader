[
  {
    "date": "2025-11-20",
    "title": "Challenges to JavaScript obfuscation in the era of large language models",
    "authors": "Daniel Ciochiu, Marius Brezovan, Nicolae Enescu, Claudiu Traistaru, Mircea Badoi, Alexandra Barcan",
    "publish": "2025 29th International Conference on System Theory, Control and Computing (ICSTCC)",
    "url": "https://doi.org/10.1109/icstcc66753.2025.11240358",
    "source": "IEEE",
    "abstract": "The growing reliance on web-based applications and their deployment across diverse domains have intensified concerns about the security of JavaScript source code executed on the client side. This study evaluates the resilience of a bundled, obfuscated application — implemented across multiple frontend frameworks — against deobfuscation by advanced Large Language Models (LLMs) including ChatGPT, Claude, Gemini, Le Chat, and DeepSeek. The analysis was conducted using both publicly available web interfaces and integrated development environment (IDE) assistants. Two complementary evaluation methods were employed: a qualitative assessment based on manual inspection, and a quantitative analysis using code similarity metrics. The results reveal a critical insight—contemporary JavaScript obfuscation techniques are increasingly vulnerable to the capabilities of modern LLMs, highlighting the urgent need for more robust client-side code protection strategies.",
    "title_zh": "大型语言模型时代对JavaScript混淆技术的挑战",
    "abstract_zh": "随着对基于Web的应用程序的依赖日益增加，以及这些应用在多样化领域中的广泛部署，客户端执行的JavaScript源代码安全问题愈发受到关注。本研究评估了一种经过打包和混淆处理的应用程序——该程序在多个前端框架中实现——面对先进大型语言模型（LLMs）如ChatGPT、Claude、Gemini、Le Chat和DeepSeek进行反混淆攻击时的抗性。分析通过公开的网络界面及集成开发环境（IDE）助手两种方式展开。研究采用了两种互补的评估方法：基于人工检查的定性分析，以及基于代码相似性度量的定量分析。结果揭示了一个关键发现：当前的JavaScript混淆技术正越来越容易被现代大型语言模型破解，凸显了亟需采用更强大的客户端代码保护策略。"
  },
  {
    "date": "2025-11-20",
    "title": "COPA: A Congestion-Oriented Pin Assignment Framework for Intra-Block Physical Design Optimization",
    "authors": "Shu-Yi Tsai, Yu-Guang Chen, Kun-Min Chen, Sheng-Bing Ke, Chung-Hui Hsieh, Chih-Wei Lin, Mango Chia-Tso Chao",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240661",
    "source": "IEEE",
    "abstract": "In hierarchical integrated circuit (IC) designs, pin assignment plays a critical role in determining routing quality and overall layout efficiency. While conventional approaches primarily focus on wirelength minimization, the impact of feedthrough pin assignment on internal routing congestion has been relatively overlooked. Our observation reveals that proper assignment of feedthrough pins can significantly reduce routing congestion without increasing wirelength. In this paper, we present COPA, a Congestion-Oriented Pin Assignment framework that explicitly targets congestion mitigation within intra-block routing regions. By formulating the feedthrough pin assignment problem as a nonlinear optimization task, COPA employs a gradient-based algorithm that iteratively adjusts pin positions based on congestion severity, quantified through a routing-demand-driven metric. Experimental results on industrial testcases demonstrate that COPA achieves an average 7.6% reduction in total overflow, with a maximum improvement of up to 9%, compared to initial feedthrough pin assignments generated by a commercial EDA tool.",
    "title_zh": "COPA：一种面向块内物理设计优化的拥塞导向引脚分配框架",
    "abstract_zh": "在层次化集成电路（IC）设计中，引脚分配对布线质量与整体布局效率起着至关重要的作用。尽管传统方法主要关注于缩短线长，但馈通引脚分配对内部布线拥塞的影响却相对被忽视。我们的观察表明，合理分配馈通引脚可显著降低布线拥塞，同时无需增加线长。本文提出了一种名为COPA（Congestion-Oriented Pin Assignment，面向拥塞优化的引脚分配）的框架，专门针对块内布线区域内的拥塞问题进行缓解。COPA将馈通引脚分配问题建模为一个非线性优化任务，并采用基于梯度的算法，根据拥塞严重程度（通过一种由布线需求驱动的度量指标量化）迭代调整引脚位置。在工业级测试用例上的实验结果表明，与商用EDA工具生成的初始馈通引脚分配相比，COPA平均实现了7.6%的总溢出减少，最大改善可达9%。"
  },
  {
    "date": "2025-11-20",
    "title": "A Parallel Analytical Legalization Algorithm via Alternating Direction Method of Multipliers",
    "authors": "Jaekyung Im, Seokhyeong Kang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240741",
    "source": "IEEE",
    "abstract": "Legalization tries to resolve the cell overlaps and align every cells to the placement sites while honoring the global placement results. The existing legalization works are mostly relying on heuristical cell-by-cell search within a window and this makes large suboptimality in their algorithm. Only a few works have attempted to solve the legalization problem through analytical method, but they also suffered huge runtime overhead and suboptimality due to the discrete nature of legalization. In this paper, we revisit the classical legalization problem and propose a new parallel analytical legalization method based on alternating direction method of multipliers (ADMM) with heterogeneous CPU-GPU parallelism. Experimental results show that our method significantly improves the solution quality compared to existing open-source legalizers.",
    "title_zh": "基于交替方向乘子法的并行解析合法化算法",
    "abstract_zh": "legalization 试图解决单元重叠问题，将每个单元对齐到布局位置，同时保留全局布局结果。现有的 legalization 方法大多依赖于在窗口内进行启发式的逐单元搜索，这导致算法存在较大的次优性。仅有少数工作尝试通过解析方法解决 legalization 问题，但它们也因 legalization 的离散特性而面临巨大的运行时间开销和次优性。本文重新审视了经典的 legalization 问题，提出了一种基于交替方向乘子法（ADMM）的新并行解析 legalization 方法，并采用异构 CPU-GPU 并行计算架构。实验结果表明，与现有的开源 legalizer 相比，我们的方法显著提升了求解质量。"
  },
  {
    "date": "2025-11-20",
    "title": "LLM4Verilog: Building Large-Scale, High-Quality Data Infrastructure for Verilog Code Generation via Community Efforts",
    "authors": "Zhongzhi Yu, Chaojian Li, Yongan Zhang, Mingjie Liu, Nathaniel Pinckney, Wenfei Zhou, Rongjian Liang, Haoyu Yang, Haoxing Ren, Yingyan Celine Lin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240644",
    "source": "IEEE",
    "abstract": "Despite recent advancements in code generation with large language models (LLMs), generating hardware code such as Verilog remains a significant challenge due to the scarcity of large-scale, high-quality datasets in the hardware domain. Existing approaches, including scraping open-source repositories and relying on manually curated datasets, often suffer from limited diversity, quality, and scalability. To address these limitations, we introduce LLM4Verilog, an exploratory, collaborative initiative aimed at constructing a large-scale, high-quality, open-source Verilog dataset. Our initiative integrates a community-driven data collection pipeline with a two-stage data filtering technique to ensure high dataset quality. The first stage removes duplicates and low-quality samples, resulting in a large-scale dataset called LLM4Verilog-complete. The second stage applies an LLM-driven quality scoring method, VeriScore, to perform fine-grained filtering and produce a high-quality, ready-to-use dataset called LLM4Verilog-filtered. We evaluate the effectiveness of these datasets through fine-tuning three different LLMs on our dataset, achieving 6.6%~11.2% and 5.3%~13.2% improvements in pass@1 scores on VerilogEval-human and VerilogEval-Machine, respectively, compared to models fine-tuned with prior state-of-the-art datasets. Notably, these improvements are achieved without relying on complex fine-tuning or data augmentation techniques, highlighting our dataset’s strong potential as a foundational resource for enhancing LLMs’ Verilog code generation capabilities. For more information about our initiative and resulting dataset, please refer to https://nvlabs.github.io/LLM4HWDesign/.",
    "title_zh": "LLM4Verilog：通过社区协作构建大规模高质量的Verilog代码生成数据基础设施",
    "abstract_zh": "尽管大型语言模型（LLMs）在代码生成方面取得了显著进展，但由于硬件领域缺乏大规模、高质量的数据集，生成硬件代码（如Verilog）仍面临重大挑战。现有的方法，包括从开源仓库中抓取数据或依赖人工整理的数据集，通常存在多样性不足、质量参差以及可扩展性差等问题。为解决这些局限性，我们推出了 LLM4Verilog——一项探索性、协作性的倡议，旨在构建一个大规模、高质量、开源的Verilog数据集。该倡议结合了社区驱动的数据收集流程与两阶段数据过滤技术，以确保数据集的高质量。\n\n第一阶段通过去除重复项和低质量样本，生成一个大规模数据集，命名为 LLM4Verilog-complete。第二阶段采用基于大语言模型的评分机制——VeriScore，进行细粒度筛选，最终产出一个高质量、可直接使用的数据集，称为 LLM4Verilog-filtered。\n\n我们在三个不同的LLM上对这两个数据集进行了微调评估，结果表明：相较于使用先前最先进的数据集进行微调的模型，我们的模型在 VerilogEval-human 和 VerilogEval-Machine 上的 pass@1 分数分别提升了 6.6%~11.2% 和 5.3%~13.2%。值得注意的是，这些性能提升并未依赖复杂的微调策略或数据增强技术，充分体现了本数据集作为基础资源，在提升LLMs生成Verilog代码能力方面的巨大潜力。\n\n有关本倡议及所生成数据集的更多信息，请访问：https://nvlabs.github.io/LLM4HWDesign/"
  },
  {
    "date": "2025-11-20",
    "title": "The Curious Case of Global Stable Loads",
    "authors": "Shagnik Pal, Jeeho Ryoo, Lizy K. John",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00022",
    "source": "IEEE",
    "abstract": "A recent study presented an interesting statistic that about one-third and up to two-thirds of the load instructions in many x86 programs are redundant. These redundant loads fetch the same value from the same address throughout the execution of the program, and are called global stable loads (GSLs). We explore the behavior of GSLs across different architectures, compilers, and compiler optimizations. We also study GSLs in emerging ML workloads that are not explored in prior work. Our characterization of SPEC programs reveal a high locality in GSLs, that only 4.2-5.2% of static GSLs are responsible for 90-95% of the dynamic GSLs. We identify the reasons behind these dominant static GSLs, and posit that many of these GSLs are identifiable by the compiler prior to execution.In contrast to using a purely microarchitectural solution to eliminate GSLs, we consider a compiler-hardware co-designed solution wherein the compiler marks the likely stable loads, and the microarchitecture uses a small and fast cache to service these loads quickly and efficiently. We achieve a maximum speedup up to 7.43% for SPEC Float programs, and attain an energy reduction of 12.95% for the L1-D cache. We achieve these results using minimal hardware and without the need for a complex microarchitectural mechanism, yet the results are similar to state-of-the-art solutions.",
    "title_zh": "全球稳定负荷的奇妙案例",
    "abstract_zh": "一项最新研究提出了一个有趣的统计数据：在许多x86程序中，约三分之一至三分之二的加载指令是冗余的。这些冗余加载指令在整个程序执行过程中反复从同一地址读取相同的值，被称为全局稳定加载（Global Stable Loads, GSLs）。我们对GSLs在不同架构、编译器及编译器优化下的行为进行了深入分析，并首次研究了新兴机器学习工作负载中的GSLs，而此前的研究尚未涉及此类场景。\n\n对SPEC程序的特征分析表明，GSLs具有很高的局部性——仅有4.2%至5.2%的静态GSLs，却承担了90%至95%的动态GSLs。我们揭示了这些主导性静态GSLs背后的原因，并提出：其中许多GSLs可在程序执行前由编译器识别出来。\n\n与采用纯微架构方案来消除GSLs的方法不同，我们提出一种编译器与硬件协同设计的解决方案：编译器预先标记出可能稳定的加载指令，微架构则利用一个小型且快速的缓存来高效、迅速地服务这些加载请求。该方法在SPEC Float程序上实现了最高达7.43%的加速比，同时使L1数据缓存的能耗降低了12.95%。整个方案仅需极少的硬件开销，无需复杂的微架构机制，但性能表现却与当前最先进的解决方案相当。"
  },
  {
    "date": "2025-11-20",
    "title": "3D DRC: Design Rule Checking for 3D IC with U-Net-based Non-Manhattan Optimization",
    "authors": "Shunjie Chang, Youran Wu, Jianli Chen, Jun Yu, Kun Wang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240842",
    "source": "IEEE",
    "abstract": "In the back-end verification of integrated circuits (IC), design rule checking (DRC) plays a critical role in ensuring that the layout complies with process constraints and guarantees manufacturability. Traditional DRC for 2D layouts involves checks on parameters including line width, spacing, and density. With the advancement of 3D IC and advanced packaging technologies, new DRC requirements have emerged. These include checks on inter-die spacing, alignment relationships, and the handling of non-Manhattan geometries. Previous academic research has focused almost exclusively on optimizing 2D DRC. Meanwhile, industrial tools such as Calibre 3DSTACK can only perform 3D rule checks on layouts that have already passed 2D DRC, which increases both workflow complexity and time consumption. Targeted at these limitations, we propose 3D DRC, a U-Net-based DRC tool designed for 3D IC. Our method performs both 2D and 3D rule checks in a single pass on 3D structures, with specific optimizations for non-Manhattan geometries in 3D IC. Additionally, the tool also supports traditional 2D DRC when applied to purely 2D layouts. We evaluated the performance of 3D DRC on a variety of open-source PDK rule sets. Extensive experiments demonstrate that the model exhibits strong generalization and high accuracy, achieving an average recall of 97.5% and precision of 97.7%. Furthermore, the model demonstrates high runtime efficiency, with the 2D mode running 22.3× faster than Calibre nmDRC, and the 3D mode being 13.3× faster than Calibre 3DSTACK.",
    "title_zh": "基于U-Net的非曼哈顿优化三维集成电路设计规则检查（3D DRC）",
    "abstract_zh": "在集成电路（IC）的后端验证中，设计规则检查（DRC）对于确保版图符合工艺约束并保障可制造性起着至关重要的作用。传统的二维（2D）版图DRC主要涉及线宽、间距和密度等参数的检查。随着三维集成电路（3D IC）及先进封装技术的发展，新的DRC需求应运而生，包括晶圆间间距、对准关系以及非曼哈顿几何结构的处理等检查项。以往的学术研究几乎全部聚焦于优化2D DRC，而工业界工具如Calibre 3DSTACK仅能在2D DRC通过之后才进行3D规则检查，这不仅增加了流程复杂度，也显著提高了验证时间。\n\n针对上述局限性，我们提出了一种基于U-Net架构的3D DRC工具——3D DRC，专为3D IC设计。该方法能够在单次遍历中同时完成2D与3D规则检查，并针对3D IC中的非曼哈顿几何结构进行了专门优化。此外，当应用于纯2D版图时，该工具仍可支持传统2D DRC功能。我们在多种开源PDK规则集上评估了3D DRC的性能。大量实验结果表明，该模型具备出色的泛化能力与高精度，平均召回率达到97.5%，精确度达到97.7%。同时，模型展现出极高的运行效率：在2D模式下，其速度比Calibre nmDRC快22.3倍；在3D模式下，比Calibre 3DSTACK快13.3倍。"
  },
  {
    "date": "2025-11-20",
    "title": "RSizing: Robust Bayesian Optimization for Analog Circuit Sizing Under Process Variations",
    "authors": "Jindong Tu, Yapeng Li, Peng Xu, Tuo Li, Guoqing Li, Zushuai Xie, Bei Yu, Tinghuan Chen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240990",
    "source": "IEEE",
    "abstract": "The increasing complexity of CMOS technology and circuit designs has intensified the need for robust analog design automation tools that can handle process variations effectively. This paper presents RSizing, a novel approach for analog circuit sizing that optimizes performance while ensuring robustness against process variations. Our method employs a three-phase strategy: First, it identifies promising design regions through nominal condition optimization to prune the design space efficiently. Second, it performs variation-aware optimization using heteroscedastic Gaussian processes (HGP) to model circuit performance under process variations, capturing the non-uniform nature of process-induced fluctuations across the design space. The HGP models are combined with an efficient acquisition function based on Thompson sampling to guide the exploration of robust designs using limited Monte Carlo simulations. Finally, it refines the solutions through additional targeted Monte Carlo simulations and model calibration. Experimental results on three benchmark circuits demonstrate that RSizing achieves superior performance compared to existing methods, consistently meeting yield requirements while optimizing multiple performance metrics with significantly reduced computational cost.",
    "title_zh": "RSizing：面向工艺变化的模拟电路尺寸设计的鲁棒贝叶斯优化",
    "abstract_zh": "随着CMOS技术与电路设计复杂性的不断提升，对能够有效应对工艺变化的鲁棒性模拟设计自动化工具的需求日益迫切。本文提出了一种名为RSizing的新颖模拟电路尺寸优化方法，该方法在优化性能的同时，确保设计对工艺变化具有强鲁棒性。我们的方法采用三阶段策略：首先，通过在标称条件下的优化识别出有前景的设计区域，从而高效地剪枝设计空间；其次，利用异方差高斯过程（HGP）进行考虑工艺变化的优化，以建模工艺变化下的电路性能，捕捉设计空间中由工艺波动引起的非均匀性特征。结合基于Thompson采样的高效获取函数，HGP模型能够在有限的蒙特卡洛仿真次数下引导对鲁棒设计的探索；最后，通过额外的针对性蒙特卡洛仿真和模型校准对解进行精细化优化。在三个基准电路上的实验结果表明，RSizing相较于现有方法表现出更优的性能，在显著降低计算成本的同时，始终满足良率要求，并有效优化多个性能指标。"
  },
  {
    "date": "2025-11-20",
    "title": "XRSight: An End-to-End Hardware-Software Co-Design Platform for XR SoC Evaluation",
    "authors": "Prashanth Ganesh, Zekai Lin, Yakun Sophia Shao",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00044",
    "source": "IEEE",
    "abstract": "The increasing complexity and performance requirements of extended reality (XR) platforms necessitate a full-stack approach to optimize system architectures. We present XRSight, a hardware-software co-design framework for developing and characterizing extended reality (XR) workloads on embedded systems-on-chip (SoCs). This open-source tool integrates a cycle-accurate, FPGA-accelerated simulation environment with a representative XR application suite to enable architectural exploration and detailed performance analysis. By bridging real-world XR workloads with a modular system-on-chip (SoC) platform, our framework offers fine-grained visibility into SoC execution behavior across compute, memory, and interconnect subsystems. We demonstrate the utility of our work through case studies that examine how different hardware configurations and workload mappings affect end-to-end system performance. Our results show how the tool can inform hardware design decisions and workload optimizations. This framework lowers the barrier to entry for researchers and architects exploring XR-centric system designs in an open and extensible SoC ecosystem.",
    "title_zh": "XRSight：一种用于XR SoC评估的端到端软硬件协同设计平台",
    "abstract_zh": "扩展现实（XR）平台日益增长的复杂性和性能需求，要求采用全栈方法来优化系统架构。我们提出了XRSight，这是一个用于在嵌入式片上系统（SoC）上开发和表征扩展现实（XR）工作负载的软硬件协同设计框架。该开源工具将周期精确、基于FPGA加速的仿真环境与一组具有代表性的XR应用相结合，支持架构探索和详细的性能分析。通过将真实世界的XR工作负载与模块化的片上系统（SoC）平台相连接，我们的框架能够对SoC在计算、内存和互连子系统中的执行行为提供细粒度的可视化洞察。我们通过案例研究展示了该工具的有效性，探讨了不同硬件配置和工作负载映射对端到端系统性能的影响。实验结果表明，该工具可为硬件设计决策和工作负载优化提供有力支持。本框架降低了研究人员和系统架构师在开放且可扩展的SoC生态系统中探索以XR为中心的系统设计的门槛。"
  },
  {
    "date": "2025-11-20",
    "title": "ChronoTE: Crosstalk-Aware Timing Estimation for Routing Optimization via Edge-Enhanced GNNs",
    "authors": "Leilei Jin, Rongliang Fu, Zhen Zhuang, Liang Xiao, Fangzhou Liu, Bei Yu, Tsung-Yi Ho",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240759",
    "source": "IEEE",
    "abstract": "Accurate timing estimation during the routing stage is critical for modern VLSI design closure, especially under increasing crosstalk effects in advanced technology nodes. During the routing process, the crosstalk effect is usually modeled by predicting coupling capacitance with congestion information. However, such estimations are often overly pessimistic, as crosstalk-induced delay is influenced not only by coupling capacitance but also by the relative arrival times of signals. In this work, we propose ChronoTE, a novel edge-enhanced graph neural network (GNN) framework that performs crosstalk-aware net delay estimation by jointly modeling physical topology and timing characteristics. By embedding timing-window-aware features into edge representations, ChronoTE enables accurate delay prediction without requiring full routing or parasitic extraction. Experimental results on industrial-scale open-source designs demonstrate that ChronoTE, by delivering sign-off quality delay estimation in the early global routing stage, significantly accelerates design closure and contributes to area reduction.",
    "title_zh": "ChronoTE：一种面向布线优化的交叉干扰感知时序估计方法，基于边增强图神经网络",
    "abstract_zh": "在现代VLSI设计的收敛过程中，布线阶段的精确时序估算至关重要，尤其是在先进工艺节点下串扰效应日益加剧的背景下。在布线过程中，串扰效应通常通过结合拥塞信息来预测耦合电容进行建模。然而，这种估算方法往往过于保守，因为由串扰引起的延迟不仅取决于耦合电容，还与信号的相对到达时间密切相关。本文提出了一种名为ChronoTE的新方法，这是一种基于边增强图神经网络（GNN）的框架，能够联合建模物理拓扑结构和时序特性，实现对串扰敏感的网表延迟预测。ChronoTE通过在边的表示中嵌入时序窗口感知特征，能够在无需完整布线或寄生参数提取的情况下实现高精度的延迟预测。在工业级开源设计上的实验结果表明，ChronoTE可在早期全局布线阶段提供符合签核标准的时序估算，显著加速设计收敛，并有助于减少芯片面积。"
  },
  {
    "date": "2025-11-20",
    "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms",
    "authors": "Zain Taufique, Aman Vyas, Antonio Miele, Pasi Liljeberg, Anil Kanduri",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240767",
    "source": "IEEE",
    "abstract": "Compound AI (cAI) systems chain multiple AI models to solve complex problems. cAI systems are typically composed of deep neural networks (DNNs), transformers, and large language models (LLMs), exhibiting a high degree of computational diversity and dynamic workload variation. Deploying cAI services on mobile edge platforms poses a significant challenge in scheduling concurrent DNN-transformer inference tasks, which arrive dynamically in an unknown sequence. Existing mobile edge AI inference strategies manage multi-DNN or transformer-only workloads, relying on design-time profiling, and cannot handle concurrent inference of DNNs and transformers required by cAI systems. In this work, we address the challenge of scheduling cAI systems on heterogeneous mobile edge platforms. We present Twill, a run-time framework to handle concurrent inference requests of cAI workloads through task affinity-aware cluster mapping and migration, priority-aware task freezing/unfreezing, and Dynamic Voltage/Frequency Scaling (DVFS), while minimizing inference latency within power budgets. We implement and deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate Twill against state-of-the-art edge AI inference techniques over contemporary DNNs and LLMs, reducing inference latency by 54% on average, while honoring power budgets.",
    "title_zh": "交织：在异构移动边缘平台上的复合人工智能系统调度",
    "abstract_zh": "复合人工智能（cAI）系统通过串联多个AI模型来解决复杂问题。这类系统通常由深度神经网络（DNN）、Transformer模型以及大语言模型（LLM）组成，表现出高度的计算多样性与动态的工作负载变化特性。在移动边缘平台部署cAI服务时，调度并发的DNN-Transformer推理任务面临巨大挑战，因为这些任务以未知顺序动态到达。现有的移动边缘AI推理策略仅能处理多DNN或仅Transformer类型的工作负载，依赖于设计阶段的性能分析，无法应对cAI系统所要求的DNN与Transformer并发推理需求。本文针对异构移动边缘平台上的cAI系统调度难题，提出Twill——一个运行时框架，通过任务亲和性感知的集群映射与迁移、优先级感知的任务冻结/解冻机制，以及动态电压/频率调节（DVFS），在满足功耗预算的前提下，最小化推理延迟。我们在Nvidia Jetson Orin NX平台上实现了Twill框架，并将其与当前最先进的边缘AI推理技术进行对比评估。实验结果表明，Twill在主流DNN和LLM上平均将推理延迟降低54%，同时严格遵守功耗限制。"
  },
  {
    "date": "2025-11-20",
    "title": "ILP-Driven FPGA Multiplier Synthesis: A Scalable Framework for Area-Latency Co-Optimization",
    "authors": "Shangshang Yao, Kunlong Li, Li Shen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240978",
    "source": "IEEE",
    "abstract": "Modern computing paradigms impose diverging requirements on arithmetic circuits, with cloud applications prioritizing throughput and edge devices demanding area efficiency under power constraints. While Field-Programmable Gate Arrays (FPGAs) leverage heterogeneous DSP-LUT fabrics for flexibility, their rigid DSP layouts and LUT-centric architectural constraints hinder scalable multiplier designs. Existing FPGA-based approaches face intrinsic scalability limitations from primitive cascading techniques and inflexible performance-resource tradeoffs. This paper proposes an Integer Linear Programming (ILP)-driven framework for Pareto-optimal multiplier synthesis, enabling arbitrary bit-widths via LUT-compressor modeling, application-aware configurations (performance-focused vs. area-minimized modes), and an automated toolchain translating ILP solutions to synthesizable Verilog. Evaluations on Xilinx UltraScale+ series FPGAs demonstrate 27.8% critical path delay reduction in high-performance mode and 21.4% LUT resource savings in areaefficient mode for 16-bit multipliers versus Xilinx LogiCORE<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">™</sup> IP. The framework’s adaptive optimization bridges cloud-edge computational divergence, achieving a 15.5-34.2% area-delay product improvement across 8-16b designs.",
    "title_zh": "基于ILP的FPGA乘法器综合：一种面向面积-延迟协同优化的可扩展框架",
    "abstract_zh": "现代计算范式对算术电路提出了截然不同的需求：云应用注重吞吐量，而边缘设备则在功耗约束下追求面积效率。尽管现场可编程门阵列（FPGA）利用异构的DSP-LUT结构实现了灵活性，但其固定的DSP布局和以LUT为中心的架构限制，严重阻碍了可扩展乘法器设计的发展。现有的FPGA实现方法受限于基本单元级联技术的固有瓶颈，且性能与资源之间的权衡缺乏灵活性，难以实现高效扩展。本文提出一种基于整数线性规划（ILP）驱动的乘法器综合框架，通过LUT-压缩器建模，支持任意位宽的乘法器设计，具备面向应用的配置模式（如性能优先或面积最小化模式），并构建了自动化工具链，将ILP求解结果转化为可综合的Verilog代码。在Xilinx UltraScale+系列FPGA上的实验表明，相较于Xilinx LogiCORE™ IP核，本框架在16位乘法器中分别实现了高性能模式下27.8%的关键路径延迟降低，以及面积优化模式下21.4%的LUT资源节省。该框架通过自适应优化有效弥合了云计算与边缘计算之间的计算需求差异，在8至16位的设计中实现了15.5%至34.2%的面积-延迟乘积改进。"
  },
  {
    "date": "2025-11-20",
    "title": "Efficient Analytical Placement Algorithm with Hybrid Fence Region Constraints Using Non-Newtonian Fluid Model",
    "authors": "Jai-Ming Lin, Hung-Wei Hsu, Tan Huang, Chen-Fa Tsai, De-Shiun Fu, Shih-Cheng Huang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240878",
    "source": "IEEE",
    "abstract": "This paper proposes a Hybrid-Region-Aware Multi-Electrostatic System to address various types of region constraints in the placement problem, which is crucial for meeting the demands of modern chip designs with multiple power domains and providing designers with the flexibility needed to achieve performance goals. Previous methods have attempted to build multiple electrostatic systems for different fence region types to manage this issue. However, these approaches fail to handle situations where instances from different fence region types are allowed to be placed within the same region for a specific type of fence region constraint. To overcome these limitations, we propose generating General electrostatic system that eliminates overlaps between instances across isolated electrostatic systems while minimizing disruptions to the placement result. Additionally, instances assigned to a fence region may initially be displaced from their designated placeable regions due to wirelength forces, requiring extra time and effort to reposition them. To address this issue, we develop a resistive force formulation based on a non-Newtonian fluid model and integrate it into the analytical framework to enhance both convergence stability and efficiency. Experimental results demonstrate the efficiency and effectiveness of our approach, achieving an 11-14% reduction in iteration count compared to MORPH and DREAMPlace 3.0 on academic benchmarks, and a 7.8x reduction in runtime compared to Innovus on industrial benchmarks.",
    "title_zh": "基于非牛顿流体模型的混合围栏区域约束高效分析布局算法",
    "abstract_zh": "本文提出了一种混合区域感知的多静电系统（Hybrid-Region-Aware Multi-Electrostatic System），以解决放置问题中各类区域约束的挑战，这对于满足现代芯片设计中多电源域的需求，并为设计师实现性能目标提供灵活性至关重要。以往的方法尝试针对不同类型的围栏区域构建多个静电系统来管理这一问题。然而，这些方法无法处理在特定类型的围栏区域约束下，允许多种类型实例被放置于同一区域的情况。为克服上述局限性，我们提出构建一种通用静电系统，该系统在消除不同孤立静电系统间实例重叠的同时，最大限度地减少对布局结果的干扰。此外，被分配到围栏区域的实例可能因布线长度力的作用而初始偏离其指定可放置区域，需要额外的时间和精力重新定位。为解决此问题，我们基于非牛顿流体模型提出了一个阻抗力公式，并将其集成到分析框架中，从而显著提升了收敛稳定性和效率。实验结果表明，本方法具有高效性和有效性：在学术基准测试中，相比MORPH和DREAMPlace 3.0，迭代次数减少了11%-14%；在工业基准测试中，相比Innovus，运行时间缩短了7.8倍。"
  },
  {
    "date": "2025-11-20",
    "title": "GeoFA: A Geometric Finite Automaton Engine for Efficient Layout Pattern Matching",
    "authors": "Qingsheng Qiu, Ziwen Zheng, Boyu Shi, Chao Wang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240662",
    "source": "IEEE",
    "abstract": "As Integrated Circuit (IC) design complexity increases exponentially, Layout Pattern Matching (LPM) has become a critical technique for physical verification, yet faces significant challenges in efficiency and accuracy. This paper presents a high-performance two-stage LPM framework centered around the innovative GeoFA (Geometric Finite Automaton) engine. The first stage successfully applies Deterministic Finite Automaton (DFA) theory for the first time to 2D layout pattern matching with geometric tolerances, enabling rapid candidate region screening with near-linear time complexity. The second stage then performs precise geometric verification, leveraging optimized spatial indexing, robust Boolean operations, and specific optimizations for cell arrays, to guarantee 100% sign-off accuracy. Experimental results demonstrate that the framework achieves 100% accuracy across diverse layouts. On standard benchmarks, it provides a speedup of approximately 1.7x over state-of-the-art academic work. Furthermore, compared to the industry-standard tool Calibre, it delivers average speedups of up to 201.7x and 22.9x on complex single-layer and cell-level matching tasks, respectively, demonstrating exceptional performance and scalability.",
    "title_zh": "GeoFA：一种用于高效布局模式匹配的几何有限自动机引擎",
    "abstract_zh": "随着集成电路（IC）设计复杂度呈指数级增长，版图模式匹配（LPM）已成为物理验证中的关键技术，但其在效率与准确性方面仍面临巨大挑战。本文提出了一种高性能的两阶段LPM框架，核心是创新的GeoFA（几何有限自动机）引擎。第一阶段首次将确定性有限自动机（DFA）理论应用于具有几何容差的二维版图模式匹配，实现了近线性时间复杂度的快速候选区域筛选。第二阶段则进行精确的几何验证，通过优化的空间索引、鲁棒的布尔运算以及针对单元阵列的特定优化，确保了100%的签核精度。实验结果表明，该框架在多种版图上均实现了100%的准确率。在标准基准测试中，其性能相比当前最先进的学术工作提升了约1.7倍。此外，与工业标准工具Calibre相比，该框架在复杂的单层匹配和单元级匹配任务中分别实现了高达201.7倍和22.9倍的平均加速比，充分展现了其卓越的性能与可扩展性。"
  },
  {
    "date": "2025-11-20",
    "title": "Capacitance Extraction via Machine Learning with Application to Interconnect Geometry Exploration",
    "authors": "Cheng-Yu Tsai, Suwan Kim, Sung-Kyu Lim",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240935",
    "source": "IEEE",
    "abstract": "As Moore’s law slows down, foundries and design houses are resorting to design technology co-optimization (DTCO) to squeeze more performance out of a technology node. However, the efficiency of EDA tools plays a key role in driving DTCO. The existing commercial parasitic extraction tools are not able to respond to a process parameter change efficiently. For example, it takes 25 minutes to re-generate a parasitic netlist for a mere layer thickness change using existing commercial tools. This runtime becomes the bottleneck for standard cell DTCO. In this work, we demonstrate a machine-learning-based method targeted on standard cells that can efficiently extract parasitic capacitance within seconds while maintaining competitive error distribution compared to the state-of-the-art rule-based 2.5D extraction method, which suffers from pattern mismatch since no real layout is provided at the pre-characterization stage. We extract patterns from actual standard cell layouts as training data for the ML model, and the model can predict coupling capacitance with unseen layer thickness within milliseconds.",
    "title_zh": "基于机器学习的电容提取及其在互连几何探索中的应用",
    "abstract_zh": "随着摩尔定律逐渐放缓，晶圆代工厂和设计公司正转向设计与工艺协同优化（DTCO），以在每个技术节点上榨取更多性能。然而，电子设计自动化（EDA）工具的效率在推动DTCO方面起着关键作用。现有的商业寄生参数提取工具对工艺参数变化的响应速度不够高效。例如，仅因层厚变化，使用现有商业工具重新生成寄生网表就需要25分钟，这一运行时间已成为标准单元DTCO的瓶颈。本文提出一种面向标准单元的基于机器学习的方法，能够在数秒内高效完成寄生电容提取，同时保持与当前最先进的基于规则的2.5D提取方法相当的误差分布水平。后者由于在预表征阶段缺乏实际版图信息，存在图案不匹配的问题。我们从真实的标准单元版图中提取图案作为机器学习模型的训练数据，该模型可在毫秒级时间内预测未见过的层厚下的耦合电容。"
  },
  {
    "date": "2025-11-20",
    "title": "AccelStack: A Cost-Driven Analysis of 3D-Stacked LLM Accelerators",
    "authors": "Chen Bai, Xin Fan, Zhenhua Zhu, Wei Zhang, Yuan Xie",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240867",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) show viability for artificial general intelligence (AGI) with high computing power and memory bandwidth demands. While existing LLM accelerators leverage high-bandwidth memory (HBM) and 2.5D packaging to address the challenge, emerging hybrid bonding techniques unlock new opportunities for 3D-stacked LLM accelerators. This paper proposes AccelStack, a cost-driven analysis for the new architecture via two innovations. First, a performance model capturing memory-on-logic is presented. Second, a cost model for die-on-die (DoD), die-on-wafer (DoW), and wafer-on-wafer (WoW) is proposed. Evaluations show 3D-stacked accelerators achieve up to 7.17× and 2.09× faster inference than NVIDIA A100 (FP16) and H100 (FP8) simulation results across various LLM workloads, with chiplet-based designs reducing recurring engineering costs by 38.09% versus monolithic implementations.",
    "title_zh": "AccelStack：面向3D堆叠大模型加速器的成本驱动分析",
    "abstract_zh": "大型语言模型（LLMs）在高算力和高内存带宽需求的背景下，展现出实现通用人工智能（AGI）的可行性。尽管现有的LLM加速器通过采用高带宽内存（HBM）和2.5D封装技术应对这一挑战，但新兴的混合键合技术为3D堆叠式LLM加速器带来了新的机遇。本文提出AccelStack，一种面向新型架构的成本驱动分析方法，包含两项创新：其一，提出了一个能够捕捉“逻辑与内存集成”特性的性能模型；其二，构建了针对芯片对芯片（DoD）、芯片对晶圆（DoW）以及晶圆对晶圆（WoW）三种封装方式的成本模型。评估结果表明，在多种LLM工作负载下，3D堆叠式加速器的推理速度相比NVIDIA A100（FP16）和H100（FP8）的仿真结果分别提升最高达7.17倍和2.09倍；同时，基于Chiplet的设计相较于单片式实现，可将重复性工程成本降低38.09%。"
  },
  {
    "date": "2025-11-20",
    "title": "LLM-Augmented Multi-Modal Fusion for SoC Design Space Exploration",
    "authors": "Donger Luo, Tianyi Li, Xinheng Li, Qi Sun, Cheng Zhuo, Bei Yu, Jingyi Yu, Hao Geng",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240720",
    "source": "IEEE",
    "abstract": "The increasing complexity of modern SoC designs creates challenges in efficiently exploring vast design spaces. Current approaches often reduce microarchitectures to simple parameter vectors, overlooking their rich information embedded in both functional behaviors and implementation details. This paper proposes an LLM-augmented multi-modal fusion method that captures this dual nature of microarchitecture design. By recombining design parameters with their natural language descriptors, we leverage a domain-knowledge-enhanced LLM to extract semantic features that represent functional behavior. Simultaneously, we process Chisel-compiled RTL through a graph neural network to capture structural implementation details. This multi-modal approach enables more effective feature extraction from limited evaluation data. We integrate these rich features into an MLP enhanced with Monte Carlo dropout. This approach provides uncertainty quantification while enabling end-to-end training, allowing the pre-trained feature extractors to be fine-tuned during exploration through Bayesian optimization. Experimental results and ablation studies on a Gemmini-based RISC-V SoC demonstrate that our approach significantly improves exploration efficiency and prediction quality under data limitations.",
    "title_zh": "基于大语言模型增强的多模态融合用于片上系统设计空间探索",
    "abstract_zh": "现代片上系统（SoC）设计的复杂性日益增加，给大规模设计空间的高效探索带来了挑战。当前的方法通常将微架构简化为简单的参数向量，忽略了其在功能行为和实现细节中蕴含的丰富信息。本文提出了一种基于大语言模型（LLM）增强的多模态融合方法，以捕捉微架构设计的双重特性。通过将设计参数与其自然语言描述重新组合，我们利用一个融入领域知识的LLM提取表征功能行为的语义特征；同时，采用图神经网络处理Chisel编译生成的RTL代码，以捕获结构化的实现细节。这种多模态方法能够在有限评估数据条件下实现更有效的特征提取。我们将这些丰富的特征集成到一个结合蒙特卡洛丢弃（Monte Carlo dropout）的多层感知机（MLP）中，该方法不仅能够提供不确定性量化，还支持端到端训练，使预训练的特征提取器可在贝叶斯优化过程中进行微调。在基于Gemmini的RISC-V SoC上的实验结果与消融研究证明，该方法在数据受限的情况下显著提升了设计探索效率和预测精度。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Side Channel Vulnerability Analysis of Flexible Neuromorphic Circuits",
    "authors": "Priyanjana Pal, Brojogopal Sapui, Mehdi B. Tahoori",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240706",
    "source": "IEEE",
    "abstract": "The rapid advancement of flexible electronics (FE) has driven significant innovation across diverse sectors, including healthcare, wearables, smart packaging, and IoT devices, owing to their adaptability, lightweight form factor, and cost-effectiveness compared to traditional silicon-based electronics. A key computing paradigm in this domain is bespoke classifiers, where model parameters are hardcoded in neuromorphic hardware to meet strict area, power, and cost constraints. By tailoring bespoke hardware to specific tasks, these circuits achieve significant accuracy under tight resource budgets but also introduce distinct security vulnerabilities. The intrinsic flexibility of substrates, unconventional manufacturing processes, and limited protective packaging make such systems particularly vulnerable to security threats, with side-channel attacks (SCAs) being a critical concern. In this work, we systematically investigate SCA vulnerabilities in bespoke TFT-based multilayer perceptron (MLP) classifiers, considering both analog (flexible analog multilayer perceptron (f-AMLP)) and digital (flexible digital multilayer perceptron (f-DMLP)) realizations. For digital classifiers, we apply correlation power analysis (CPA), leveraging well-established leakage models from silicon-based systems. For analog classifiers, where leakage is continuous, nonlinear, and strongly influenced by device-level variability, we develop a tailored convolutional neural network (CNN)-based regression attack capable of extracting inputs from noisy power traces. Experimental results across benchmark datasets show that f-DMLPs can be compromised with 70–85% cumulative attack success rate (ASR) after ≈ 4k–5k traces using CPA, while f-AMLPs, though slower to attack initially, reach up to 90–95% ASR after ≈ 8k–9k traces with CNN-based approach.",
    "title_zh": "特邀论文：柔性神经形态电路的侧信道漏洞分析",
    "abstract_zh": "柔性电子（FE）的快速发展推动了医疗健康、可穿戴设备、智能包装及物联网（IoT）等多个领域的显著创新，这得益于其相较于传统硅基电子器件所具备的适应性强、轻量化以及成本低等优势。在该领域中，一种关键的计算范式是定制化分类器，其模型参数被硬编码于类脑硬件中，以满足严格的面积、功耗和成本限制。通过针对特定任务量身定制硬件，这些电路在资源极度受限的情况下仍能实现较高的精度，但同时也引入了独特的安全漏洞。由于基底材料固有的柔韧性、非传统的制造工艺以及有限的防护封装，此类系统尤其容易受到安全威胁，其中侧信道攻击（SCAs）尤为突出。本文系统地研究了基于薄膜晶体管（TFT）的定制化多层感知机（MLP）分类器在侧信道攻击下的脆弱性，涵盖模拟实现（柔性模拟多层感知机，f-AMLP）与数字实现（柔性数字多层感知机，f-DMLP）两种形式。对于数字分类器，我们采用相关功耗分析（CPA）方法，并借鉴硅基系统中成熟的泄漏模型；而对于模拟分类器，由于其泄漏信号具有连续性、非线性特征且受器件级变异性的强烈影响，我们提出了一种专门设计的基于卷积神经网络（CNN）的回归攻击方法，能够从噪声功率轨迹中提取输入信息。实验结果表明，在基准数据集上的测试显示，f-DMLP分类器在约4000至5000次功耗迹线后即可被攻破，累计攻击成功率（ASR）达到70%–85%；而f-AMLP分类器虽初始攻击速度较慢，但在约8000至9000次迹线后，通过CNN方法可实现高达90%–95%的ASR。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: CURE-Fuzz: Curiosity-Driven Reinforcement Learning for Agile Hardware Testing",
    "authors": "Hanwei Fan, Ya Wang, Xiaofeng Zhou, Sicheng Li, Binguang Zhao, Yangdi Lyu, Jiang Xu, Wei Zhang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240915",
    "source": "IEEE",
    "abstract": "Modern processors feature complex architectures that necessitate the generation of extensive test programs to ensure functional correctness, making testing the most time-consuming stage of the processor design flow. Existing automated verification frameworks for agile design exhibit significant limitations, such as fixed program structures restricting flexibility, uncontrolled control flows leading to invalid instructions, and low coverage of the vast state space. To address these limitations, we propose CURE-Fuzz, a curiosity-driven reinforcement learning framework designed to enhance agile hardware testing. By integrating a hierarchical test generation model with a curiosity-driven exploration mechanism, CURE-Fuzz enables precise control over test program structure and dependencies while efficiently navigating unexplored processor states. Evaluations on Rocket and Boom core demonstrate that CURE-Fuzz achieves higher coverage and exhibits superior bug detection capabilities compared to state-of-the-art fuzzers.",
    "title_zh": "特邀论文：CURE-Fuzz：基于好奇心驱动的强化学习敏捷硬件测试",
    "abstract_zh": "现代处理器采用复杂的架构，这要求生成大量测试程序以确保功能正确性，使得测试成为处理器设计流程中最耗时的阶段。现有的敏捷设计自动化验证框架存在显著局限：程序结构固定，灵活性不足；控制流无法有效控制，导致产生无效指令；对庞大状态空间的覆盖度较低。为解决上述问题，我们提出 CURE-Fuzz——一种基于好奇心驱动的强化学习框架，旨在提升敏捷硬件测试的效率与效果。通过将分层测试生成模型与好奇心驱动的探索机制相结合，CURE-Fuzz 能够精确控制测试程序的结构与依赖关系，同时高效探索处理器未覆盖的状态。在 Rocket 和 Boom 核心上的评估结果表明，CURE-Fuzz 在覆盖率和缺陷检测能力方面均优于当前最先进的 fuzz 工具。"
  },
  {
    "date": "2025-11-20",
    "title": "HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions",
    "authors": "Jing Wang, Shang Liu, Yao Lu, Zhiyao Xie",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240664",
    "source": "IEEE",
    "abstract": "High-level synthesis (HLS) accelerates hardware design by enabling the automatic translation of high-level descriptions into efficient hardware implementations. However, debugging HLS code is a challenging and labor-intensive task, especially for novice circuit designers or software engineers without sufficient hardware domain knowledge. The recent emergence of Large Language Models (LLMs) is promising in automating the HLS debugging process. Despite the great potential, three key challenges persist when applying LLMs to HLS logic debugging: 1) High-quality circuit data for training LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in hardware is inherently more complex than identifying software bugs with existing golden test cases. 3) The absence of reliable test cases requires multi-tasking solutions, performing both bug identification and correction. In this work, we propose a customized solution named HLSDebugger<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>, to address the challenges. HLSDebugger first generates and releases a large labeled dataset with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts an encoder-decoder structure, performing bug location identification, bug type prediction, and bug correction with the same model. HLSDebugger significantly outperforms advanced LLMs like GPT-4 in bug identification and by more than 3× in bug correction. It makes a substantial advancement in the exploration of automated debugging of HLS code.",
    "title_zh": "HLSDebugger：基于大模型解决方案的HLS代码逻辑错误识别与修复",
    "abstract_zh": "高层次综合（HLS）通过将高级描述自动转换为高效的硬件实现，加速了硬件设计过程。然而，HLS代码的调试是一项极具挑战性且耗时的任务，尤其对于缺乏足够硬件领域知识的初学者或软件工程师而言更是如此。近年来，大型语言模型（LLMs）在自动化HLS调试方面展现出巨大潜力。尽管前景广阔，但在将LLMs应用于HLS逻辑调试时仍面临三个关键挑战：1）用于训练LLM的高质量电路数据稀缺，构成重大障碍；2）识别硬件中的逻辑错误本质上比利用现有基准测试用例发现软件错误更为复杂；3）由于缺乏可靠的测试用例，需要采用多任务解决方案，同时完成错误定位与修复。针对上述问题，本文提出一种定制化解决方案——HLSDebugger<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>。HLSDebugger首先生成并发布了一个包含30万条数据样本的大型标注数据集，专门针对HLS逻辑错误。该模型采用编码器-解码器结构，使用同一模型完成错误定位、错误类型预测和错误修复任务。实验结果表明，HLSDebugger在错误识别方面显著优于GPT-4等先进LLM，在错误修复性能上更是提升超过3倍。这一工作在推动HLS代码自动化调试研究方面取得了重要进展。"
  },
  {
    "date": "2025-11-20",
    "title": "H3D-LLM: Heterogeneous 3D Chiplet Design for LLM Inference with Dynamic Task Scheduling and Memory-Aware Orchestration",
    "authors": "Hui Kou, Chenjie Xia, Jialin Yang, Liyi Li, Hao Cai, Xin Si, Bo Liu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240702",
    "source": "IEEE",
    "abstract": "The exponential growth of Large Language Model (LLM) intensifies hardware demands for energy-efficient, low-latency architectures with scalable memory bandwidth. While 3D chiplet integration addresses conventional systems’ memory wall limitations, three critical challenges persist: asymmetric compression constraints from divergent sparsity-precision requirements across attention/projection layers, tier-level load imbalance from static resource allocation in dynamic computation patterns, and coupling-induced signal degradation in high-density TSV networks, especially under LLM-phase-specific traffic with spatiotemporal burstiness. To address these, we present H3D-LLM, a vertically heterogeneous architecture combining analog/digital Computing-in-Memory (CIM) and Neural Processing Unit (NPU) chiplets through three innovations. First, a Sparse-Aware Dynamic Execution Framework (SADEF) with Precision-Adaptive Quantization Mechanism (PAQM) enables hardware-aware compression via layer-wise unstructured sparsity detection and INT4/8-FP/BF16 mixed precision. Second, a 3D Spatio-Temporal Interleaved Parallelism (3D-STIP) with Semantic-Aware Tiered Storage (SATS) eliminates resource imbalance and improves memory efficiency through dynamic sub-batch partitioning and Key-Value (KV) cache aware management. Third, a Phase-Adaptive TSV Management (PATM) scheme with dynamic encoding and cluster-based allocation enhances inter-connect efficiency and signal integrity through runtime-aware partitioning and phase-specific dataflow scheduling. Evaluations on Llama-7B demonstrate that H3D-LLM achieves 12.3× higher energy efficiency and 8.4× faster inference than the A800 GPU, while its TSV strategy increases eye height by 12% and reduces bit error rate by up to 60× compared to naïve 3D accelerators.",
    "title_zh": "H3D-LLM：面向大模型推理的异构3D芯片设计，支持动态任务调度与内存感知协同",
    "abstract_zh": "大规模语言模型（LLM）的指数级增长加剧了对高效能、低延迟架构以及可扩展内存带宽的硬件需求。尽管3D芯粒集成技术缓解了传统系统面临的“内存墙”瓶颈，但仍存在三大关键挑战：注意力/投影层因稀疏性与精度需求差异导致的非对称压缩约束；动态计算模式下静态资源分配引发的层级负载不均；以及在高密度TSV网络中，尤其在具有时空突发特性的LLM特定阶段流量下，耦合引起的信号退化问题。为应对上述挑战，我们提出H3D-LLM——一种垂直异构架构，通过三种创新将模拟/数字存内计算（CIM）与神经处理单元（NPU）芯粒相结合。第一，提出一种面向稀疏性的动态执行框架（SADEF），结合自适应精度量化机制（PAQM），实现基于逐层无结构稀疏性检测的硬件感知压缩，并支持INT4/8与FP/BF16混合精度运算。第二，设计三维时空交织并行机制（3D-STIP）与语义感知分层存储方案（SATS），通过动态子批划分和键值（KV）缓存感知管理，消除资源不平衡，显著提升内存效率。第三，提出阶段自适应TSV管理方案（PATM），通过运行时感知的分区策略与阶段特异性数据流调度，结合动态编码与基于集群的资源分配，有效提升互连效率与信号完整性。在Llama-7B上的评估表明，H3D-LLM相比A800 GPU实现了12.3倍更高的能效和8.4倍更快的推理速度；其TSV策略使眼图高度提升12%，误码率降低高达60倍，显著优于传统的3D加速器方案。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: LLM-Enhanced GPU-Optimized Physical Design at Scale",
    "authors": "Yi-Chen Lu, Hao-Hsiang Hsiao, Haoxing Ren",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240986",
    "source": "IEEE",
    "abstract": "Modern Physical Design (PD) flows face a dual challenge: proprietary, heterogeneous design data and the rapid evolution of process nodes, both of which block models from transferring to new chips. To overcome these hurdles, we demonstrate a unified, data-driven framework that distills critical netlist optimization moves, including gate sizing, buffer insertion, and cell relocation, into \"optimization primitives\" learned by Large Language Models (LLMs). Particularly, we develop a high-quality, synthetic optimization data generation pipeline with commercial tools at scale, while using a GPU-accelerated differentiable Static Timing Analysis (STA) engine to create fast feedback loop, enabling end-to-end gradient propagation to guide model learning. By training on both real and synthetic data across multiple technology generations, our approach captures fundamental PD optimization patterns that transfer seamlessly to unseen designs, overcoming the constraints of fragmented design representations and proprietary data in industrial PD flows.",
    "title_zh": "特邀论文：基于大模型增强的GPU优化大规模物理设计",
    "abstract_zh": "现代物理设计（PD）流程面临双重挑战：专有的异构设计数据以及工艺节点的快速演进，这两者均阻碍了模型向新芯片的迁移。为克服这些障碍，我们提出了一种统一的数据驱动框架，将关键的网表优化操作——包括晶体管尺寸调整、缓冲器插入和单元重定位——提炼为由大型语言模型（LLMs）学习得到的“优化原语”。特别地，我们开发了一个高质量、可扩展的合成优化数据生成流水线，利用商业工具在大规模下生成数据；同时采用GPU加速的可微分静态时序分析（STA）引擎，构建快速反馈回路，实现端到端的梯度传播，从而指导模型学习。通过在多个技术代际的真实与合成数据上进行训练，我们的方法捕捉到了普适的物理设计优化规律，并能无缝迁移到未见过的设计中，突破了工业PD流程中碎片化设计表示和专有数据带来的限制。"
  },
  {
    "date": "2025-11-20",
    "title": "Multi-modal Generative AI: Multi-modal LLMs, Diffusions and the Unification",
    "authors": "Xin Wang, Yuwei Zhou, Bin Huang, Hong Chen, Wenwu Zhu",
    "publish": "IEEE Transactions on Circuits and Systems for Video Technology",
    "url": "https://doi.org/10.1109/tcsvt.2025.3635224",
    "source": "IEEE",
    "abstract": "Multi-modal generative AI (Artificial Intelligence) has attracted increasing attention from both academia and industry. Particularly, two dominant families of techniques have emerged: i) Multi-modal large language models (LLMs) demonstrate impressive ability for <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">multi-modal understanding</i>; and ii) Diffusion models exhibit remarkable multi-modal powers in terms of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">multi-modal generation</i>. Therefore, this paper provides a comprehensive overview of multi-modal generative AI, including multi-modal LLMs, diffusions, and the unification for understanding and generation. To lay a solid foundation for unified models, we first provide a detailed review of both multi-modal LLMs and diffusion models, respectively, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video LLMs as well as text-to-image/video generation. Furthermore, we explore the emerging efforts toward unified models for understanding and generation. To achieve the unification of understanding and generation, we investigate key designs including autoregressive-based and diffusion-based modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then introduce several strategies for unified models, analyzing their potential advantages and disadvantages. In addition, we summarize the common datasets widely used for multi-modal generative AI pretraining. Last but not least, we present several challenging future research directions that may contribute to the ongoing advancement of multi-modal generative AI.",
    "title_zh": "多模态生成式人工智能：多模态大语言模型、扩散模型与统一之路",
    "abstract_zh": "多模态生成式人工智能（Artificial Intelligence）正日益受到学术界和产业界的广泛关注。特别是，两类主导技术逐渐兴起：i）多模态大语言模型（LLMs）在**多模态理解**方面展现出惊人的能力；ii）扩散模型（Diffusion Models）在**多模态生成**方面表现出卓越的性能。因此，本文对多模态生成式人工智能进行了全面综述，涵盖多模态大语言模型、扩散模型以及理解与生成的统一范式。\n\n为构建统一模型的坚实基础，本文首先分别对多模态大语言模型和扩散模型进行了详尽回顾，内容包括其概率建模流程、多模态架构设计，以及在图像/视频大语言模型和文本到图像/视频生成等前沿应用中的进展。此外，本文还探讨了当前迈向理解与生成统一模型的新兴研究努力。为了实现理解与生成的统一，我们深入分析了关键建模设计，包括基于自回归和基于扩散的建模方法，以及密集型架构与专家混合（Mixture-of-Experts, MoE）架构。随后，我们介绍了若干统一模型的策略，并对其潜在优势与局限性进行了系统分析。\n\n同时，本文总结了多模态生成式人工智能预训练中广泛使用的公共数据集。最后，本文提出了若干具有挑战性的未来研究方向，这些方向有望推动多模态生成式人工智能的持续发展与突破。"
  },
  {
    "date": "2025-11-20",
    "title": "Learning Architectural Cache Simulator Behaviour",
    "authors": "Pranjali Jain, Meiru Han, Zhizhou Zhang, Brandon Lee, Jonathan Balkind",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00025",
    "source": "IEEE",
    "abstract": "Modern applications exhibit memory access patterns with complex spatial and temporal relationships. Traditional architectural simulators utilized to evaluate these applications are highly sequential in nature, particularly for stateful components like caches. In this paper, we present an innovative approach to cache simulation by reframing the problem from a deep learning perspective. We exploit the fact that memory access traces in any part of a processor design can be represented as two-dimensional heatmaps. Our key insight is that the behaviour of a cache acts as a filter on these heatmap images which can be learned as a function using deep learning techniques. Leveraging this observation, we introduce CacheBox, a framework that employs a Generative Adversarial Network (GAN) to learn and replicate the filtering behaviour of caches using memory access heatmaps. We demonstrate that CacheBox effectively generalises across multiple state-of-the-art benchmarks, various cache configurations, different cache hierarchy levels, and even alternative microarchitectural structures with high accuracy. We also show that CacheBox enables highly parallelized inference, allowing for simultaneous processing of multiple memory access heatmaps.",
    "title_zh": "学习架构级缓存模拟器的行为",
    "abstract_zh": "现代应用程序展现出具有复杂空间与时间关联性的内存访问模式。传统用于评估这些应用的架构模拟器在本质上高度串行，尤其是对于缓存等有状态组件而言。本文提出一种创新的缓存模拟方法，将问题从深度学习的角度重新建模。我们利用了处理器设计中任意部分的内存访问轨迹均可表示为二维热图这一特性。我们的核心洞察是：缓存的行为可被视为对这些热图图像的一种滤波操作，而这种滤波行为可通过深度学习技术建模为一个函数。基于这一观察，我们提出了CacheBox框架，该框架采用生成对抗网络（GAN）来学习并复现缓存的滤波行为，其输入为内存访问热图。实验表明，CacheBox能够以高精度在多个前沿基准测试、不同缓存配置、多种缓存层级，甚至其他微架构结构之间实现有效泛化。此外，我们还证明CacheBox支持高度并行化的推理，可同时处理多个内存访问热图。"
  },
  {
    "date": "2025-11-20",
    "title": "MoE-OPU: An FPGA Overlay Processor Leveraging Expert Parallelism for MoE-based Large Language Models",
    "authors": "Shaoqiang Lu, Yangbo Wei, Junhong Qian, Chen Wu, Xiao Shi, Lei He",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240807",
    "source": "IEEE",
    "abstract": "The advent of Large Language Models (LLMs) like DeepSeek, empowered by the Mixture-of-Experts (MoE) architecture, has driven significant advancements across diverse applications. However, a critical challenge arises during inference: Only a small fraction of experts are activated, causing severe token allocation imbalances among experts. This inefficiency poses substantial storage and computational burdens on resource-constrained devices, exacerbated by the lack of optimization strategies that integrate expert usage-aware parameter pruning and parallel scheduling, ultimately leading to suboptimal resource utilization. To address these limitations, we propose MoE-OPU, an FPGA-based overlay processor that optimizes parallel MoE inference through three key innovations. First, we introduce N:M sparsity (1:4/2:4/4:8/6:8/8:8) in the MLP layers and mixed-precision quantization (BF16/FP8/INT4) guided by expert activation frequency, reducing the parameter size by up to 2.76× while maintaining model accuracy (only 1.53% average drop after fine-tuning). Second, a lightweight prediction network dynamically predicts next-layer \"hot\" experts by analyzing historical activation patterns and current hidden states, achieving an average prediction hit rate of 83.4%. Third, a reconfigurable multi-core architecture maximizes the utilization of HBM bandwidth via a systolic array that natively supports sparse and mixed-precision computations, coupled with parallel concatenation to balance compute and memory efficiency. Experimental results on a Xilinx V80 FPGA with the DeepSeek-V2-lite model demonstrate that MoE-OPU outperforms the NVIDIA A100 GPU, delivering a 6.78× higher token throughput. Compared to RTX 4090 and U200 FPGA, MoE-OPU achieves 13.37× and 7.85× improvements, respectively. These advancements highlight the potential of algorithm-hardware co-design for scalable deployment of MoE-based LLMs on edge devices.",
    "title_zh": "MoE-OPU：一种利用专家并行性的FPGA叠加处理器，用于基于MoE的大语言模型",
    "abstract_zh": "大型语言模型（LLM）如DeepSeek，得益于混合专家（Mixture-of-Experts, MoE）架构的加持，在各类应用中取得了显著进展。然而，在推理阶段面临一个关键挑战：仅有少量专家被激活，导致专家间令牌分配严重不均。这种低效性给资源受限设备带来了巨大的存储与计算负担，而缺乏融合专家使用感知的参数剪枝与并行调度优化策略，进一步加剧了资源利用效率低下问题。为解决上述局限，我们提出了一种基于FPGA的叠加处理器MoE-OPU，通过三项核心创新实现并行MoE推理的优化。首先，我们在MLP层引入N:M稀疏性（1:4/2:4/4:8/6:8/8:8）以及基于专家激活频率引导的混合精度量化（BF16/FP8/INT4），在保持模型精度的前提下，将参数规模最大缩减2.76倍，微调后仅带来1.53%的平均性能下降。其次，设计了一个轻量级预测网络，通过分析历史激活模式与当前隐藏状态，动态预测下一层“热点”专家，实现了83.4%的平均预测命中率。第三，采用可重构多核架构，通过原生支持稀疏与混合精度计算的脉动阵列，最大化HBM带宽利用率，并结合并行拼接机制，有效平衡计算与内存效率。在Xilinx V80 FPGA平台上，基于DeepSeek-V2-lite模型的实验结果表明，MoE-OPU的令牌吞吐量较NVIDIA A100 GPU提升6.78倍；相较于RTX 4090和U200 FPGA，分别实现13.37倍和7.85倍的性能提升。这些成果凸显了算法与硬件协同设计在边缘设备上实现MoE类LLM可扩展部署的巨大潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "Improving the Performance of Out-of-Core LLM Inference Using Heterogeneous Host Memory",
    "authors": "Sudhanshu Gupta, Sandhya Dwarkadas",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00035",
    "source": "IEEE",
    "abstract": "The memory footprint of modern applications like large language models (LLMs) far exceeds the memory capacity of accelerators they run on and often spills over to host memory. As model sizes continue to grow, DRAM-based memory is no longer sufficient to contain these models, resulting in further spill-over to storage and necessitating the use of technologies like Intel Optane and CXL-enabled memory expansion. While such technologies provide more capacity, their higher latency and lower bandwidth has given rise to heterogeneous memory configurations that attempt to strike a balance between capacity and performance. This paper evaluates the impact of such memory configurations on a GPU running out-of-core LLMs. Starting with basic host/device bandwidth measurements using an Optane and Nvidia A100 equipped NUMA system, we present a comprehensive performance analysis of serving OPT-30B and OPT-175B models using FlexGen, a state-of-the-art serving framework.Our characterization shows that FlexGen’s weight placement algorithm is a key bottleneck limiting performance. Based on this observation, we evaluate two alternate weight placement strategies, one each optimizing for inference latency and throughput. When combined with model quantization, our strategies improve latency and throughput by 27% and 5x, respectively. These figures are within 9% and 6% of an all-DRAM system, demonstrating how careful data placement can effectively enable the substitution of DRAM with high-capacity but slower memory, improving overall system energy efficiency.",
    "title_zh": "利用异构主机内存提升外存大语言模型推理性能",
    "abstract_zh": "现代应用程序（如大语言模型LLMs）的内存占用远超其运行所依赖加速器的内存容量，常常需要溢出至主机内存。随着模型规模持续扩大，基于DRAM的内存已无法容纳这些模型，导致进一步溢出至存储设备，从而不得不采用Intel Optane和CXL支持的内存扩展技术等方案。尽管这些技术提供了更大的内存容量，但其更高的延迟和更低的带宽也催生了异构内存架构，旨在平衡容量与性能之间的矛盾。本文评估了此类内存配置对GPU上运行“外存”大语言模型的影响。我们首先在配备Optane和Nvidia A100的NUMA系统上进行基础的主机/设备间带宽测量，随后利用当前最先进的推理框架FlexGen，对OPT-30B和OPT-175B模型的部署进行了全面的性能分析。研究发现，FlexGen的权重放置算法是限制性能的关键瓶颈。基于此观察，我们提出了两种替代的权重放置策略：一种专注于优化推理延迟，另一种则侧重提升吞吐量。结合模型量化技术后，我们的策略分别将延迟降低27%、吞吐量提升5倍。这些性能提升分别仅比全DRAM系统低9%和6%，充分证明了通过精心设计的数据布局，可以有效以高容量但较慢的内存替代DRAM，从而显著提升整体系统的能效表现。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: End-to-end RFIC Topology Synthesis and Design combining Reinforcement learning and Inverse Design",
    "authors": "Kaushik Sengupta, Jonathan Zhou, Emir Ali Karahan, Juho Park",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11241013",
    "source": "IEEE",
    "abstract": "Distinct from low-frequency analog design, design of radio-frequency, millimeter-wave and terahertz frequency chips follows a complex co-design process between circuits and electromagnetics (EM). These two domains are strongly coupled to each other, and this makes the design process extremely complex. Similar to circuit topologies, EM topologies can be very diverse ranging from lumped elements such as inductors, capacitors to distributed passive structures such as transmission lines, antennas, and all possible combinations, taking one or more elements of the set. The design process from ideation to schematic to layout can be extremely time-consuming relying heavily on expert knowledge, manual interventions, and iterative tuning of predefined circuit and EM topologies. These methods follow a bottom-up approach, starting with fixed geometric templates and using trial-and-error optimization. Other than long design times, this limits innovation and accessible design spaces. Here, we present a series of approaches that introduce a fundamentally different strategy: a universal inverse design method that can generate complex electromagnetic structures and subsequently fabrication-ready RFIC design and layout from specifications. We present a reinforcement learning design framework that canvasses the space for RFIC topology, circuits and circuit parameters and then synthesizes the interface electromagnetic structures through a deep learning enabled inverse design framework. Collaboratively, this creates a specifications-to-GDS design and synthesis flow for RFICs. The designs that emerge through this are often non-traditional and non-intuitive, but can break many of the trade-offs of known topologies. We present fabricated and measured results in silicon ICs demonstrating the feasibility of the presented approaches.",
    "title_zh": "特邀论文：结合强化学习与逆向设计的端到端射频集成电路拓扑综合与设计",
    "abstract_zh": "与低频模拟设计不同，射频、毫米波及太赫兹频率芯片的设计需要电路与电磁（EM）之间复杂的协同设计过程。这两个领域相互紧密耦合，使得设计过程极为复杂。与电路拓扑结构类似，电磁拓扑结构也具有极高的多样性，从集总元件如电感、电容，到分布式的无源结构如传输线、天线，以及这些元素的任意组合，均可构成设计选项。从概念构思到原理图再到版图的设计流程往往耗时极长，严重依赖专家经验、人工干预以及对预定义电路和电磁拓扑结构的反复调优。这些传统方法采用自下而上的方式，从固定的几何模板出发，通过试错法进行优化。除了设计周期漫长之外，这种方法还限制了创新空间和可探索的设计范围。\n\n在此，我们提出一系列根本性不同的新策略：一种通用的逆向设计方法，能够根据给定规格生成复杂的电磁结构，并进一步生成可直接用于制造的射频集成电路（RFIC）设计与版图。我们构建了一个基于强化学习的设计框架，系统地探索射频集成电路的拓扑结构、电路形式及其参数空间，并通过深度学习驱动的逆向设计框架，合成相应的电磁接口结构。两者协同工作，形成从规格到GDS（版图数据格式）的完整RFIC设计与合成流程。由此产生的设计方案通常非传统且非直观，但能突破现有拓扑结构中的诸多性能权衡。我们展示了在硅基集成电路中实现并实测验证的结果，证明了所提方法的可行性。"
  },
  {
    "date": "2025-11-20",
    "title": "EqMap: FPGA LUT Remapping using E-Graphs",
    "authors": "Matthew Hofmann, Berk Gokmen, Zhiru Zhang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240672",
    "source": "IEEE",
    "abstract": "FPGA technology mapping is a well-studied problem and has been an area of interest in EDA tool design for decades. In most respects, the computational complexity of technology mapping is understood, and heuristic algorithms have been successfully employed to mitigate compile times. Even with an extensive body of research on technology mapping, our experiments show there is still substantial room for improvement in the quality of results. As a solution, we introduce EqMap, an e-graph driven compiler that can better span the wide gap between SAT-based exact synthesis and heuristic cut enumeration techniques. EqMap’s improvements to synthesis produced circuits with 12% fewer LUTs on average over the vendor tools—without ever increasing circuit depth. We also provide an empirical analysis of the runtime of EqMap and show that it is still practical for large designs. Finally, we demonstrate that our compiler infrastructure is reusable, and future work can use our compiler for RTL equivalence checking or auditing the QoR of synthesis tools.",
    "title_zh": "EqMap：基于E-图的FPGA LUT重映射",
    "abstract_zh": "FPGA技术映射是一个被广泛研究的问题，多年来一直是电子设计自动化（EDA）工具设计中的关注焦点。在大多数方面，技术映射的计算复杂性已得到充分理解，启发式算法也已被成功应用于缓解编译时间问题。尽管关于技术映射已有大量研究成果，但我们的实验表明，结果质量仍有显著提升空间。为此，我们提出了EqMap——一种基于等式图（e-graph）驱动的编译器，能够更有效地弥合基于SAT的精确综合与启发式切割枚举技术之间的巨大差距。EqMap在综合生成的电路中平均减少了12%的查找表（LUT），且从未增加电路的深度，优于厂商提供的工具。我们还对EqMap的运行时间进行了实证分析，证明其对于大规模设计依然具有实用性。最后，我们展示了该编译器基础设施的可重用性，未来工作可利用该编译器进行RTL等价性检查，或用于评估综合工具的性能质量（QoR）。"
  },
  {
    "date": "2025-11-20",
    "title": "COTIA: Concolic Testing with Intelligent Agent",
    "authors": "Yan Tan, Xiangchen Meng, Yangdi Lyu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240634",
    "source": "IEEE",
    "abstract": "Simulation plays a crucial role in the verification of hardware designs, ensuring that they behave correctly before fabrication. However, traditional simulation methods can be inefficient when dealing with complex designs, especially in corner cases. To mitigate this inefficiency, Concolic testing has emerged as a promising technique, utilizing symbolic execution to guide the simulation process. However, the heuristics used in path exploration for Concolic testing often struggle with local optima, resulting in suboptimal verification outcomes and incomplete coverage of the design space. In this paper, we propose an agent-based framework to dynamically adjust path exploration strategies by leveraging beam search and large language models (LLMs). Experimental results demonstrate that this approach significantly improves branch coverage, especially for hard-to-detect branches, while also optimizing the use of computational resources.",
    "title_zh": "COTIA：基于智能代理的混合测试",
    "abstract_zh": "仿真在硬件设计验证中起着至关重要的作用，能够确保设计在实际制造前行为正确。然而，传统的仿真方法在处理复杂设计时效率较低，尤其是在边缘情况方面。为缓解这一效率问题，结合符号执行来引导仿真过程的Concolic测试技术应运而生。然而，Concolic测试中用于路径探索的启发式策略常常陷入局部最优，导致验证效果不理想，并未能充分覆盖设计空间。本文提出一种基于代理的框架，通过引入束搜索（beam search）和大语言模型（LLMs），动态调整路径探索策略。实验结果表明，该方法显著提升了分支覆盖率，尤其在难以检测的分支上表现突出，同时有效优化了计算资源的使用。"
  },
  {
    "date": "2025-11-20",
    "title": "(Invited Paper) Overview of 2025 CAD Contest at ICCAD",
    "authors": "Chung-Kuan Cheng, Shao-Yun Fang, Yi-Yu Liu, Tsun-Ming Tseng",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240649",
    "source": "IEEE",
    "abstract": "The \"CAD Contest at ICCAD\" is a challenging, multi-month, research and development competition, focusing on advanced, real-world problems in the field of electronic design automation (EDA). Since 2012, the contest has been publishing many sophisticated circuit design problems, from system-level design to physical design, together with industrial benchmarks and solution evaluators. Contestants can participate in one or more problems provided by EDA/IC industry. The winners will be awarded at an ICCAD special session dedicated to this contest. Every year, the contest attracts more than a hundred teams, fosters productive industry-academia collaborations, and leads to hundreds of publications in top-tier conferences and journals. The 2025 CAD Contest has 247 teams from all over the world, which generates the highest participation record. Moreover, the problems of this year cover state-of-the-art EDA research trends such as hardware trojan detection, design optimization with multibit flip-flops, and performance-driven incremental placement optimization from well-known EDA/IC companies. We believe the contest keeps enhancing impact and boosting EDA researches.",
    "title_zh": "（特邀论文）2025年ICCAD CAD竞赛综述",
    "abstract_zh": "“ICCAD CAD竞赛”是一项具有挑战性的、为期数月的研究与开发竞赛，聚焦于电子设计自动化（EDA）领域中的先进且真实世界的问题。自2012年以来，该竞赛已发布众多复杂的电路设计课题，涵盖系统级设计到物理设计等多个层面，并提供工业界基准数据和解决方案评估工具。参赛者可选择参与EDA/IC产业界提供的一个或多个问题。优胜者将在ICCAD特别会议中获得表彰。每年，该竞赛吸引超过百支队伍参与，促进了产学研之间的有效合作，并催生了数百篇发表在顶级会议和期刊上的论文。2025年CAD竞赛吸引了来自全球的247支队伍，创下最高参与纪录。此外，今年的竞赛题目覆盖了当前EDA研究的前沿趋势，如硬件木马检测、基于多比特触发器的设计优化，以及来自知名EDA/IC企业的性能驱动型增量布局优化等课题。我们相信，该竞赛正持续提升其影响力，有力推动了EDA领域的研究发展。"
  },
  {
    "date": "2025-11-20",
    "title": "Promise: Property Mining for Sequential Synthesis",
    "authors": "Jiahui Xu, Jordi Cortadella, Lana Josipović",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240727",
    "source": "IEEE",
    "abstract": "Modularity—composing a large system using individually designed units—is an essential practice in hardware design. Yet, modularity might compromise quality: when individually designed units are put together, some of their states may become unreachable and, consequently, the logic that implements them is redundant. Sequential synthesis aims to remove redundant circuit logic by leveraging state unreachability. It critically depends on invariants—relations between signals and registers that hold in all reachable states—to prove the validity of redundancies. Yet, existing invariant generation techniques are mostly problem-specific (for a particular circuit or a property) or reliant on localized reasoning. We propose Promise, a fast circuit redundancy removal strategy. Promise exploits the rich information from simulation traces and uses efficient polynomial-time algorithms to infer global circuit invariants, optimizing the circuit and aiding other sequential synthesis procedures. Experiments show that Promise effectively optimizes circuits produced by high-level synthesis tools. Promise is open-sourced and available at github.com/ETHZ-DYNAMO/promise.",
    "title_zh": "承诺：面向序列合成的属性挖掘",
    "abstract_zh": "模块化——通过独立设计的单元组合成大型系统——是硬件设计中的基本实践。然而，模块化可能会影响系统质量：当各个独立设计的单元组合在一起时，某些状态可能变得不可达，从而导致其逻辑实现成为冗余。顺序综合技术通过利用状态不可达性来消除冗余电路逻辑，其关键依赖于不变量——在所有可达状态中始终成立的信号与寄存器之间的关系——以验证冗余的有效性。然而，现有的不变量生成方法大多针对特定问题（如特定电路或特定属性），或依赖局部推理。我们提出了Promise，一种快速的电路冗余消除策略。Promise充分利用仿真轨迹中的丰富信息，并采用高效的多项式时间算法推断全局电路不变量，从而优化电路结构，并辅助其他顺序综合流程。实验表明，Promise能够有效优化高层次综合工具生成的电路。Promise已开源，可在github.com/ETHZ-DYNAMO/promise获取。"
  },
  {
    "date": "2025-11-20",
    "title": "Optimizing SFQ Circuit Design: A Timing-Driven Framework for Performance-Constrained Area Minimization",
    "authors": "Robert S. Aviles, Rassul Bairamkulov, Ziyu Liu, Peter A. Beerel",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240800",
    "source": "IEEE",
    "abstract": "Single Flux Quantum (SFQ) digital logic offers a promising path to energy-efficient, high-performance computing, but faces significant scalability challenges–particularly due to the area overhead associated with gate-level path balancing and the pipelining of splitter trees. Prior efforts to reduce this overhead often unnecessarily compromise throughput, diminishing SFQ’s key performance advantage. To mitigate this, we propose a timing-aware optimization framework that unifies and enhances both traditional full path balancing (FPB) and multi-phase clocking approaches. Inspired by timing-driven EDA strategies in CMOS, our approach explicitly models gate delays and jointly optimizes clock phase assignments, pipelining, and splitter tree construction, minimizing area and latency overhead while achieving a given performance target. On benchmark circuits, our timing-aware 2-phase clocking achieves up to 30% area and 29% latency reductions over FPB at a 27ps clock period. Compared to prior multi-phase methods operating at 42ps, we achieve 22% lower area and 23% lower latency. At a tighter 21ps performance target, our timing-aware single-phase clocking improves area and latency by 28% and 7%, respectively. All optimizations are implemented using scalable, polynomial-time algorithms.",
    "title_zh": "优化SFQ电路设计：一种面向性能约束的面积最小化时序驱动框架",
    "abstract_zh": "单通量量子（SFQ）数字逻辑为实现能效高、性能优的计算提供了极具前景的路径，但其面临显著的可扩展性挑战——尤其是由于门级路径平衡以及分叉树流水线化带来的面积开销。以往降低该开销的努力往往不必要地牺牲了吞吐量，削弱了SFQ的核心性能优势。为缓解这一问题，我们提出了一种时序感知的优化框架，统一并增强了传统的全路径平衡（FPB）与多相时钟技术。受CMOS领域时序驱动EDA策略的启发，我们的方法显式建模门延迟，并联合优化时钟相位分配、流水线设计及分叉树构建，在满足特定性能目标的同时，最大限度地减少面积和延迟开销。在基准电路测试中，相较于FPB在27皮秒时钟周期下的表现，我们提出的时序感知双相时钟方案实现了最高达30%的面积缩减和29%的延迟降低；相比先前在42皮秒时钟周期下运行的多相方法，本方案实现22%的面积减少和23%的延迟降低。在更严格的21皮秒性能目标下，我们的时序感知单相时钟方案分别将面积和延迟优化了28%和7%。所有优化均采用可扩展、多项式时间复杂度的算法实现。"
  },
  {
    "date": "2025-11-20",
    "title": "Characterizing and Optimizing Real-Time Optimal Control for Embedded SoCs",
    "authors": "Shengjun Kris Dong, Dima Nikiforov, Widyadewi Soedarmadji, Minh Nguyen, Vikram Jain, Christopher W. Fletcher, Yakun Sophia Shao",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00047",
    "source": "IEEE",
    "abstract": "Resource-limited robots face significant challenges in executing computationally intensive tasks, such as locomotion and manipulation, particularly for real-time optimal control algorithms like Model Predictive Control (MPC). This paper provides a comprehensive design space exploration to identify optimal hardware computation architectures for these demanding model-based control algorithms. We profile and optimize representative architectural designs, including general-purpose scalar CPUs, vector processors, and specialized accelerators. By characterizing kernel-level benchmarks and end-to-end robotic scenarios, including a hardware-in-the-loop evaluation on a fabricated RISC-V multi-core vector SoC, we present a quantitative comparison of performance, area, and utilization across distinct architectural design points. Our findings demonstrate that targeted architectural modifications, coupled with deep software and system optimizations, enable up to 3.71x speedups for MPC, resulting in up to 27% system-level power reductions while completing robotic tasks. Finally, we propose a code generation flow designed to simplify the complex engineering effort required for mapping robotic workloads onto specialized architectures.",
    "title_zh": "面向嵌入式SoC的实时最优控制表征与优化",
    "abstract_zh": "资源受限的机器人在执行计算密集型任务（如运动控制和操作）时面临巨大挑战，尤其是在实时最优控制算法（如模型预测控制MPC）方面。本文通过全面的设计空间探索，旨在识别适用于这些高要求基于模型控制算法的最佳硬件计算架构。我们对具有代表性的架构设计进行了性能分析与优化，包括通用标量CPU、向量处理器以及专用加速器。通过内核级基准测试和端到端机器人应用场景的评估，包括在一款自研的RISC-V多核向量SoC上的硬件在环测试，我们对不同架构设计点在性能、面积和利用率方面的表现进行了定量比较。研究结果表明，经过针对性的架构改进以及深入的软硬件协同优化，MPC算法的执行速度最高可提升3.71倍，同时在完成机器人任务的过程中实现高达27%的系统级功耗降低。最后，本文提出了一种代码生成流程，旨在简化将机器人工作负载映射到专用架构所需的复杂工程工作。"
  },
  {
    "date": "2025-11-20",
    "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs",
    "authors": "Kimia Tasnia, Alexander Garcia, Tasnuva Farheen, Sazadur Rahman",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240771",
    "source": "IEEE",
    "abstract": "The rapid adoption of large language models (LLMs) in hardware design has primarily focused on generating functionally correct Verilog code, overlooking critical Power-Performance-Area (PPA) metrics essential for industrial-grade designs. To bridge this gap, we propose VeriOpt, a novel framework that leverages role-based prompting and PPA-aware optimization to enable LLMs to produce high-quality, synthesizable Verilog. VeriOpt structures LLM interactions into specialized roles (e.g., Planner, Programmer, Reviewer, Evaluator) to emulate human design workflows, while integrating PPA constraints directly into the prompting pipeline. By combining multi-modal feedback (e.g., synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves PPA-efficient code generation without sacrificing functional correctness. Experimental results demonstrate up to 88% reduction in power, 76% reduction in area and 73% improvement in timing closure compared to baseline LLM-generated RTL, validated using industry-standard EDA tools. At the same time achieves 86% success rate in functionality evaluation. Our work advances the state-of-the-art AI-driven hardware design by addressing the critical gap between correctness and quality, paving the way for reliable LLM adoption in production workflows.",
    "title_zh": "VeriOpt：通过多角色大语言模型实现PPA感知的高质量Verilog生成",
    "abstract_zh": "大型语言模型（LLMs）在硬件设计中的快速应用，主要集中在生成功能正确的Verilog代码上，而忽视了工业级设计中至关重要的功耗-性能-面积（PPA）指标。为弥合这一差距，我们提出了VeriOpt——一种新颖的框架，通过基于角色的提示策略与PPA感知优化，使LLMs能够生成高质量、可综合的Verilog代码。VeriOpt将LLM的交互过程结构化为多个专业化角色（如规划者、程序员、评审员、评估者），以模拟人类的设计工作流程，并将PPA约束直接融入提示生成管道。通过结合多模态反馈（如综合报告、时序图）与PPA感知提示，VeriOpt在不牺牲功能正确性的前提下，实现了高效的PPA优化。实验结果表明，相较于基线LLM生成的RTL代码，VeriOpt在功耗上最高降低88%，面积减少76%，时序收敛性提升73%，所有结果均通过行业标准EDA工具验证。同时，在功能正确性评估中达到了86%的成功率。本研究推动了AI驱动硬件设计的前沿发展，解决了功能正确性与设计质量之间的关键鸿沟，为LLM在生产级工作流中的可靠应用铺平了道路。"
  },
  {
    "date": "2025-11-20",
    "title": "3D Personalised Modeling of an Aortic Valve",
    "authors": "Denis Alekseev, Shamil Gaisin, Aikush Nazaryan, Ruslan Gumerov, Ilya Afanasyev, Alexandr Avsievich, Anton Ivaschenko, Andrey Kuzmin",
    "publish": "2025 Systems and Technologies of the Digital HealthCare (STDH)",
    "url": "https://doi.org/10.1109/stdh66836.2025.11227507",
    "source": "IEEE",
    "abstract": "Aortic valve pathologies are the most dangerious cardiovascular diseases worldwide. Aortic valve prostheses may help to decrease the mortality caused by the corresponding diseases. Currently the creation of personalized valve prostheses is time-and resource-consuming, since the technology for their development from collecting the necessary patient data to 3D printing of the prosthesis is not automated. The paper aims at the creation of 3D models of personalized aortic valve prostheses. The task of the research is to develop the program code for the automatic creation of a threedimensional model according to the specified parameters. The proposed solution includes usage of Blender 3.6 software environment for 3D modeling, special algorithm for mathematical calculations of the 3D model parameters. As a result, a script was developed using known analytical expressions. The script provides the user with the ability to set the parameters of a personalized model and automatically build or update models based on the entered data. The ability to export a file in stl format for subsequent printing on a 3D bioprinter is also implemented. The developed script for creating a three-dimensional model of a personalized aortic valve allows for increased modeling efficiency by reducing the time it takes to create an artificial valve for a patient according to the principles of personalized medicine.",
    "title_zh": "主动脉瓣的三维个性化建模",
    "abstract_zh": "主动脉瓣疾病是全球最危险的心血管疾病之一。主动脉瓣假体有助于降低相关疾病导致的死亡率。目前，个性化瓣膜假体的制作过程耗时且资源消耗大，因为从收集患者必要数据到3D打印假体的整个技术流程尚未实现自动化。本文旨在创建个性化的主动脉瓣假体三维模型。研究任务是开发程序代码，以根据指定参数自动创建三维模型。所提出的解决方案采用Blender 3.6软件环境进行三维建模，并结合专门设计的数学计算算法来确定三维模型的各项参数。最终，基于已知的解析表达式开发出一个脚本。该脚本使用户能够设定个性化模型的参数，并根据输入的数据自动构建或更新模型。同时，脚本还实现了将文件导出为STL格式的功能，以便后续在3D生物打印机上进行打印。所开发的个性化主动脉瓣三维模型生成脚本，通过减少根据个性化医疗原则为患者制作人工瓣膜所需的时间，显著提高了建模效率。"
  },
  {
    "date": "2025-11-20",
    "title": "DecoRTL: A Run-Time Decoding Framework for RTL Code Generation with LLMs",
    "authors": "Mohammad Akyash, Kimia Azar, Hadi Kamali",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240830",
    "source": "IEEE",
    "abstract": "As one of their many applications, large language models (LLMs) have recently shown promise in automating register transfer level (RTL) code generation. However, conventional LLM decoding strategies, originally designed for natural language, often fail to meet the structural and semantic demands of RTL, leading to hallucinated, repetitive, or invalid code outputs. In this paper, we first investigate the root causes of these decoding failures through an empirical analysis of token-level entropy during RTL generation. Our findings reveal that LLMs exhibit low confidence in regions of structural ambiguity or semantic complexity, showing that standard decoding strategies fail to differentiate between regions requiring determinism (syntax-critical regions) and those that benefit from creative exploratory variability (design-critical regions). Then, to overcome this, we introduce DecoRTL, a novel run-time decoding strategy, that is both syntax-aware and contrastive for RTL code generation. DecoRTL integrates two complementary components: (i) self-consistency sampling, which generates multiple candidates and re-ranks them based on token-level agreement to promote correctness while maintaining diversity; and (ii) syntax-aware temperature adaptation, which classifies tokens by their syntactical and functional roles and adjusts the sampling temperature accordingly, enforcing low temperature for syntax-critical tokens and higher temperature for exploratory ones. Our approach operates entirely at inference time without requiring any additional model fine-tuning. Through evaluations on multiple open-source LLMs using the VerilogEval benchmark, we demonstrate significant improvements in syntactic validity, functional correctness, and output diversity, while the execution overhead (performance overhead) is imperceptible <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>.",
    "title_zh": "DecoRTL：一种基于大语言模型的RTL代码生成运行时解码框架",
    "abstract_zh": "作为大型语言模型（LLMs）的众多应用之一，近年来其在自动注册传输级（RTL）代码生成方面展现出巨大潜力。然而，传统的LLM解码策略最初是为自然语言设计的，在面对RTL代码生成时，往往难以满足其结构和语义上的严格要求，导致生成结果出现幻觉、重复或无效代码等问题。本文通过在RTL生成过程中对词元级别熵的实证分析，首次深入探究了这些解码失败的根本原因。研究发现，当模型处于结构模糊或语义复杂的区域时，其置信度显著降低，而标准解码策略无法有效区分需要确定性输出的语法关键区域与需要创造性探索的架构关键区域。\n\n为解决上述问题，我们提出了一种全新的运行时解码策略——DecoRTL，该策略专为RTL代码生成而设计，具备语法感知性和对比性。DecoRTL集成了两个互补组件：(i) 自洽采样（self-consistency sampling），通过生成多个候选输出并基于词元级别的共识进行重新排序，以提升正确性的同时保持多样性；(ii) 语法感知温度调节（syntax-aware temperature adaptation），根据词元的语法和功能角色对其进行分类，并相应调整采样温度——对语法关键词元采用低温度以确保准确性，对探索性词元则使用较高温度以促进创新性。\n\n本方法完全在推理阶段运行，无需任何额外的模型微调。通过在多个开源LLM上使用VerilogEval基准进行评估，我们证明了DecoRTL在语法正确性、功能正确性和输出多样性方面均取得显著提升，且执行开销（性能开销）几乎可以忽略不计<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>。"
  },
  {
    "date": "2025-11-20",
    "title": "DynVec: An End-to-End Framework for Efficient Vector-Dataflow Execution",
    "authors": "Jiangnan Li, Xianfeng Cao, Kaixiang Zhu, Wenbo Yin, Lingli Wang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240863",
    "source": "IEEE",
    "abstract": "High-performance computing (HPC) and hardware acceleration increasingly rely on dataflow architectures to achieve scalable parallelism and efficiency. High-level synthesis (HLS) facilitates accelerator design from high-level programs, but conventional tools often require intrusive source-level modifications and struggle to optimize irregular workloads. Dynamically scheduled HLS frameworks offer a promising direction for addressing control flow divergence and memory irregularity by generating dataflow accelerators. However, they lack compile-time parallelism optimizations such as vectorization and incur significant hardware overhead. Moreover, modern compilers can generate vectorized code using memory access and computational patterns. Nevertheless, in programs with irregular control flow or data-dependent behavior, such patterns are unknown until runtime, limiting the effectiveness of static vectorization strategies.To address these challenges, we propose DynVec, a unified vector-dataflow framework that integrates dynamic scheduling and vectorization to exploit runtime parallelism beyond conventional models. We address the vectorization of irregular kernels through an MLIR-based context-aware vectorizer that effectively identifies vectorizable operations and, through dataflow scheduling, generates a vector-dataflow execution graph that explicitly models control flow constructs, data and control interfaces, and memory operations. DynVec encapsulates high-level elastic units designed with built-in vectorization support, allowing customizable and adaptive execution behavior. Our compiler preserves the structural hierarchy of the kernel by combining vector and scalar operations in a bottom-up, type-safe manner. Experiments show that our approach achieves significant speedup compared to state-of-the-art HLS implementations across various regular and irregular applications. Moreover, compared to hybrid accelerators that separately support dynamic parallelism and vectorization, DynVec delivers superior performance.",
    "title_zh": "DynVec：一种用于高效向量数据流执行的端到端框架",
    "abstract_zh": "高性能计算（HPC）与硬件加速正越来越多地依赖数据流架构，以实现可扩展的并行性与高效性。高层次综合（HLS）能够从高级程序中自动设计加速器，但传统工具通常需要对源代码进行侵入式修改，且难以优化不规则的工作负载。动态调度的HLS框架为解决控制流分歧和内存访问不规则问题提供了有前景的方向，可通过生成数据流加速器来应对挑战。然而，这类方法缺乏编译时的并行优化能力（如向量化），并带来显著的硬件开销。此外，现代编译器虽能利用内存访问模式和计算模式生成向量化代码，但在存在不规则控制流或数据依赖行为的程序中，这些模式直到运行时才可确定，从而限制了静态向量化策略的有效性。\n\n为应对上述挑战，我们提出 DynVec——一种统一的向量-数据流框架，将动态调度与向量化相结合，以在运行时挖掘超越传统模型的并行潜力。针对不规则内核的向量化问题，DynVec采用基于MLIR的上下文感知向量化器，能够有效识别可向量化的操作，并通过数据流调度生成显式建模控制流结构、数据与控制接口以及内存操作的向量-数据流执行图。DynVec封装了具备内置向量化支持的高层弹性单元，支持可定制、自适应的执行行为。我们的编译器通过自底向上、类型安全的方式融合向量与标量操作，保留了内核的结构层次。实验结果表明，与当前最先进的HLS实现相比，该方法在多种规则与不规则应用场景中均实现了显著的速度提升。此外，相较于需分别支持动态并行与向量化功能的混合型加速器，DynVec展现出更优越的性能表现。"
  },
  {
    "date": "2025-11-20",
    "title": "HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing",
    "authors": "Haochen Huang, Shuzhang Zhong, Zhe Zhang, Shuangchen Li, Dimin Niu, Hongzhong Zheng, Runsheng Wang, Meng Li",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240984",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures achieve superior model performance with reduced computation costs, but at the cost of high memory capacity and bandwidth requirements. Near-Memory Processing (NMP) accelerators that stack memory directly on the compute through hybrid bonding have demonstrated high bandwidth with high energy efficiency, becoming a promising architecture for MoE models. However, as NMP accelerators comprise distributed memory and computation, how to map the MoE computation directly determines the LLM inference efficiency. Existing parallel mapping strategies, including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from either high communication costs or unbalanced computation utilization, leading to inferior efficiency. The dynamic routing mechanism of MoE LLMs further aggravates the efficiency challenges. Therefore, in this paper, we propose HD-MoE to automatically optimize the MoE parallel computation across an NMP accelerator. HD-MoE features an offline automatic hybrid parallel mapping algorithm and an online dynamic scheduling strategy to reduce the communication costs while maximizing the computation utilization. With extensive experimental results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1× to 1.8× over TP, 1.1× to 1.5× over EP, and 1.0× to 1.4× over the baseline Hybrid TP-EP with Compute-Balanced parallelism strategies.",
    "title_zh": "HD-MoE：基于三维近内存处理的混合动态并行Mixture-of-Expert大语言模型",
    "abstract_zh": "具有混合专家（MoE）架构的大语言模型（LLMs）在降低计算成本的同时实现了卓越的模型性能，但其代价是带来了较高的内存容量和带宽需求。通过采用通过混合键合将内存直接堆叠在计算单元之上的近内存处理（NMP）加速器，已展现出高带宽与高能效的优势，成为支持MoE模型的有前景架构。然而，由于NMP加速器包含分布式的内存与计算资源，如何将MoE计算任务进行有效映射直接决定了大语言模型推理的效率。现有的并行映射策略，如张量并行（TP）和专家并行（EP），要么通信开销过高，要么计算负载不均衡，导致整体效率不佳。此外，MoE模型中动态路由机制进一步加剧了效率挑战。因此，本文提出HD-MoE，一种能够自动优化NMP加速器上MoE并行计算的方案。HD-MoE结合离线的自动混合并行映射算法与在线的动态调度策略，在降低通信开销的同时最大化计算资源利用率。大量实验结果表明，相较于TP，HD-MoE实现了1.1×至1.8×的加速；相较于EP，加速比为1.1×至1.5×；相较于采用计算均衡策略的基线混合并行方法（Hybrid TP-EP），加速比为1.0×至1.4×。"
  },
  {
    "date": "2025-11-20",
    "title": "Athena: A Plug-and-Play Advisor for Retrieval-Augmented Generation using VectorDB",
    "authors": "Ning Liang, Fabian Wenz, Jana Giceva, Lisa Wu Wills",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00013",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a popular technique for addressing several challenges of Large Language Model (LLM) systems, including static model knowledge, hallucination, and limited input sequence lengths. Although RAG mitigates common pitfalls of current LLM systems, its inherent heterogeneity and configurability introduce new challenges. The performance of RAG is crucial for meeting the high-throughput and low-latency demands of LLM services. Different components of RAG operate on different hardware platforms, and their complexity scales with the configurability and complexity of the rest of the system. For example, larger embeddings may enhance retrieval accuracy, but also increase the latency of embedding creation and indexing, thereby compromising the RAG system’s performance and energy consumption.Thus, a comprehensive characterization of an end-to-end RAG system becomes necessary. In this work, we build an end-to-end RAG benchmarking framework, Athena, that supports various embedding models, vector databases, index/search algorithms, and LLMs. By characterizing the system under various RAG settings built using Athena, we demystify RAG by identifying performance bottlenecks and quantifying the impact of each sub-component on overall system performance. In addition, the plug-and-play, open-sourced Athena framework is designed to assist future RAG research.",
    "title_zh": "阿提亚：基于向量数据库的即插即用式检索增强生成顾问",
    "abstract_zh": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为解决大型语言模型（LLM）系统诸多挑战的流行技术，包括模型知识静态化、幻觉问题以及输入序列长度受限等。尽管RAG有效缓解了当前LLM系统的常见缺陷，但其固有的异构性和可配置性也带来了新的挑战。RAG系统的性能对于满足LLM服务对高吞吐量和低延迟的严苛要求至关重要。RAG的不同组件运行在不同的硬件平台上，且其复杂度随着系统整体的可配置性和复杂性的增加而上升。例如，更大的嵌入向量虽然可能提升检索准确性，但也会增加嵌入生成与索引的延迟，从而影响RAG系统的整体性能和能耗。因此，对端到端RAG系统进行全面表征变得尤为必要。\n\n在本研究中，我们构建了一个名为Athena的端到端RAG基准测试框架，该框架支持多种嵌入模型、向量数据库、索引/搜索算法以及大语言模型。通过在Athena构建的不同RAG配置下对系统进行表征分析，我们揭示了RAG的内在机制，识别出关键性能瓶颈，并量化了各个子组件对整体系统性能的影响。此外，Athena作为一个即插即用、开源的框架，旨在为未来的RAG研究提供有力支持。"
  },
  {
    "date": "2025-11-20",
    "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection",
    "authors": "Damian Gnieciak, Tomasz Szandala",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3635168",
    "source": "IEEE",
    "abstract": "Modern software relies on a multitude of automated testing and quality assurance tools to prevent errors, bugs and potential vulnerabilities. This study sets out to provide a head-to-head, quantitative and qualitative evaluation of six automated approaches: three industry-standard rule-based static code-analysis tools (SonarQube, CodeQL and SnykCode) and three state-of-the-art large language models hosted on the GitHub Models platform (GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten real-world C# projects that embed 63 vulnerabilities across common categories such as SQL injection, hard-coded secrets and outdated dependencies, we measure classical detection accuracy (precision, recall, F-score), analysis latency, and the developer effort required to vet true positives. The language-based scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs’ advantage originates from superior recall, confirming an ability to reason across broader code contexts. However, this benefit comes with substantial trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language models mislocate issues at line-or-column granularity due to tokenisation artefacts. Overall, language models successfully rival traditional static analysers in finding real vulnerabilities. Still, their noisier output and imprecise localisation limit their standalone use in safety-critical audits. We therefore recommend a hybrid pipeline: employ language models early in development for broad, context-aware triage, while reserving deterministic rule-based scanners for high-assurance verification. The open benchmark and JSON-based result harness released with this paper lay a foundation for reproducible, practitioner-centric research into next-generation automated code security.",
    "title_zh": "大型语言模型与静态代码分析工具：漏洞检测的系统性基准评测",
    "abstract_zh": "现代软件依赖大量自动化测试与质量保证工具来预防错误、漏洞及潜在的安全风险。本研究对六种自动化方法进行了全面的定量与定性评估：三种行业标准的基于规则的静态代码分析工具（SonarQube、CodeQL 和 SnykCode），以及三种托管在 GitHub Models 平台上的前沿大型语言模型（GPT-4.1、Mistral Large 和 DeepSeek V3）。我们采用一套精心挑选的十个项目（均为 C# 编写），涵盖 SQL 注入、硬编码密钥、过时依赖等常见漏洞类别，共包含 63 个已知漏洞，从经典检测准确率（精确率、召回率、F1 分数）、分析延迟时间，以及开发者验证真实阳性结果所需的工作量等多个维度进行衡量。\n\n结果显示，基于语言模型的扫描器在平均 F1 分数上显著优于传统静态分析工具：分别为 0.797、0.753 和 0.750；而静态工具的得分则分别为 0.260、0.386 和 0.546。语言模型的优势主要源于更高的召回率，表明其具备在更广泛代码上下文中进行推理的能力。然而，这一优势也伴随着显著的代价：DeepSeek V3 的误报率最高，且所有语言模型均因分词机制的局限性，无法精确定位问题至行或列级别，导致定位精度不足。\n\n总体而言，语言模型在发现真实漏洞方面已能与传统静态分析工具相媲美。但其输出噪声较大、定位不精准，限制了其在高安全要求审计场景中的独立应用。因此，我们建议采用混合式工作流：在开发早期阶段使用语言模型进行广域、上下文感知的初步筛选，而在关键验证环节则保留确定性的规则驱动型扫描器以确保高可信度。\n\n本文发布的开放基准数据集和基于 JSON 格式的评测结果框架，为下一代自动化代码安全研究提供了可复现、面向实践者的坚实基础。"
  },
  {
    "date": "2025-11-20",
    "title": "Revolution or Hype? Seeking the Limits of Large Models in Hardware Design",
    "authors": "Qiang Xu, Leon Stok, Rolf Drechsler, Xi Wang, Grace Li Zhang, Igor L. Markov",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240750",
    "source": "IEEE",
    "abstract": "Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models (LCMs) have sparked excitement across the electronic design automation (EDA) community, promising a revolution in circuit design and optimization. Yet, this excitement is met with significant skepticism: Are these AI models a genuine revolution in circuit design, or a temporary wave of inflated expectations? This paper serves as a foundational text for the corresponding ICCAD 2025 panel, bringing together perspectives from leading experts in academia and industry. It critically examines the practical capabilities, fundamental limitations, and future prospects of large AI models in hardware design. The paper synthesizes the core arguments surrounding reliability, scalability, and interpretability, framing the debate on whether these models can meaningfully outperform or complement traditional EDA methods. The result is an authoritative overview offering fresh insights into one of today’s most contentious and impactful technology trends.",
    "title_zh": "革命还是炒作？探索大模型在硬件设计中的极限",
    "abstract_zh": "近年来，大型语言模型（LLMs）和大型电路模型（LCMs）的突破性进展在电子设计自动化（EDA）领域引发了广泛关注，预示着电路设计与优化将迎来一场革命。然而，这种热情背后也伴随着显著的质疑：这些AI模型究竟是电路设计领域真正的变革，还只是短暂的、被过度期待的热潮？本文作为2025年ICCAD会议专题讨论会的基础文献，汇集了来自学术界与产业界顶尖专家的多元视角，对大模型在硬件设计中的实际能力、根本局限以及未来前景进行了深入批判性分析。文章系统梳理了关于可靠性、可扩展性和可解释性的核心论点，围绕这些模型是否能够实质性超越或有效补充传统EDA方法展开讨论。最终，本文呈现了一份权威性综述，为当今最具争议且影响深远的技术趋势提供了全新的洞见。"
  },
  {
    "date": "2025-11-20",
    "title": "Adaptive Pin Pattern Modification on Standard Cells Towards ECO Routing",
    "authors": "Jaehoon Ahn, Sehyeon Chung, Taewhan Kim",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240901",
    "source": "IEEE",
    "abstract": "In deep-submicron technology nodes, I/O pin accessibility on the cells is crucial for successful net routing in physical design. For this reason, the conventional design flows have paid a considerable attention to acquiring the cell library with high pin accessibility to facilitate the net routing task. Nevertheless, the increase in routing failures is inherently unavoidable as the cell size shrinks with the progress of the technology node. In this context, this work proposes a new technique called adaptive pin-pattern modification (APM) to tackle this fundamental problem. Precisely, our proposed ECO-routing method based on adaptive pin-pattern modification makes use of three novel pin-pattern modifications on standard cells, which are referred to as pin-shift-trim (PST), pin-free (PF) and pin-bridge (PB). By applying these pin-pattern modifications adaptively and systematically to suit the circumstances to the individual cell instances with pin access failure, our ECO-router is able to explore the routable paths more extensively and effectively over the sequential, maze-routing based, ECO-routers but spend a much shorter time over the concurrent ECO-routers. Experimental results show that our proposed APM-enabled ECO-router resolves 17.7% of pin inaccessibility cases that a commercial tool has failed to find legal routes, with no penalty of chip PPA, and achieves over 116x speedup compared to the concurrent MCF (multi-commodity flow) based ECO-router.",
    "title_zh": "面向ECO布线的标准单元自适应引脚图案修改",
    "abstract_zh": "在深亚微米技术节点中，单元的I/O引脚可访问性对于物理设计中的布线成功至关重要。因此，传统设计流程一直高度重视获取具有高引脚可访问性的单元库，以促进布线任务的顺利进行。然而，随着技术节点的不断演进，单元尺寸持续缩小，布线失败不可避免地增加。针对这一根本性问题，本文提出了一种名为自适应引脚模式修改（Adaptive Pin-pattern Modification, APM）的新技术。具体而言，我们提出的基于APM的ECO布线方法，在标准单元上引入了三种新颖的引脚模式修改策略，分别为引脚移位修剪（Pin-Shift-Trim, PST）、无引脚（Pin-Free, PF）和引脚桥接（Pin-Bridge, PB）。通过根据每个存在引脚访问失败的单元实例的具体情况，自适应且系统性地应用这些引脚模式修改，我们的ECO路由器能够更广泛、更有效地探索可布线路径，相较于传统的基于顺序迷宫算法的ECO路由器，显著提升了布线能力；同时，与并行式ECO路由器相比，其运行时间大幅缩短。实验结果表明，本研究所提出的APM增强型ECO路由器成功解决了商业工具无法找到合法布线路径的17.7%引脚不可访问问题，且未对芯片的PPA（功耗、性能、面积）造成任何负面影响，相较基于多商品流（Multi-Commodity Flow, MCF）的并行ECO路由器实现了超过116倍的加速效果。"
  },
  {
    "date": "2025-11-20",
    "title": "ProtoCellLayout: Prototype-Guided Graph Learning for Accurate and Generalizable Standard Cell Layout PPA Estimation",
    "authors": "Zhiyuan Luo, Le Zhou, Zenghui Zhang, Jie Zhou, Zhien Li, Huiqing You, Feng Yao, Zhenyu Zhao",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240788",
    "source": "IEEE",
    "abstract": "As Moore’s Law approaches its physical limits, Design Technology Co-Optimization(DTCO) has become a critical pathway for advancing semiconductor process nodes. As a communication channel between design and technology, standard cell library design demands rapid and precise estimation for functionally identical cells with variant layouts. However, existing estimation methods face significant limitations: Geometry-based approaches using wirelength estimation, while efficient, lack accuracy and granularity for timing-arc analysis; Post-layout SPICE simulations are accurate but extremely time-consuming. Current machine learning methods confront two major challenges: Most approaches perform prediction solely at the pre-layout stage, neglecting layout-dependent effects; Model training requires massive simulation samples, incurring substantial computational and temporal overhead. To address these issues, this paper proposes ProtoCellLayout, a novel cell layout power, performance, and area(PPA) estimation framework. Core innovations include: 1) Systematically explores the representation methods of standard cell layouts and proposes a graph-based method named Topology-Geometry Graph (TGG), which integrates circuit topology and geometric features. By explicitly encoding transistor-level connections and metal layer geometries, the TGG method overcomes the limitations of image-based and semantic representations in modeling physical effects. 2) A prototype learning mechanism leveraging CMOS logic’s structural regularity and routing pattern similarity, enabling performance generalization across complex cells using minimal simple-cell training data. Experiments on an industrial library demonstrate that ProtoCellLayout achieves <3.5% percentage error while reducing estimation time from hours to under one minute for cell delay, leakage, internal power, and transition time. Its performance rankings align closely with SPICE simulation, achieving over 95% accuracy in layout selection.",
    "title_zh": "ProtoCellLayout：基于原型引导的图学习用于精确且可泛化的标准单元布局PPA估计",
    "abstract_zh": "随着摩尔定律逐渐逼近其物理极限，设计与工艺协同优化（Design Technology Co-Optimization, DTCO）已成为推动半导体工艺节点演进的关键路径。作为连接设计与工艺的通信通道，标准单元库的设计要求对功能等效但布局各异的单元实现快速且精确的性能估计。然而，现有估计方法面临显著局限：基于几何的方法（如线长估算）虽效率高，但在时序弧分析中缺乏精度和粒度；而后版图SPICE仿真虽然准确，却耗时极长。当前机器学习方法也面临两大挑战：多数方法仅在布局前阶段进行预测，忽略了布局依赖效应；模型训练需要大量仿真样本，带来巨大的计算与时间开销。\n\n为解决上述问题，本文提出ProtoCellLayout——一种新型的标准单元布局功耗、性能与面积（PPA）估计框架。其核心创新包括：1）系统性地探索标准单元布局的表示方法，提出一种基于图结构的表示方法——拓扑-几何图（Topology-Geometry Graph, TGG），该方法融合电路拓扑与几何特征。通过显式编码晶体管级连接关系及金属层几何形状，TGG克服了传统图像化与语义化表示在建模物理效应方面的不足。2）引入原型学习机制，利用CMOS逻辑的结构规律性与布线模式相似性，仅需少量简单单元的训练数据即可实现对复杂单元的性能泛化。\n\n在工业级标准单元库上的实验表明，ProtoCellLayout在单元延迟、漏电、内部功耗和转换时间等关键指标上均实现了低于3.5%的百分比误差，将原本需数小时的估计时间缩短至不到一分钟。其性能排序结果与SPICE仿真高度一致，在布局选择任务中准确率超过95%。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: 2025 ICCAD CAD Contest Problem A: Hardware Trojan Detection on Gate-Level Netlist",
    "authors": "Chung-Han Chou, Chih-Jen Jacky Hsu, Hung-Chun Chiu, Kai-Chiang Wu, Yu-Guang Chen, Zhuo Li",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240658",
    "source": "IEEE",
    "abstract": "The increasing reliance on third-party intellectual property (IP) cores in modern integrated circuit (IC) design has introduced significant security vulnerabilities, particularly the risk of Hardware Trojans (HTs). These malicious modifications can compromise system integrity, leading to data leakage, unauthorized access, or functional failures. Traditional detection methods often depend on the availability of a golden chip, which is not always feasible. This paper presents the 2025 ICCAD CAD Contest Problem A, which challenges participants to develop machine learning-based solutions for detecting HTs directly from gate-level netlists without requiring a golden reference. The problem is formulated with defined Trojan behaviors, input/output specifications, and evaluation metrics, including correctness and F1 score. The contest aims to foster innovation in HT detection by leveraging advanced data-driven techniques and scalable analysis frameworks.",
    "title_zh": "特邀论文：2025 ICCAD CAD 竞赛问题A：门级网表上的硬件木马检测",
    "abstract_zh": "现代集成电路（IC）设计对第三方知识产权（IP）核的依赖日益增加，这带来了重大的安全漏洞，尤其是硬件木马（HTs）的风险。这些恶意修改可能破坏系统完整性，导致数据泄露、未授权访问或功能失效。传统的检测方法通常依赖于“黄金芯片”（golden chip）的存在，但这一条件在实际中并不总是可行。本文介绍了2025年ICCAD CAD竞赛问题A，该问题要求参赛者开发基于机器学习的解决方案，直接从门级网表中检测硬件木马，而无需依赖黄金参考样本。该问题设定了明确的木马行为、输入/输出规范以及评估指标，包括正确率和F1分数。本次竞赛旨在通过先进的数据驱动技术与可扩展的分析框架，推动硬件木马检测领域的创新。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: APS: Open-Source Hardware-Software Co-Design Framework for Agile Processor Specialization",
    "authors": "Youwei Xiao, Yuyang Zou, Yansong Xu, Yuhao Luo, Yitian Sun, Chenyun Yin, Ruifan Xu, Renze Chen, Yun Liang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240817",
    "source": "IEEE",
    "abstract": "APS is an open-source framework for agile hardware-software co-design of domain-specific processors. It provides both hardware synthesis and compiler infrastructure to facilitate the development of instruction extensions (ISAXs) for application acceleration. The framework proposes a unified instruction extension interface for seamless integration with diverse RISC-V SoC ecosystems. Based on the unified interface, APS introduces a cross-level architecture description language (CADL) for comprehensive instruction behavior specification, which is translated into a dynamic pipeline architecture through its synthesis flow. Besides, APS’s compiler infrastructure introduces a pattern-matching engine for the automated utilization of ISAXs in general programs. It also incorporates bitwidth-aware vectorization that leverages operand bitwidth information to reduce the overhead of calling ISAXs. We conduct case studies across multiple workloads, including cryptography, machine learning, and digital signal processing. With fewer than 175 lines of ISAX description, APS achieves 2.29× to 14.99× speedup for each case study, demonstrating APS’s practical productivity and acceleration capability. Overall, APS offers a complete, end-to-end methodology that significantly reduces the development cycle of ISAXs, making agile processor specialization practical to the research and open-source hardware communities.",
    "title_zh": "特邀论文：APS：用于敏捷处理器专用化的开源软硬件协同设计框架",
    "abstract_zh": "APS 是一个开源框架，用于特定领域处理器的敏捷软硬件协同设计。它提供了硬件综合与编译器基础设施，以促进应用加速指令扩展（ISAX）的开发。该框架提出了一种统一的指令扩展接口，可无缝集成到多种 RISC-V 系统级芯片（SoC）生态中。基于这一统一接口，APS 引入了一种跨层次架构描述语言（CADL），用于全面规范指令行为，并通过其综合流程将其转换为动态流水线架构。此外，APS 的编译器基础设施包含一个模式匹配引擎，可自动在通用程序中利用 ISAX；同时，还引入了位宽感知的向量化技术，利用操作数的位宽信息，降低调用 ISAX 的开销。我们在多个工作负载上进行了案例研究，涵盖密码学、机器学习和数字信号处理等领域。仅需不到 175 行的 ISAX 描述，APS 在各项案例研究中均实现了 2.29× 至 14.99× 的加速比，充分展示了 APS 在实际生产力和加速能力方面的优势。总体而言，APS 提供了一套完整、端到端的方法论，显著缩短了 ISAX 的开发周期，使敏捷处理器定制化成为研究界和开源硬件社区切实可行的选择。"
  },
  {
    "date": "2025-11-20",
    "title": "DiffCCD: Differentiable Concurrent Clock and Data Optimization",
    "authors": "Yuhao Ji, Yuntao Lu, Zuodong Zhang, Zizheng Guo, Yibo Lin, Bei Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240631",
    "source": "IEEE",
    "abstract": "Timing optimization following clock tree synthesis (post-CTS) is a crucial step in very large scale integration (VLSI) physical design for achieving timing closure. During this stage, clock skew significantly impacts circuit timing performance, making useful skew optimization essential for enhancing design quality. However, traditional skew optimization methods face challenges due to their insufficient consideration of physical implementation constraints. To overcome these limitations, we propose a GPU-accelerated differentiable concurrent clock and data (CCD) optimization framework, which simultaneously optimizes clock skew and logic delays to enhance overall timing performance with the consideration of physical constraints. We implement the CCD optimization method as a step involving buffer sizing in the clock network and refining placement results. The key innovation of our approach lies in formulating a smooth and differentiable process for CCD optimization with a calibration mechanism to ensure accurate gradient computations. Additionally, we employ an alternating direction method of multipliers (ADMM)-based strategy to decompose the entire optimization problem into several manageable subproblems, effectively balancing timing optimization with physical implementation constraints. Experimental results on open-source industrial benchmarks demonstrate that our CCD optimization framework achieves superior timing closure compared to a baseline approach within an open-source physical design tool. Our method yields an average improvement of 22.4% in worst negative slack (WNS) and 45.0% in total negative slack (TNS), along with a 9.434× runtime speedup. To our knowledge, this is the first work to incorporate clock skew effects into gradient-based timing optimization.",
    "title_zh": "DiffCCD：可微分的并发时钟与数据优化",
    "abstract_zh": "时序优化在时钟树综合（Post-CTS）阶段是超大规模集成电路（VLSI）物理设计中实现时序收敛的关键步骤。在此阶段，时钟偏移对电路时序性能具有显著影响，因此进行有效的时钟偏移优化对于提升设计质量至关重要。然而，传统的时钟偏移优化方法由于未能充分考虑物理实现约束，面临诸多挑战。为克服这些局限性，本文提出一种基于GPU加速的可微分并行时钟与数据（CCD）优化框架，该框架能够同时优化时钟偏移和逻辑延迟，在考虑物理约束的前提下显著提升整体时序性能。我们将CCD优化方法集成到时钟网络中的缓冲器尺寸调整以及布局结果优化的流程中。本方法的核心创新在于构建了一个平滑且可微的CCD优化过程，并引入校准机制以确保梯度计算的准确性。此外，我们采用基于交替方向乘子法（ADMM）的策略，将整个优化问题分解为若干可管理的子问题，有效平衡了时序优化与物理实现约束之间的关系。在开源工业基准测试上的实验结果表明，相较于开源物理设计工具中的基线方法，所提出的CCD优化框架在时序收敛方面表现更优：平均提升了22.4%的最差负 slack（WNS）和45.0%的总负 slack（TNS），同时实现了9.434倍的运行速度提升。据我们所知，这是首个将时钟偏移效应融入基于梯度的时序优化中的工作。"
  },
  {
    "date": "2025-11-20",
    "title": "NetShield Lite: A Network Security Tool with Real-Time DoS Detection and AI-Enhanced Vulnerability Assessment",
    "authors": "Mihaela Blömer, Bogdan Gabriel Drăghici, Ovidiu Petru Stan",
    "publish": "2025 29th International Conference on System Theory, Control and Computing (ICSTCC)",
    "url": "https://doi.org/10.1109/icstcc66753.2025.11240263",
    "source": "IEEE",
    "abstract": "Network security has become increasingly important as cyber-attacks continue to evolve and threaten both personal and organizational systems. This paper presents NetShield Lite, a lightweight network security tool that provides vulnerability scanning, real-time DoS attack detection, and automated security hardening recommendations. The system integrates Python-based port scanning with nmap, packet analysis using Scapy, and local AI assistance through Ollama. NetShield Lite features a modular architecture combining command-line functionality with a web dashboard for real-time monitoring. The tool implements a novel timestamp-based approach for tracking DoS sessions and an adaptive hardening system based on user experience levels. Experimental results demonstrate effectiveness in detecting SYN flood, UDP flood, and ICMP flood attacks while maintaining low resource consumption. The system is suitable for both educational purposes and practical network security monitoring.",
    "title_zh": "NetShield Lite：一款具备实时拒绝服务攻击检测与人工智能增强型漏洞评估功能的网络安全部件",
    "abstract_zh": "随着网络攻击手段不断演变，网络安全的重要性日益凸显，对个人和组织系统构成持续威胁。本文提出了一款名为NetShield Lite的轻量级网络安全工具，该工具具备漏洞扫描、实时拒绝服务（DoS）攻击检测以及自动化安全加固建议功能。系统结合基于Python的端口扫描技术与nmap，利用Scapy进行数据包分析，并通过Ollama提供本地人工智能辅助。NetShield Lite采用模块化架构，兼具命令行操作功能与Web仪表板，支持实时监控。该工具创新性地采用基于时间戳的机制来追踪DoS会话，并设计了根据用户经验水平自适应调整的安全加固系统。实验结果表明，该系统在有效检测SYN洪水、UDP洪水和ICMP洪水攻击的同时，保持了较低的资源消耗。NetShield Lite适用于教育场景及实际网络环境中的安全监控需求。"
  },
  {
    "date": "2025-11-20",
    "title": "GradMap: A Gradient-Descent Approach to Simultaneous Technology Mapping, Buffer Insertion, and Gate Sizing",
    "authors": "Hsin-Ying Tsai, Chung-Kai Wu, Chih-Cheng Hsu, Jie-Hong R. Jiang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240773",
    "source": "IEEE",
    "abstract": "Technology mapping is an essential step bridging logic synthesis and physical design in the electronic design automation (EDA) flow. However, technology mapping algorithms often assume a simplified delay model and may not faithfully optimize the circuit under a realistic cost function involving area, delay, and power objectives. In this work, we introduce GradMap, a gradient-descent-based technology mapping framework that integrates a differentiable static timing analysis (STA) engine with a linear delay model for accurate delay estimation. Additionally, it enables simultaneous buffer insertion and gate sizing during technology mapping, providing a holistic optimization solution. Experimental results show that the new method achieves an average improvement of 7% in area-driven optimization, 33% in delay-driven optimization, and 25% in area-delay-product optimization compared to the ABC technology mapper. We note that although the gradient-descent method has significant runtime overhead compared to the highly efficient ABC mapper, it is highly parallelizable with GPU acceleration (in a way similar to the neural network training process), besides attaining high optimization quality far beyond ABC’s ability.",
    "title_zh": "GradMap：一种基于梯度下降的同步技术映射、缓冲插入与门尺寸优化方法",
    "abstract_zh": "技术映射是电子设计自动化（EDA）流程中连接逻辑综合与物理设计的一个关键步骤。然而，现有的技术映射算法通常基于简化的延迟模型，难以在涉及面积、延迟和功耗的现实成本函数下实现精准优化。本文提出GradMap，一种基于梯度下降的技术映射框架，该框架将可微分的静态时序分析（STA）引擎与线性延迟模型相结合，实现了精确的延迟估算。此外，该方法在技术映射过程中同时支持缓冲器插入与门单元尺寸优化，提供了一种全面的优化解决方案。实验结果表明，相较于ABC技术映射器，该新方法在面积驱动优化中平均提升7%，在延迟驱动优化中提升33%，在面积-延迟乘积优化中提升25%。尽管与高度高效的ABC映射器相比，梯度下降方法存在显著的运行时间开销，但其具有高度并行化特性，可通过GPU加速（类似于神经网络训练过程），且能够实现远超ABC能力的高质量优化效果。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: 2025 ICCAD CAD Contest Problem B: Power and Timing Optimization Using Multibit Flip-Flop",
    "authors": "Sheng-Wei Yang, Jhih-Wei Hsu, Yu-Hsuan Cheng, Chin-Fang Cindy Shen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240779",
    "source": "IEEE",
    "abstract": "Contemporary semiconductor fabrication nodes present escalating challenges in achieving optimal power-performance-area (PPA) trade-offs, necessitating sophisticated optimization methodologies for digital circuit design. The 2025 ICCAD CAD Contest Problem B [1] introduces significant advances in multibit flip-flop optimization research, establishing a comprehensive benchmarking framework that bridges theoretical algorithm development with practical industrial implementation requirements. This enhanced contest platform, building upon the foundational work of the 2024 iteration [2], delivers unprecedented contributions to the electronic design automation (EDA) research community through mandatory operation traceability protocols, industry-standard LEF/DEF format integration, and enhanced computational resource allocation (16-core processing capability). Our primary research contribution establishes a rigorous validation infrastructure that enables comprehensive algorithmic transparency while addressing real-world optimization challenges encountered in production semiconductor design flows. The framework introduces innovative research enablers including complete transformation audit trails, cross-platform validation compatibility, and systematic performance evaluation under authentic design constraints. Through strategic banking and debanking optimization techniques, research participants engage with fundamental circuit optimization trade-offs while contributing to the advancement of multibit flip-flop optimization science. The enhanced 2025 framework provides the global research community with unprecedented insights into algorithm behavior patterns, transformation correctness verification methodologies, and scalable performance characteristics—essential foundations for advancing state-of-the-art multibit flip-flop optimization research.",
    "title_zh": "特邀论文：2025 ICCAD CAD 竞赛问题 B：基于多比特触发器的功耗与时序优化",
    "abstract_zh": "当代半导体制造工艺节点在实现最优功耗-性能-面积（PPA）权衡方面面临日益严峻的挑战，这要求数字电路设计采用更为复杂的优化方法。2025年ICCAD CAD竞赛问题B [1] 在多比特寄存器优化研究领域取得了重要进展，建立了一个全面的基准测试框架，有效衔接了理论算法研发与实际工业应用需求之间的鸿沟。该增强版竞赛平台在2024年版本[2]的基础上进一步发展，通过强制性的操作可追溯性协议、行业标准LEF/DEF格式集成以及提升的计算资源分配（支持16核处理能力），为电子设计自动化（EDA）研究领域做出了前所未有的贡献。我们主要的研究成果构建了一个严谨的验证基础设施，不仅实现了算法透明度的全面保障，也切实应对了生产级半导体设计流程中所面临的现实优化难题。该框架引入了一系列创新性的研究支持机制，包括完整的转换审计追踪、跨平台验证兼容性，以及在真实设计约束条件下的系统化性能评估。通过战略性地运用银行化与去银行化优化技术，参赛研究人员得以深入探索基础电路优化中的权衡关系，同时推动多比特寄存器优化科学的进步。2025年增强版框架为全球研究社区提供了对算法行为模式、转换正确性验证方法及可扩展性能特征的前所未有的洞察，这些构成了推进前沿多比特寄存器优化研究的关键基础。"
  },
  {
    "date": "2025-11-20",
    "title": "DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits",
    "authors": "Chengjie Liu, Jiajia Li, Yabing Feng, Wenhao Huang, Weiyu Chen, Yuan Du, Jun Yang, Li Du",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240784",
    "source": "IEEE",
    "abstract": "Analog circuit design consists of the pre-layout and layout phases. Among them, the pre-layout phase directly decides the final circuit performance, but heavily depends on experienced engineers to do manual design according to specific application scenarios. To overcome these challenges and automate the analog circuit pre-layout design phase, we introduce DiffCkt: a diffusion model-based hybrid neural network framework for the automatic transistor-level generation of analog circuits, which can directly generate corresponding circuit structures and device parameters tailored to specific performance requirements. To more accurately quantify the efficiency of circuits generated by DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which is determined by both the figure of merit (FOM) of a single generated circuit and the time consumed. Compared with relative research, DiffCkt has improved CGEI by a factor of 2.21 ~ 8365× , reaching a state-of-the-art (SOTA) level. In conclusion, this work shows that the diffusion model has the remarkable ability to learn and generate analog circuit structures and device parameters, providing a revolutionary method for automating the pre-layout design of analog circuits. The circuit dataset is now available at https://github.com/CjLiu-NJU/DiffCkt.",
    "title_zh": "DiffCkt：一种基于扩散模型的混合神经网络框架，用于模拟电路的自动晶体管级生成",
    "abstract_zh": "模拟电路设计包括预布局（pre-layout）和布局（layout）两个阶段。其中，预布局阶段直接决定了最终电路的性能，但目前高度依赖经验丰富的工程师根据具体应用场景进行手动设计，存在效率低、周期长等问题。为克服上述挑战并实现模拟电路预布局设计的自动化，本文提出DiffCkt：一种基于扩散模型的混合神经网络框架，可自动完成晶体管级模拟电路的生成，能够直接根据特定性能需求生成对应的电路结构与器件参数。为了更准确地衡量DiffCkt生成电路的效率，我们引入了电路生成效率指数（Circuit Generation Efficiency Index, CGEI），该指标综合考虑单个生成电路的优值因子（FOM）以及生成所耗时间。与现有相关研究相比，DiffCkt在CGEI上提升了2.21至8365倍，达到了当前最先进的水平（SOTA）。综上所述，本工作表明扩散模型在学习和生成模拟电路结构及器件参数方面具有卓越能力，为模拟电路预布局设计的自动化提供了革命性的方法。相关电路数据集现已开源，地址为：https://github.com/CjLiu-NJU/DiffCkt。"
  },
  {
    "date": "2025-11-20",
    "title": "PPAAS: PVT and Pareto Aware Analog Sizing via Goal-conditioned Reinforcement Learning",
    "authors": "Seunggeun Kim, Ziyi Wang, Sungyoung Lee, Youngmin Oh, Hanqing Zhu, Doyun Kim, David Z. Pan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240795",
    "source": "IEEE",
    "abstract": "Device sizing is a critical yet challenging step in analog and mixed-signal circuit design, requiring careful optimization to meet diverse performance specifications. This challenge is further amplified under process, voltage, and temperature (PVT) variations, which cause circuit behavior to shift across different corners. While reinforcement learning (RL) has shown promise in automating sizing for fixed targets, training a generalized policy that can adapt to a wide range of design specifications under PVT variations requires much more training samples and resources. To address these challenges, we propose a Goal-conditioned RL framework that enables efficient policy training for analog device sizing across PVT corners, with strong generalization capability. To improve sample efficiency, we introduce Pareto-front Dominance Goal Sampling, which constructs an automatic curriculum by sampling goals from the Pareto frontier of previously achieved goals. This strategy is further enhanced by integrating Conservative Hindsight Experience Replay to stabilize training and accelerate convergence. To reduce simulation overhead, our framework incorporates a Skip-on-Fail simulation strategy. Experiments on benchmark circuits demonstrate ∼1.6× improvement in sample efficiency and ∼4.1× improvement in simulation efficiency compared to existing sizing methods. Code and benchmarks are publicly available HERE.",
    "title_zh": "PPAAS：基于目标条件强化学习的PVT与帕累托感知模拟电路尺寸优化",
    "abstract_zh": "器件尺寸优化是模拟及混合信号电路设计中一个关键且极具挑战性的步骤，需要精心优化以满足多样化的性能指标。这一挑战在工艺、电压和温度（PVT）变化的影响下进一步加剧，导致电路行为在不同工作角落之间发生显著偏移。尽管强化学习（RL）在固定目标的器件尺寸自动化方面已展现出潜力，但要训练出能够适应广泛设计规格并在PVT变化下保持良好泛化能力的通用策略，仍需大量训练样本与计算资源。为应对这些挑战，我们提出了一种目标条件化的强化学习框架，可高效训练适用于各类PVT工作角落的模拟器件尺寸优化策略，并具备出色的泛化能力。为提升样本效率，我们引入了“帕累托前沿主导目标采样”方法，通过从先前达成目标的帕累托前沿中自动采样目标，构建自适应的学习课程。该策略进一步结合保守性事后经验回放（Conservative Hindsight Experience Replay），有效稳定训练过程并加速收敛。此外，为降低仿真开销，我们的框架采用“失败跳过”仿真策略。在基准电路上的实验表明，相较于现有尺寸优化方法，本方案实现了约1.6倍的样本效率提升和约4.1倍的仿真效率提升。代码与基准数据集已公开，详见此处。"
  },
  {
    "date": "2025-11-20",
    "title": "Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation",
    "authors": "Ruiyang Ma, Daikang Kuang, Ziqian Liu, Jiaxi Zhang, Ping Fan, Guojie Luo",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240909",
    "source": "IEEE",
    "abstract": "Debugging hardware designs requires significant manual effort during hardware development. After engineers identify a bug-triggering test case in simulation-based hardware verification, they usually spend considerable time analyzing the execution trace to localize the bug. Although numerous automated hardware debugging techniques exist, they are not applicable to large designs and deep bugs. A primary reason for their limitations is that these techniques only utilize the information of a single bug-triggering test case for bug localization, which prevents them from effectively analyzing intricate hardware systems and figure out the root cause of bugs. To solve this problem, in this paper, we transform the hardware bug localization problem into a test generation problem, aiming to find a set of effective witness test cases beyond the initial bug-triggering test case to enhance hardware bug localization. Witness test cases refer to the cases that do not trigger the bug in the faulty design. By analyzing the execution differences between passing and failing test cases with spectrum-based method, we can eliminate innocent design statements and localize the buggy ones. To further refine the suspicious area, we define the criteria for effective witness test cases and use a mutation-based strategy to generate such test cases. Based on this approach, we propose an automated hardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs from various hardware designs. The experimental results show that Wit- effectively localize 49% / 73% / 88% bugs within Top-1 / Top-5 / Top-10 ranks, significantly outperforming state-of-the-art bug localization techniques. Additionally, we evaluate Wit-HW on 13 real-world bugs collected from open-source hardware projects, showcasing the robust performance of our method.",
    "title_zh": "Wit-HW：通过见证测试用例生成实现硬件设计代码中的缺陷定位",
    "abstract_zh": "硬件设计的调试在硬件开发过程中需要投入大量的人工努力。当工程师在基于仿真的硬件验证中发现导致错误的测试用例后，通常需要花费大量时间分析执行轨迹以定位问题所在。尽管已存在众多自动化的硬件调试技术，但这些技术对大型设计和深层缺陷并不适用。其主要局限性在于：这些方法仅利用单一触发错误的测试用例的信息进行错误定位，无法有效分析复杂的硬件系统，难以找出错误的根本原因。为解决这一问题，本文将硬件错误定位问题转化为测试生成问题，旨在寻找一组超越初始触发错误测试用例的有效“见证测试用例”，以增强硬件错误定位能力。“见证测试用例”是指在存在缺陷的设计中不会触发错误的测试用例。通过使用基于谱的方法分析通过与失败测试用例之间的执行差异，可以排除无关的设计语句，从而精确定位出有缺陷的代码段。为进一步缩小可疑区域，我们定义了有效见证测试用例的标准，并采用基于变异的策略来生成此类测试用例。基于上述思路，我们提出了一种名为Wit-HW的自动化硬件错误定位框架。我们在来自不同硬件设计的41个错误上评估了Wit-HW的表现，实验结果表明，Wit-HW能够在Top-1 / Top-5 / Top-10排名内分别有效定位49% / 73% / 88%的错误，显著优于当前最先进的错误定位技术。此外，我们还在从开源硬件项目中收集的13个真实世界错误上对Wit-HW进行了评估，充分展示了该方法的鲁棒性能。"
  },
  {
    "date": "2025-11-20",
    "title": "VeriRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning",
    "authors": "Fu Teng, Miao Pan, Xuhong Zhang, Zhezhi He, Yiyao Yang, Xinyi Chai, Mengnan Qi, Liqiang Lu, Jianwei Yin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11241003",
    "source": "IEEE",
    "abstract": "Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeekstyle approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VeriRL is publicly available at https://github.com/omniAI-Lab/VeriRL.",
    "title_zh": "VeriRL：通过强化学习提升基于大语言模型的Verilog代码生成",
    "abstract_zh": "近年来，代码生成技术在软件领域取得了显著进展，但硬件描述语言（HDL）如Verilog由于其并发语义、语法严格性以及仿真复杂性，仍鲜有深入研究。本文针对这些挑战，提出了一种专为Verilog代码生成设计的强化学习（RL）框架。我们首先构建了Veribench-53K——一个从超过70万道Verilog问题中精心筛选出的高质量数据集，该数据集包含结构化提示、复杂度标签以及多样化的测试平台。为解决奖励信号稀疏且噪声大的问题，我们提出一种基于回溯重评分（Trace-back based Rescore）的机制，通过利用推理路径和迭代优化来提升反馈可靠性，并支持奖励模型的训练。此外，为缓解强化学习微调过程中灾难性遗忘与过拟合的问题，我们引入了一种样本平衡加权策略，根据奖励概率分布自适应地调节学习动态。上述创新被整合进一个迭代式强化学习流水线中，实现策略与奖励模型的协同进化。与近期依赖大规模闭源模型蒸馏的CraftRTL方法，以及在稀疏反馈下表现不佳的DeepSeek-style方法相比，我们的方法仅使用更小但质量更高的数据集，结合强化学习优化，展现出更优的性能。在Verilog生成任务上的实验表明，本方法在测试通过率、功能正确性和编译鲁棒性方面均达到当前最优水平。研究结果凸显了强化学习驱动方法在面向硬件领域的结构化代码生成中的巨大潜力。VeriRL项目已开源，访问地址为：https://github.com/omniAI-Lab/VeriRL。"
  },
  {
    "date": "2025-11-20",
    "title": "Keeping up with Large Language Models: A Holistic Methodology of Compute, Memory, Communication, and Cost Modeling",
    "authors": "Wenzhe Guo, Joyjit Kundu, Uras Tos, Weijiang Kong, Giuliano Sisto, Timon Evenblij, Manu Perumkunnil",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00019",
    "source": "IEEE",
    "abstract": "Large language models (LLMs), based on transformer architectures, have revolutionized numerous domains within artificial intelligence, science, and engineering due to their exceptional scalability and adaptability. However, the exponential growth in LLM size and complexity has outpaced advancements in compute capacity, memory bandwidth, network performance, and cost efficiency, posing significant challenges to their scalability on distributed systems. To address these limitations, alternative model architectures, optimization strategies, communication-aware network topologies, and novel system design approaches have been proposed in literature. This paper introduces a performance-cost modeling methodology for LLM training and inference that integrates state-of-the-art compute techniques with memory optimizations, and latest communication techniques. Building on an analytical performance model, our approach incorporates recent innovations such as the flash attention technique and mixture of experts models to address the memory bandwidth and compute bottlenecks. It also considers the impact of different network topologies and topology-specific communication algorithms with 5D parallellism. The framework also integrates a chiplet cost model. The proposed modeling methodology provides valuable insights to guide future compute system design and facilitates hardware-software co-development, in particular due to its ability to analyze performance-cost trade-offs for various system architectural configurations.",
    "title_zh": "跟上大语言模型的发展：一种涵盖计算、内存、通信和成本建模的综合方法",
    "abstract_zh": "大规模语言模型（LLMs）基于Transformer架构，在人工智能、科学和工程等多个领域实现了革命性突破，这得益于其卓越的可扩展性和适应性。然而，随着LLM规模与复杂性的指数级增长，其发展速度已远远超过计算能力、内存带宽、网络性能以及成本效率的提升，给分布式系统中的可扩展性带来了严峻挑战。为应对这些限制，学术界提出了多种替代模型架构、优化策略、通信感知的网络拓扑结构以及创新的系统设计方法。本文提出了一种针对LLM训练与推理的性能-成本建模方法，该方法融合了最先进的计算技术、内存优化手段以及最新的通信技术。在基于分析型性能模型的基础上，我们的方法引入了诸如Flash Attention技术和混合专家（Mixture of Experts）模型等最新进展，以有效缓解内存带宽和计算瓶颈问题。同时，该框架还考虑了不同网络拓扑结构及其对应的拓扑专用通信算法，并结合五维并行（5D parallelism）进行分析。此外，该框架集成了芯片小片（chiplet）成本模型。所提出的建模方法为未来计算系统的设计提供了重要洞见，促进了软硬件协同开发，尤其体现在能够对各类系统架构配置下的性能-成本权衡进行深入分析。"
  },
  {
    "date": "2025-11-20",
    "title": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal Circuit Encoder-Decoder Alignment",
    "authors": "Wenji Fang, Wang Jing, Yao Lu, Shang Liu, Zhiyao Xie",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240762",
    "source": "IEEE",
    "abstract": "The success of foundation AI has motivated the research of circuit foundation models, which are customized to assist the integrated circuit (IC) design process. However, existing pre-trained circuit foundation models are typically limited to standalone encoders for predictive tasks or decoders for generative tasks. These two model types are developed independently, operate on different circuit modalities, and reside in separate latent spaces. This restricts their ability to complement each other for more advanced capabilities. In this work, we present GenEDA, the first framework that cross-modally aligns circuit encoders with decoders within a shared latent space. GenEDA bridges the gap between graph-based circuit representation learning and text-based large language models (LLMs), enabling communication between their respective latent spaces. To achieve the alignment, we propose two paradigms to support both open-source trainable LLMs and commercial frozen LLMs. We leverage this aligned architecture to develop the first generative foundation model for netlists, unleashing LLMs’ generative reasoning capability on the low-level and bit-blasted netlists. GenEDA enables three unprecedented generative netlist functional reasoning tasks, where it reversely generates high-level functionalities such as specifications and RTL code from low-level netlists. These tasks move beyond traditional gate function classification to direct generation of full-circuit functionality. Experiments demonstrate that GenEDA significantly boosts advanced LLMs’ (e.g., GPT and DeepSeek series) performance in all tasks.",
    "title_zh": "GenEDA：通过跨模态电路编码器-解码器对齐实现生成式网表功能推理",
    "abstract_zh": "基础人工智能的成功激发了电路基础模型的研究，这类模型专为辅助集成电路（IC）设计流程而定制。然而，现有的预训练电路基础模型通常仅限于独立的编码器用于预测任务或解码器用于生成任务。这两种模型类型是独立开发的，作用于不同的电路模态，并存在于各自分离的潜在空间中，这限制了它们在实现更高级功能时相互补充的能力。在本工作中，我们提出了GenEDA，这是首个在共享潜在空间内跨模态对齐电路编码器与解码器的框架。GenEDA弥合了基于图的电路表示学习与基于文本的大语言模型（LLMs）之间的鸿沟，实现了二者潜在空间之间的通信。为了实现这种对齐，我们提出了两种范式，分别支持开源可训练的LLM和商用冻结的LLM。借助这一对齐架构，我们开发出首个针对网表（netlists）的生成式基础模型，使大语言模型能够在低层次、位展开的网表上发挥其生成推理能力。GenEDA实现了三项前所未有的生成式网表功能推理任务：它能够从低层次网表逆向生成高层次功能，如规格说明和RTL代码。这些任务超越了传统的门级功能分类，直接生成完整电路的功能描述。实验表明，GenEDA显著提升了先进大语言模型（如GPT和DeepSeek系列）在所有任务中的表现。"
  },
  {
    "date": "2025-11-20",
    "title": "Differentiable Timing-Driven FPGA Placement with Smooth Optimization and ML-Based Delay Calibration",
    "authors": "Yu-Kang Lin, Zhili Xiong, David Z. Pan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240642",
    "source": "IEEE",
    "abstract": "Placement is a critical stage in FPGA physical design, determining instance locations within available device resources. FPGA timing-driven placement aims to minimize routed wirelength while optimizing timing metrics such as total negative slack (TNS) and worst negative slack (WNS). In this work, we propose a differentiable timing-driven FPGA placement framework, enabling effective optimization convergence. We introduce a novel timing preconditioning method that smooths the timing objective landscape, along with a general multiplier scheduling scheme to effectively balance multiple objectives, including wirelength, WNS, and TNS. To further improve timing estimation during placement, we develop an XGBoost-based net delay prediction model to calibrate the timing model. Experiments on the ISPD 2016 contest benchmarks demonstrate that our placer achieves a 4% reduction in critical path delay, a 19% improvement in half-perimeter wirelength, and maintains a similar total place-and-route runtime (0.99 ×) relative to a state-of-the-art GPU-accelerated timing-driven FPGA placer.",
    "title_zh": "基于平滑优化与机器学习延迟校准的可微分时序驱动FPGA布局",
    "abstract_zh": "布局是FPGA物理设计中的关键阶段，决定了实例在可用器件资源中的具体位置。FPGA时序驱动布局的目标是在最小化布线长度的同时优化时序指标，如总负松弛（TNS）和最差负松弛（WNS）。本文提出了一种可微分的时序驱动FPGA布局框架，能够实现有效的优化收敛。我们引入了一种新颖的时序预处理方法，以平滑时序目标函数的优化景观，并设计了一种通用的乘子调度方案，有效平衡多个目标，包括布线长度、WNS和TNS。为进一步提升布局过程中的时序估计精度，我们开发了一种基于XGBoost的网络延迟预测模型，用于校准时序模型。在ISPD 2016竞赛基准上的实验结果表明，我们的布局器实现了关键路径延迟降低4%，半周长布线长度改善19%，同时保持与当前最先进的GPU加速时序驱动FPGA布局器相近的总体布局-布线运行时间（0.99倍）。"
  },
  {
    "date": "2025-11-20",
    "title": "ProFuse: Test Case Prioritization Based on Multi Dimensional Feature Fusion for Logic Synthesis Tools Testing Acceleration",
    "authors": "Peiyu Zou, Xiaochen Li, Xu Zhao, Shikai Guo, Zhide Zhou, Yue Ma, He Jiang",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3634318",
    "source": "IEEE",
    "abstract": "Logic synthesis tools translate Hardware Description Language (HDL) designs into hardware implementation. To test these tools, numerous test cases are usually executed on the tools, yet only a few of them can trigger faults, leading to inefficient testing. Since executing test cases on logic synthesis tools often requires significant cost on complicated synthesis and simulation, fault-triggering test cases should be prioritized to execute. However, existing prioritization methods face challenges in accurately predicting the fault-triggering capability of dynamically generated test cases and modeling the unique syntactic and structure complexities of these HDL-based programs. <p xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Therefore, we propose ProFuse, a multi-dimensional feature fusion method for logic synthesis tool test case prioritization. ProFuse leverages Abstract Syntax Trees (AST) and Data Flow Graphs (DFG) to extract novel syntactic and structure features from HDL designs. These features are processed by a joint model of Multilayer Perceptron (MLP) and Graph Convolutional Network (GCN) to rank fault-triggering test cases accurately. ProFuse achieves an Average Percentage of Fault Detection (APFD) score of 0.9285, outperforming the state-of-the-art prioritization methods by 11.38% to 82.49%. ProFuse can efficiently rank randomly generated test cases to discover 15 new faults in logic synthesis tools (i.e., Yosys and Vivado). The Vivado community acknowledged our work for improving their tool.</p>",
    "title_zh": "ProFuse：基于多维特征融合的测试用例优先级划分方法，用于加速逻辑综合工具的测试",
    "abstract_zh": "逻辑综合工具可将硬件描述语言（HDL）设计转换为硬件实现。为了测试这些工具，通常需要执行大量测试用例，但其中仅有少数能够触发故障，导致测试效率低下。由于在逻辑综合工具上执行测试用例往往需要耗费大量成本，尤其是在复杂的综合与仿真过程中，因此应优先执行那些具有故障触发能力的测试用例。然而，现有的优先级排序方法在准确预测动态生成测试用例的故障触发能力方面面临挑战，同时难以建模基于HDL程序独特的语法和结构复杂性。\n\n为此，我们提出了ProFuse——一种用于逻辑综合工具测试用例优先级排序的多维特征融合方法。ProFuse利用抽象语法树（AST）和数据流图（DFG），从HDL设计中提取新颖的语法与结构特征。这些特征通过多层感知机（MLP）与图卷积网络（GCN）的联合模型进行处理，从而精确地对可能触发故障的测试用例进行排序。实验结果表明，ProFuse在平均故障检测百分比（APFD）指标上达到0.9285，相较于当前最先进的优先级排序方法提升了11.38%至82.49%。ProFuse能够高效地对随机生成的测试用例进行排序，并成功在逻辑综合工具（Yosys和Vivado）中发现了15个新的缺陷。Vivado社区已认可我们的工作，认为其显著提升了该工具的质量与可靠性。"
  },
  {
    "date": "2025-11-20",
    "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems",
    "authors": "Davide Zoni, Andrea Galimberti, Adriano Guarisco",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240659",
    "source": "IEEE",
    "abstract": "Designing and validating efficient cache-coherent memory subsystems is a critical yet complex task in the development of modern multi-core system-on-chip architectures. Rhea is a unified framework that streamlines the design and system-level validation of RTL cache-coherent memory subsystems. On the design side, Rhea generates synthesizable, highly configurable RTL supporting various architectural parameters. On the validation side, Rhea integrates Verilator’s cycle-accurate RTL simulation with gem5’s full-system simulation, allowing realistic workloads and operating systems to run alongside the actual RTL under test. We apply Rhea to design MSI-based RTL memory subsystems with one and two levels of private caches and scaling up to sixteen cores. Their evaluation with 22 applications from state-of-the-art benchmark suites shows intermediate performance relative to gem5 Ruby’s MI and MOESI models. The hybrid gem5-Verilator co-simulation flow incurs a moderate simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher fidelity by simulating real RTL hardware. This overhead decreases with scale, down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea’s effectiveness and scalability in enabling fast development of RTL cache-coherent memory subsystem designs.",
    "title_zh": "Rhea：用于快速设计与验证RTL缓存一致性内存子系统的框架",
    "abstract_zh": "设计并验证高效的缓存一致性内存子系统是现代多核片上系统架构开发中一项关键且复杂的任务。Rhea 是一个统一的框架，旨在简化 RTL 级缓存一致性内存子系统的架构设计与系统级验证流程。在设计方面，Rhea 能生成可综合、高度可配置的 RTL 代码，支持多种体系结构参数；在验证方面，Rhea 将 Verilator 的周期精确 RTL 仿真与 gem5 的全系统仿真相结合，使得真实的工作负载和操作系统能够在实际待测 RTL 上运行，从而实现高保真度的验证。我们利用 Rhea 设计了基于 MSI 协议的 RTL 内存子系统，涵盖单级和双级私有缓存，并扩展至最多十六个核心。通过对来自前沿基准测试套件的 22 个应用程序进行评估，其性能介于 gem5 Ruby 的 MI 和 MOESI 模型之间。该混合 gem5-Verilator 共同仿真流程带来了适度的仿真开销，相较于 gem5 MI 模型最高达 2.7 倍，但通过仿真真实的 RTL 硬件实现了更高的精度。随着规模增大，该开销逐渐降低，在十六核场景下降至 1.6 倍。这些结果充分证明了 Rhea 在加速 RTL 缓存一致性内存子系统设计开发方面的有效性与可扩展性。"
  },
  {
    "date": "2025-11-20",
    "title": "PROFUZZ: Directed Graybox Fuzzing via Module Selection and ATPG-Guided Seed Generation",
    "authors": "Raghul Saravanan, Sudipta Paria, Aritra Dasgupta, Swarup Bhunia, Sai Manoj P D",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240782",
    "source": "IEEE",
    "abstract": "Hardware fuzzing is critical for uncovering vulnerabilities in modern integrated circuits by systematically exploring input spaces. A major challenge lies in generating high-quality seeds that maximize coverage and verification efficiency. While Coverage-Guided Fuzzing (CGF) enhances overall exploration, it lacks precision when targeting specific submodules. DirectFuzz addresses this with directed test generation but suffers from key limitations, including limited HDL support, abstraction mismatches, and poor scalability for large target regions. In this work, to overcome these challenges, we propose PROFUZZ, a Directed Graybox Fuzzing (DGF) framework that integrates Automatic Test Pattern Generation (ATPG) for precise and scalable seed generation. By leveraging ATPG’s structural analysis capabilities, PROFUZZ improves coverage effectiveness and supports large-scale hardware designs. Experimental results show that PROFUZZ outperforms DirectFuzz with 30× greater scalability in terms of handling target sites, 11.66% higher coverage, and 2.76× faster execution, demonstrating its potential to advance the state-of-the-art in directed hardware fuzzing.",
    "title_zh": "PROFUZZ：通过模块选择与ATPG引导的种子生成实现定向灰盒模糊测试",
    "abstract_zh": "硬件模糊测试对于系统性地探索输入空间、发现现代集成电路中的漏洞至关重要。一个主要挑战在于生成高质量的初始种子，以最大化覆盖率并提高验证效率。尽管覆盖引导模糊测试（CGF）能够提升整体探索能力，但在针对特定子模块时缺乏精确性。DirectFuzz 通过定向测试生成解决了这一问题，但存在若干关键局限：对硬件描述语言（HDL）支持有限、抽象层级不匹配，以及在大规模目标区域上扩展性差。为克服这些挑战，本文提出 PROFUZZ——一种集成自动测试向量生成（ATPG）技术的定向灰盒模糊测试（DGF）框架，实现精准且可扩展的种子生成。通过利用 ATPG 的结构分析能力，PROFUZZ 显著提升了覆盖率的有效性，并支持大规模硬件设计。实验结果表明，PROFUZZ 在处理目标节点方面相比 DirectFuzz 具有 30 倍的可扩展性提升，覆盖率高出 11.66%，执行速度加快 2.76 倍，充分展示了其在推动定向硬件模糊测试领域前沿技术发展方面的巨大潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "HIPPO: A Hierarchy-Preserving and Noise-Tolerant Pre-HLS Power Modeling Framework for FPGA",
    "authors": "Zefan Lin, Zedong Peng, Mingzhe Gao, Jieru Zhao, Zhe Lin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240868",
    "source": "IEEE",
    "abstract": "Power estimation for customized accelerators, especially those derived from high-level programming languages, entails the invocation of a long electronic design automation (EDA) tool chain, thus incurring large timing overhead that hinders early design optimization. To mitigate this problem, in this paper, we propose HIPPO, an architecture-level power modeling framework for field-programmable gate arrays (FPGAs). HIPPO operates directly on C/C++ programs, whose execution is prior to and independent of any EDA tool including the very front-end, high-level synthesis (HLS). During power modeling, HIPPO exploits the intrinsic C/C++ code hierarchies, including nested loops and operations, and enables multi-level power estimation that aligns with different code hierarchies. Specifically, HIPPO can be decomposed into (1) a code transformation flow that directly converts a C/C++ program with HLS pragmas into hardware-oriented and power-aware control and dataflow graph, (2) a hierarchy-preserving power modeling methodology that combines analytical modeling and data-driven learning approaches to effectively orchestrate different code hierarchies, and (3) an adaptive dataflow coarsening strategy which ensures modeling accuracy, efficiency and robustness by suppressing noise of onboard measurement. Experimental results demonstrate that HIPPO effectively decomposes and accurately predicts both dynamic and total power consumption, achieving average errors of 8.89% (dynamic) and 6.31% (total) for nested loops, and 9.86% (dynamic) and 3.41% (total) for single loops, respectively. These results prove that HIPPO paves the way for power-efficient high-level architecture exploration.",
    "title_zh": "HIPPO：一种保持层次结构且具备噪声容忍能力的FPGA预HLS功耗建模框架",
    "abstract_zh": "针对由高级编程语言生成的定制化加速器，其功耗估算通常需要调用复杂的电子设计自动化（EDA）工具链，导致显著的时间开销，严重阻碍了早期设计优化。为解决这一问题，本文提出HIPPO——一种面向现场可编程门阵列（FPGA）的架构级功耗建模框架。HIPPO直接作用于C/C++程序，其执行过程在任何EDA工具（包括前端的高层次综合，HLS）之前完成，且完全独立于这些工具。在功耗建模过程中，HIPPO充分利用C/C++代码固有的层次结构，如嵌套循环和操作层级，并支持多层级功耗估算，与不同代码层次结构相匹配。\n\n具体而言，HIPPO可分解为三个核心部分：（1）代码转换流程，将带有HLS指令的C/C++程序直接转换为面向硬件、具备功耗感知能力的控制与数据流图；（2）保持层次结构的功耗建模方法，结合解析建模与数据驱动学习技术，有效协调不同代码层级的功耗分析；（3）自适应数据流粗化策略，通过抑制板载测量中的噪声，确保建模的准确性、效率与鲁棒性。\n\n实验结果表明，HIPPO能够有效分解并精确预测动态功耗与总功耗，对于嵌套循环，平均误差分别为8.89%（动态功耗）和6.31%（总功耗）；对于单层循环，平均误差分别为9.86%（动态功耗）和3.41%（总功耗）。这些结果充分证明，HIPPO为高效能的高层次架构探索开辟了新路径。"
  },
  {
    "date": "2025-11-20",
    "title": "LLM-on-the-Palm: Mobile LLM Inference with PIM-Enhanced NAND Flash Memory",
    "authors": "Hyunjin Kim, Sanghyeok Han, Jae-Joon Kim",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240826",
    "source": "IEEE",
    "abstract": "Large Language Model (LLM) inference on edge devices is crucial for democratizing AI and addressing privacy and security concerns associated with cloud services. However, the large parameter sizes of LLMs pose significant challenges in deployment on resource-constrained devices, affecting user experience. Mobile DRAM typically cannot accommodate these parameters, necessitating the use of NAND flash memory, but it has considerably slower access speeds. To overcome the limitations, we propose LLM-on-the-Palm, an LLM inference system for smartphones that leverages Processing-in-Memory (PIM)-enhanced NAND flash memory to enable fast inference of billion-parameter LLMs with minimal hardware modifications. Our approach requires only a single multiply-accumulate (MAC) unit per plane in a NAND flash memory - 256 MAC units for 256 GB SSD - without compromising cell density. Simulation results show that our design achieves ∼100 ms per token generation on a 6.7B model under specifications similar to an iPhone-15.",
    "title_zh": "掌上大模型：基于PIM增强型NAND闪存的移动大模型推理",
    "abstract_zh": "在边缘设备上进行大语言模型（LLM）推理对于实现人工智能的普及化，以及解决与云服务相关的隐私和安全问题至关重要。然而，LLM庞大的参数规模给资源受限设备的部署带来了巨大挑战，严重影响用户体验。移动设备中的DRAM通常无法容纳这些参数，因此必须使用NAND闪存，但其访问速度显著较慢。为克服这一局限，我们提出了“掌上大模型”（LLM-on-the-Palm）——一种专为智能手机设计的LLM推理系统，该系统利用增强型存算一体（PIM）NAND闪存技术，仅需对硬件进行极小改动，即可实现对数十亿参数LLM的快速推理。我们的方法仅需在每个NAND闪存平面中配置一个乘加（MAC）单元（256GB固态硬盘共256个MAC单元），且不牺牲存储单元密度。仿真结果表明，在与iPhone-15相似的硬件规格下，该设计可实现每生成一个token约100毫秒的推理速度，适用于67亿参数的模型。"
  },
  {
    "date": "2025-11-20",
    "title": "CoP&amp;R: Co-Optimizing Place-and-Route for Standard Cell Layout via MCTS and AllSAT",
    "authors": "Yen-Ju Su, Jiun-Cheng Tsai, Hsuan-Ming Huang, Aaron C.-W. Liang, Han-Ya Tsai, Wei-Min Hsu, Jen-Hang Yang, Charles H.-P. Wen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240928",
    "source": "IEEE",
    "abstract": "Standard cell layout design at advanced technology nodes faces a massive combinatorial explosion of transistor placement possibilities, especially when targeting optimal performance, power, and area (PPA). In this work, we propose a novel framework that integrates AllSAT-based pruning and Monte Carlo Tree Search (MCTS) to tackle this challenge efficiently. Our method first employs an AllSAT formulation that incorporates routing constraints and layout heuristics to exhaustively enumerate only the legal and promising placement solutions. This dramatically reduces the solution space while preserving high-quality candidates. We then apply a guided MCTS algorithm to explore the reduced space and identify optimal or near-optimal placements under given objectives such as total wire length (TWL). Experimental results on a diverse set of standard cells demonstrate the effectiveness of our approach. The AllSAT filtering improves average solution routability from 1.1% to 62.7%, while reducing the total placement space by over 99.9%. On top of that, our MCTS achieves a 62.2× runtime speedup over brute-force exploration, with only a 0.2% degradation in TWL quality. These results confirm that our AllSAT+MCTS framework offers a scalable and practical solution for high-quality standard cell layout synthesis.",
    "title_zh": "CoP&R：基于MCTS与AllSAT的标准化单元布局协同优化布图布线方法",
    "abstract_zh": "在先进工艺节点下，标准单元版图设计面临晶体管布局可能性的巨大组合爆炸问题，尤其是在追求性能、功耗和面积（PPA）最优时。本文提出一种新颖的框架，将基于AllSAT的剪枝方法与蒙特卡洛树搜索（MCTS）相结合，以高效应对这一挑战。我们的方法首先采用AllSAT形式化建模，融入布线约束和版图启发式规则，对合法且有前景的布局方案进行系统性枚举。该方法显著缩小了解空间，同时保留了高质量的候选解。随后，我们应用一种引导式的MCTS算法，在缩减后的空间中探索并识别出满足特定目标（如总布线长度TWL）的最优或近优布局。在多种标准单元上的实验结果表明，该方法具有显著有效性：AllSAT过滤将平均布局可布线率从1.1%提升至62.7%，同时将总布局空间压缩超过99.9%。在此基础上，MCTS相比暴力搜索实现了62.2倍的运行时间加速，仅带来0.2%的TWL质量退化。这些结果充分证明，所提出的AllSAT+MCTS框架为高质量标准单元版图综合提供了一种可扩展且实用的解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "The Munich Microfluidics Toolkit: Design Automation and Simulation Tools for Microfluidic Devices",
    "authors": "Robert Wille, Philipp Ebner, Maria Emmerich, Michel Takken",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240820",
    "source": "IEEE",
    "abstract": "Microfluidic devices have become essential in biochemical and medical research, enabling high-throughput experimentation on compact and cost-effective platforms. However, the design and realization of microfluidic devices is a manual, tedious, and error-prone task. Additionally, multiple iterations for prototyping are often needed until a physical realization works as intended. Accordingly, methods for the automatic design and simulation of microfluidic devices are key—something that is standard in the design of conventional circuits and systems. In this work, we present the Munich Microfluidics Toolkit (MMFT), an open-source toolkit that provides corresponding tools for automating the design and simulation of microfluidic systems. For selected design tasks—such as the generation of meanders, gradient generators, organs-on-chip layouts, as well as ISO-compliant routing and validation—we showcase corresponding tools and provide an overview of simulators for microfluidics. MMFT helps researchers and engineers to design microfluidic devices in an automatic fashion (often with the click of a button) and to validate them through simulation across different abstraction levels. All tools are publicly available at https://www.cda.cit.tum.de/research/microfluidics/mmft/.",
    "title_zh": "慕尼黑微流控工具包：微流控器件的设计自动化与仿真工具",
    "abstract_zh": "微流控器件在生物化学和医学研究中已成为不可或缺的工具，能够在紧凑且成本低廉的平台上实现高通量实验。然而，微流控器件的设计与实现过程通常依赖人工操作，既繁琐又容易出错。此外，往往需要多次原型迭代，才能使物理器件达到预期功能。因此，实现微流控器件的自动设计与仿真至关重要——这在传统电路与系统设计中已是标准流程。本文介绍了慕尼黑微流控工具包（Munich Microfluidics Toolkit, MMFT），这是一个开源工具包，提供了一系列自动化微流控系统设计与仿真的配套工具。针对特定设计任务——如蛇形通道生成、浓度梯度发生器设计、类器官芯片布局，以及符合ISO标准的布线与验证——我们展示了相应的工具，并概述了当前可用的微流控仿真器。MMFT可帮助研究人员和工程师以自动化方式（通常只需点击按钮）完成微流控器件的设计，并通过多层级抽象的仿真进行验证。所有工具均可在 https://www.cda.cit.tum.de/research/microfluidics/mmft/ 公开获取。"
  },
  {
    "date": "2025-11-20",
    "title": "Target Circuit Matching in Large-Scale Netlists Using GNN-Based Region Prediction",
    "authors": "Sangwoo Seo, Jimin Seo, Yoonho Lee, Donghyeon Kim, Hyejin Shin, Banghyun Sung, Chanyoung Park",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240923",
    "source": "IEEE",
    "abstract": "Subgraph matching plays an important role in electronic design automation (EDA) and circuit verification. Traditional rule-based methods have limitations in generalizing to arbitrary target circuits. Furthermore, node-to-node matching approaches tend to be computationally inefficient, particularly for large-scale circuits. Deep learning methods have emerged as a potential solution to address these challenges, but existing models fail to efficiently capture global subgraph embeddings or rely on inefficient matching matrices, which limits their effectiveness for large circuits. In this paper, we propose an efficient graph matching approach that utilizes Graph Neural Networks (GNNs) to predict regions of high probability for containing the target circuit. Specifically, we construct various negative samples to enable GNNs to accurately learn the presence of target circuits and develop an approach to directly extracting subgraph embeddings from the entire circuit, which captures global subgraph information and addresses the inefficiency of applying GNNs to all candidate subgraphs. Extensive experiments demonstrate that our approach significantly outperforms existing methods in terms of time efficiency and target region prediction, offering a scalable and effective solution for subgraph matching in large-scale circuits. The source code is available at https://github.com/sang-woo-seo/Circuit-matching-GNN.",
    "title_zh": "基于GNN的区域预测在大规模网表中的目标电路匹配",
    "abstract_zh": "子图匹配在电子设计自动化（EDA）和电路验证中起着至关重要的作用。传统的基于规则的方法在泛化到任意目标电路方面存在局限性，而节点对节点的匹配方法往往计算效率低下，尤其是在大规模电路中表现尤为明显。深度学习方法虽被视为解决这些挑战的潜在方案，但现有模型难以高效捕捉全局子图嵌入，或依赖于低效的匹配矩阵，从而限制了其在大规模电路中的应用效果。本文提出了一种高效的图匹配方法，利用图神经网络（GNN）预测可能包含目标电路的高概率区域。具体而言，我们构建了多种负样本，使GNN能够准确学习目标电路的存在特征，并提出一种直接从整个电路中提取子图嵌入的方法，该方法能够捕获全局子图信息，克服了将GNN应用于所有候选子图时的效率瓶颈。大量实验表明，与现有方法相比，本方法在时间效率和目标区域预测方面均有显著提升，为大规模电路中的子图匹配提供了一种可扩展且高效的解决方案。源代码已公开，获取地址为：https://github.com/sang-woo-seo/Circuit-matching-GNN。"
  },
  {
    "date": "2025-11-20",
    "title": "BMCFuzz: Hybrid Verification of Processors by Synergistic Integration of Bound Model Checking and Fuzzing",
    "authors": "Shidong Shen, Jinyu Liu, Weizhi Feng, Fu Song, Zhilin Wu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240887",
    "source": "IEEE",
    "abstract": "Modern processors are becoming increasingly complicated, making them hard to be bug-free. Bounded model checking (BMC) and coverage-guided fuzzing (CGF) are two main complementary techniques for verifying processors. BMC can exhaustively explore the state-space upto a given path-depth bound, but suffers from the infamous state-space explosion problem, thus limited to smaller bounds for realistic processor designs. CGF is efficient and scalable for verifying large-scale complex designs, but struggles with the coverage due to the difficulty in generating comprehensive and diverse seeds. To bring the best of both worlds, we propose BMCFuzz, a novel two-way hybrid verification approach that synergistically integrates BMC and CGF. Specifically, BMCFuzz alternatively switches BMC and CGF according to their performance in improving coverage, where CGF is leveraged to quickly explore the state space, detect flaws, and moreover record snapshots that are crucial valuations of all the circuit-level registers, while BMC with selected high-valuable snapshots as initial states is utilized to exhaustively explore uncovered points. Moreover, the witnesses of BMC are further used to generate seeds for CGF. This synergistic integration of BMC and CGF helps BMC alleviate the state-space explosion problem and feeds CGF with more high-quality seeds. We implement BMCFuzz as a fully open-source tool and evaluate it on three well-known open-source RISC-V processor designs (i.e., NutShell, Rocket, and BOOM). Experimental results show that BMCFuzz achieves higher coverage compared to the state-of-the-art methods and discovers three previously unknown bugs, demonstrating the potential of BMCFuzz as a powerful, open-source tool for advancing processor design and verification.",
    "title_zh": "BMCFuzz：通过边界模型检测与模糊测试的协同集成实现处理器的混合验证",
    "abstract_zh": "现代处理器正变得越来越复杂，难以做到完全无缺陷。边界模型检测（Bounded Model Checking, BMC）和覆盖率引导的模糊测试（Coverage-Guided Fuzzing, CGF）是验证处理器的两种主要互补技术。BMC 能够在给定路径深度限制下对状态空间进行穷尽探索，但面临著名的状态空间爆炸问题，因此对于实际的处理器设计而言，其可处理的深度范围较小。而 CGF 在验证大规模复杂设计时具有高效性和可扩展性，但在覆盖率方面存在挑战，主要源于难以生成全面且多样化的初始种子。为了融合两者的优点，我们提出了一种名为 BMCFuzz 的新型双向混合验证方法，该方法协同集成 BMC 与 CGF。具体而言，BMCFuzz 根据两者在提升覆盖率方面的表现，交替使用 BMC 和 CGF：CGF 用于快速探索状态空间、发现缺陷，并记录关键的电路级寄存器取值快照；随后，BMC 则以这些高价值快照作为初始状态，对未覆盖的区域进行穷尽探索。此外，BMC 所产生的反例（witnesses）还会被进一步用于生成高质量的 CGF 种子。这种 BMC 与 CGF 的协同整合，不仅帮助 BMC 缓解了状态空间爆炸的问题，也使 CGF 获得了更多优质种子。我们已将 BMCFuzz 实现为一个完全开源的工具，并在三个知名的开源 RISC-V 处理器设计（即 NutShell、Rocket 和 BOOM）上进行了评估。实验结果表明，与现有最先进方法相比，BMCFuzz 能达到更高的覆盖率，并发现了三个此前未知的漏洞，充分展示了 BMCFuzz 作为一种强大且开源的工具，在推动处理器设计与验证方面的巨大潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited paper - Connecting the D.O.T.S.: Design of fluidic circuit boards for multi-OoC platforms using CAD Tools for Standardization",
    "authors": "Laurens Spoelstra, Marlize Kramer, Jasper Rietveld, Joshua Loessberg-Zahl, Loes Segerink",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240924",
    "source": "IEEE",
    "abstract": "One of the advantages of microfluidic chips is their ability to enable multiple processes on a single platform. With the Translational Organ-on-Chip Platform, we can combine multiple Organ-on-Chip (OoC) systems with other building blocks to facilitate seamless integration of inline sensing and complex multi-organ experiments. A significant challenge in this research is generating the correct microfluidic routing architecture. The geometry of routing channels must be chosen carefully, as it directly affects fluid flow distribution and circulation within and between the microfluidic building blocks. Currently, this routing is typically performed manually by drawing the channel geometry in computer-aided design software. In the field of design automation, several software packages already exist that enable the automatic design of electrical connections/signals. Therefore, we present a comprehensive method for multilayer fluidic channel routing in this paper, utilizing the printed circuit board designer from Fusion electronics, a commonly used software package in the microfluidics field. We demonstrate this method by routing a complex multi-OoC system designed to study mechanobiology, which contains multiple independent fluidic loops and pneumatic circuitry. We continue to discuss the advantages and limitations of the method presented in this paper to finally arrive at a list of desired functionalities for to-be-developed microfluidics-tailored routing tools. We envision the method and observations presented here as inspiration for the development of future microfluidics-specific tools.",
    "title_zh": "特邀论文——连接D.O.T.S.：利用CAD工具设计用于多器官芯片平台的流体电路板以实现标准化",
    "abstract_zh": "微流控芯片的一个优势在于其能够在单一平台上实现多种过程的集成。借助转化型类器官芯片平台（Translational Organ-on-Chip Platform），我们可以将多个类器官芯片（OoC）系统与其他构建模块相结合，从而实现在线传感与复杂多器官实验的无缝整合。然而，该研究领域面临的一个重大挑战是设计出正确的微流控布线架构。布线通道的几何形状必须经过精心选择，因为其直接影响微流控单元内部及之间的流体分布与循环。目前，这类布线通常通过在计算机辅助设计软件中手动绘制通道几何结构来完成。在设计自动化领域，已有若干软件工具可实现电气连接/信号的自动布线。因此，本文提出了一种全面的多层流体通道布线方法，利用了Fusion Electronics这一在微流控领域广泛使用的印刷电路板设计软件。我们通过一个用于研究机械生物学的复杂多类器官芯片系统实例，展示了该方法的应用，该系统包含多个独立的流体回路以及气动控制电路。随后，我们进一步探讨了本文所提方法的优势与局限性，并最终归纳出未来专为微流控应用定制的布线工具应具备的理想功能清单。我们期望本文提出的方法与观察结果，能为今后专用微流控布线工具的研发提供启发。"
  },
  {
    "date": "2025-11-20",
    "title": "Building Reasoning LLMs for Hardware Design Generation via Function-Aligned Differentiated Revision",
    "authors": "Weimin Fu, Shijie Li, Kaichen Yang, Xuan Silvia Zhang, Yier Jin, Xiaolong Guo",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240942",
    "source": "IEEE",
    "abstract": "Recent advances in large language models have significantly improved the capabilities of programming. While these models excel at generating valid software, applying them to the hardware domain remains challenging due to the intrinsic complexity and strict structural semantics required in hardware design. Current LLM approaches for hardware generation typically focus on direct generation. This often results in hardware implementations with functional errors or structural flaws. To overcome these limitations, we propose a reasoning-enhanced training framework explicitly tailored for hardware generation tasks. Our multi-stage methodology combines systematic dataset curation via compilation filtering (achieving a 100% pass rate compared to 27 − 44% in existing datasets), Function-Aligned Differentiated Revision for comparative annotation across five RTL-relevant dimensions, supervised fine-tuning using reasoning prompts, and reinforcement learning guided by Verilator Parser. Our experiments show that explicitly incorporating reasoning substantially enhances the structural integrity and functional correctness of generated hardware designs, improving pass@1 rates by up to 20% on VerilogEval Human benchmarks and reproducing the \"Aha moment\", where the model explicitly organizes ideas before generation. Our work demonstrates that smaller, specialized reasoning models (1.5B parameters) can effectively augment larger open-source language models through reasoning transfer.",
    "title_zh": "通过功能对齐的差异化修订构建用于硬件设计生成的推理型大语言模型",
    "abstract_zh": "近年来，大型语言模型的进展显著提升了编程能力。尽管这些模型在生成有效软件方面表现出色，但在硬件领域应用仍面临挑战，原因在于硬件设计具有内在的复杂性以及严格的结构语义要求。当前针对硬件生成的LLM方法通常采用直接生成策略，这往往导致生成的硬件实现存在功能错误或结构缺陷。为克服这些局限，我们提出了一种专为硬件生成任务量身定制的增强推理训练框架。我们的多阶段方法结合了通过编译过滤进行系统性数据集构建（相比现有数据集27–44%的通过率，达到100%的通过率）、针对五个RTL相关维度的函数对齐差异化修订、基于推理提示的监督微调，以及由Verilator解析器引导的强化学习。实验结果表明，显式引入推理可显著提升生成硬件设计的结构完整性和功能正确性，在VerilogEval Human基准测试中，pass@1率最高提升达20%，并成功复现了“顿悟时刻”——即模型在生成前会明确组织思路。本研究证明，小型专用推理模型（15亿参数）可通过推理迁移有效增强更大规模的开源语言模型。"
  },
  {
    "date": "2025-11-20",
    "title": "Seeing Through Designs: Attention-Based Knowledge Transfer for Preference-Guided Microarchitecture Search",
    "authors": "Yiyang Zhao, Xuyang Zhao, Zhaori Bi, Ming Zhu, Qiwei Zhan, Keren Zhu, Fan Yang, Changhao Yan, Dian Zhou, Xuan Zeng",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240774",
    "source": "IEEE",
    "abstract": "Modern processor microarchitectures face increasing complexity, leading to larger search spaces and lengthy design-to-silicon validation flows. While reusing design knowledge across architectures offers potential efficiency gains, the common practice remains specific-architecture search due to inherent discrepancies in power, performance, and area (PPA) metrics between designs. We propose an attention-based microarchitecture search framework for effective cross-architecture knowledge transfer. Our approach propose a cross-attention network to capture interdependencies between microarchitectural topology and design tool configurations, enabling knowledge adaptation across architectures with minimal fine-tuning. Additionally, we complement it with an uncertainty-guided optimization strategy that efficiently navigates search based on specific user preferences. Experimental results demonstrate our approach outperforms previous methods with 68.16% higher hypervolume indicators and 3.85× speed-up of time in reaching the same hypervolume. Furthermore, our approach successfully discovers design points that meet user-specified PPA targets that state-of-the-art (SOTA) methods failed to identify. Our code is publicly available at https://github.com/MarsH3107/ICAN, enabling broader adoption and encouraging further research in transferable processor design optimization.",
    "title_zh": "透过设计看本质：基于注意力的知识迁移在偏好引导的微架构搜索中的应用",
    "abstract_zh": "现代处理器微架构面临日益增加的复杂性，导致搜索空间扩大以及从设计到硅片验证流程耗时更长。尽管在不同架构间复用设计知识具有提升效率的潜力，但受限于各设计之间功耗、性能和面积（PPA）指标的固有差异，当前仍普遍采用针对特定架构的搜索方法。为此，我们提出一种基于注意力机制的微架构搜索框架，以实现高效的跨架构知识迁移。我们的方法引入了一种交叉注意力网络，用于捕捉微架构拓扑与设计工具配置之间的相互依赖关系，从而在仅需极少微调的情况下，实现跨架构的知识适应。此外，我们还结合了一种基于不确定性的优化策略，能够根据用户的具体偏好高效地进行搜索。实验结果表明，相较于以往方法，我们的方案在达到相同超体积指标时，其超体积指标高出68.16%，且搜索速度提升3.85倍。更重要的是，我们的方法成功发现了若干满足用户指定PPA目标的设计点，而现有最先进（SOTA）方法未能识别出这些点。相关代码已公开发布于 https://github.com/MarsH3107/ICAN，便于广泛采用，并推动可迁移处理器设计优化领域的进一步研究。"
  },
  {
    "date": "2025-11-20",
    "title": "Parallel Non-Integer Multiple-Cell-Height Node Remapping",
    "authors": "Zong-Han Wu, Bo-Ying Huang, Yu-Chen Chen, Yao-Wen Chang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240758",
    "source": "IEEE",
    "abstract": "The fast-growing complexity of VLSI circuits with non-integer multiple-cell-height (NIMCH) standard cells poses new challenges for timing-aware node remapping. The existing remapping flow suffers from significant computational overhead due to exhaustive enumeration of all node-to-gate mapping combinations, including nodes and their fanin nodes. To remedy this inefficiency, we propose a novel NIMCH node remapping flow consisting of the following two schemes: a parallel longest-path-first scheme and an iterative minimum-slack-first scheme. Both schemes prioritize timing-critical nodes to reduce runtime. Moreover, the first scheme enables parallel execution, substantially accelerating the remapping process. Experimental results show that our proposed method achieves an average runtime speedup of 5.58X compared to the state-of-the-art approach, while preserving placement quality. The improved efficiency and scalability of our flow make it well-suited for large-scale physical design applications where rapid timing closure is essential.",
    "title_zh": "并行非整数多单元高度节点重映射",
    "abstract_zh": "随着VLSI电路中非整数倍单元高度（NIMCH）标准单元的复杂度迅速增加，时序感知节点重映射面临新的挑战。现有的重映射流程由于需要对所有节点与门之间的映射组合进行穷举枚举（包括节点及其扇入节点），导致计算开销巨大。为解决这一效率问题，我们提出一种新型的NIMCH节点重映射流程，包含以下两种方案：并行最长路径优先方案和迭代最小松弛优先方案。这两种方案均通过优先处理时序关键节点来降低运行时间。此外，第一种方案支持并行执行，显著加速了重映射过程。实验结果表明，与当前最先进的方法相比，所提方法在保持布局质量的前提下，平均运行时间提速达5.58倍。该流程在效率和可扩展性上的提升，使其非常适合于大规模物理设计应用，尤其适用于需要快速实现时序收敛的场景。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Security Under the Lens: Vulnerabilities in In-Sensor Computing Systems",
    "authors": "Mashrafi Kajol, Wei Lu, Qiaoyan Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240763",
    "source": "IEEE",
    "abstract": "In-Sensor Computing (ISC) systems integrate sensing and computing units within a single device, enabling low-latency, energy-efficient applications through direct analog-to-feature conversion. However, the intrinsic tight coupling between sensing and computational components introduces significant security vulnerabilities. These arise particularly in scenarios where adversaries have a good understanding of the analog computation mechanisms and could tamper with the ISC device, potentially allowing for manipulation, inference, or extraction of sensitive data. This work introduces exploitable backdoors in ISC that encode the output of the analog computation unit to create covert channels. Through theoretical modeling and empirical case studies, we investigate two ISC-specific covert channels: a logic covert channel and a frequency covert channel. These channels are established by deliberately manipulating the analog computation unit co-integrated with sensing materials on a shared substrate, thereby enabling adversaries to exfiltrate sensitive information, posing substantial threats to the security and privacy of real-world ISC applications.",
    "title_zh": "特邀论文：在镜头下审视安全：传感器内计算系统中的漏洞",
    "abstract_zh": "在传感器内计算（In-Sensor Computing, ISC）系统中，传感与计算单元被集成于单一设备内，通过直接的模拟信号到特征值的转换，实现低延迟、高能效的应用。然而，传感与计算组件之间固有的紧密耦合特性带来了显著的安全隐患。尤其当攻击者对模拟计算机制有深入了解，并能够篡改ISC设备时，可能引发数据操纵、信息推断或敏感数据提取等安全威胁。本文提出了一种可被利用的后门机制，该机制将模拟计算单元的输出编码为隐蔽信道。通过理论建模与实证案例研究，我们探讨了两种针对ISC系统的隐蔽信道：逻辑隐蔽信道和频率隐蔽信道。这些信道通过故意操控集成于同一基底上的传感材料与模拟计算单元之间的协同工作关系而建立，使攻击者能够窃取敏感信息，对现实世界中ISC应用的安全性与隐私性构成严重威胁。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Towards Generative AI for Analog and RF IC Design: From Spec to Layout",
    "authors": "Hyunsu Chae, Seunggeun Kim, Souradip Poddar, Xiaohan Gao, Sensen Li, David Z. Pan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240831",
    "source": "IEEE",
    "abstract": "Analog/RF IC design has long been a heavily manual process, from circuit topology generation to sizing and to layout. In the entire design process, extensive circuit simulations will be performed to check if various design constraints/objectives can be met and optimized. However, this design process is very tedious and not scalable. This paper surveys recent efforts toward agile and intelligent analog/RF IC design automation, leveraged by generative AI, from topology generation to device sizing and layout, and from surrogate modeling to inverse design, leveraging the recent AI advancements and optimizations. We also discuss challenges and opportunities toward building an end-to-end analog/RF IC design automation framework from specification to layout.",
    "title_zh": "特邀论文：面向模拟与射频集成电路设计的生成式AI——从规格到版图",
    "abstract_zh": "模拟/射频集成电路（Analog/RF IC）设计长期以来一直是一个高度依赖人工的过程，从电路拓扑生成、器件尺寸确定到版图设计，整个流程都需要大量的人工干预。在设计过程中，需要进行大量的电路仿真，以验证各种设计约束或目标是否满足并达到最优。然而，这种设计方法不仅繁琐，而且难以扩展。本文综述了近年来利用生成式人工智能推动敏捷化与智能化模拟/射频IC设计自动化的最新进展，涵盖从拓扑生成、器件尺寸优化到版图设计的全流程，以及基于代理模型的逆向设计等技术，充分利用了近期人工智能的发展和优化成果。同时，本文还探讨了构建从规格说明到版图输出的端到端模拟/射频IC设计自动化框架所面临的挑战与机遇。"
  },
  {
    "date": "2025-11-20",
    "title": "(Invited) Generalized GPU-Accelerated Dynamic Programming with Application to Mixed-Cell-Height Detailed Placement",
    "authors": "Da-Wei Huang, Shao-Yun Fang, Wen-Hao Liu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240861",
    "source": "IEEE",
    "abstract": "Dynamic programming (DP) plays a crucial role as the backbone of many core optimization algorithms across the physical design flow, including placement, clock tree synthesis, and routing. However, DP methods with quadratic or higher time complexities often face scalability challenges in large-scale designs. In this work, we propose a novel GPU-accelerated DP technique that can be broadly adaptable to various DP-based algorithms. As a case study, we present a GPU-accelerated detailed placement framework targeting mixed-cell-height designs, built upon an optimal DP formulation. By leveraging the massive parallelism of modern GPUs and incorporating advanced cost accumulation strategies, our approach consistently delivers optimal placement solutions while achieving an average runtime reduction of 96.6% and peak speedups exceeding 1600×. This work demonstrates the potential of GPU acceleration not only to overcome runtime bottlenecks but also to enhance design quality in advanced physical design tasks.",
    "title_zh": "（特邀）广义GPU加速动态规划及其在混合单元高度详细布局中的应用",
    "abstract_zh": "动态规划（DP）在物理设计流程中的诸多核心优化算法中扮演着关键角色，包括布局、时钟树综合和布线等。然而，具有二次或更高时间复杂度的DP方法在大规模设计中常常面临可扩展性挑战。本文提出了一种新型的GPU加速DP技术，可广泛适配多种基于DP的算法。作为案例研究，我们构建了一个针对混合单元高度设计的GPU加速详细布局框架，其基于最优DP公式。通过利用现代GPU的海量并行计算能力，并结合先进的代价累积策略，我们的方法始终能够提供最优布局解，同时平均运行时间减少96.6%，峰值加速比超过1600倍。本工作展示了GPU加速不仅能够有效突破运行时瓶颈，还能在先进物理设计任务中提升设计质量的潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "Synthesis of Standard Cells of Minimum Delay",
    "authors": "Sehyeon Chung, Hyunbae Seo, Taewhan Kim",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240906",
    "source": "IEEE",
    "abstract": "In this paper, a new approach to the problem of synthesizing standard cells is presented. The top priority objective in the conventional approaches has been invariably placed on minimizing cell area. However, in our approach, we place the top priority on minimizing cell delay as opposed to minimizing cell area, which has never been addressed as yet, but is very valuable and highly important for implementing high-performance chips at advanced technology nodes. Precisely, we propose a totally different approach, developing a cell delay driven layout synthesis method, which is composed of three steps: (1) a critical path driven transistor placement, which is formulated into a search tree based exhaustive placement enumeration, employing an effective pruning technique, followed by (2) an optimal transistor folding, formulating it into an instance of DP (dynamic programming) to reduce cell area by maximizing the occurrences of diffusion sharing and minimizing the oxide diffusion jog rule violations, then (3) an optimal critical net driven in-cell routing, formulating it into an instance of SMT (satisfiability modulo theory) problem. In the meantime, through experiments with benchmarks, it is shown that our cell synthesis approach is able to produce cells with up to 9.3% shorter delay. More importantly, by using those cells, we are able to increase the circuit clock frequency by 7.2% on average while retaining nearly the same chip area and power consumption over that produced by using the conventional cells.",
    "title_zh": "最小延迟标准单元的综合",
    "abstract_zh": "本文提出了一种全新的标准单元综合方法。在传统方法中，始终将最小化单元面积作为首要目标。然而，在我们的方法中，将最小化单元延迟置于首位，而非最小化单元面积——这一问题此前从未被系统研究过，但在先进工艺节点下实现高性能芯片方面具有极高的价值和重要意义。具体而言，我们提出了一种完全不同的方法，开发了一种以单元延迟为导向的布局综合技术，该技术包含三个步骤：（1）基于关键路径的晶体管布局，将其建模为基于搜索树的穷举式布局枚举，并采用高效的剪枝技术；（2）最优晶体管折叠，将其建模为动态规划（DP）问题，通过最大化扩散区共享的次数并最小化氧化层扩散拐角规则违规，从而减小单元面积；（3）最优关键网络驱动的片内布线，将其建模为SMT（可满足性模理论）问题。通过基准测试实验验证，我们的单元综合方法能够生成延迟最短可达9.3%的单元。更重要的是，使用这些新单元后，电路时钟频率平均提升7.2%，同时几乎保持与传统单元相同的芯片面积和功耗水平。"
  },
  {
    "date": "2025-11-20",
    "title": "MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging",
    "authors": "Jinwei Tang, Jiayin Qin, Nuo Xu, Pragnya Sudershan Nalla, Yu Cao, Yang Katie Zhao, Caiwen Ding",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240973",
    "source": "IEEE",
    "abstract": "As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.",
    "title_zh": "MAHL：基于多智能体大模型引导的分层芯粒设计及自适应调试",
    "abstract_zh": "随着程序工作负载（如人工智能）的规模和算法复杂性不断增加，其主要挑战在于高维度特性，涵盖计算核心数量、数组尺寸以及内存层次结构。为克服这些障碍，亟需创新的方法。敏捷芯片设计已在多个阶段受益于机器学习的融合，包括逻辑综合、布局布线等环节。近期，大语言模型（LLMs）在硬件描述语言（HDL）生成方面展现出令人瞩目的能力，这为将它们的能力拓展至2.5D集成这一先进工艺带来了希望——该技术可有效降低面积开销与开发成本。然而，基于LLM的芯粒（chiplet）设计仍面临诸多挑战，如设计扁平化、验证成本高昂以及参数优化不精确等问题，严重制约了其在芯粒设计中的应用潜力。\n\n针对上述问题，我们提出MAHL：一种分层式基于大语言模型的芯粒设计生成框架，包含六个协同工作的智能体，能够实现AI算法与硬件之间的高效映射，具体包括：分层描述生成、检索增强型代码生成、基于多样性流的验证机制，以及多粒度设计空间探索。这些组件共同提升了芯粒设计的生成效率，并实现了更优的功耗（Power）、性能（Performance）与面积（Area）（PPA）综合优化。\n\n实验结果表明，MAHL不仅显著提升了简单RTL设计的生成准确性，还在真实世界芯粒设计的生成精度上取得突破——在最佳情况下，其Pass@5指标从传统LLM的0提升至0.72。相较于当前最先进的专家系统方法CLARIE，在特定优化目标下，MAHL实现了相当甚至更优的PPA表现。"
  },
  {
    "date": "2025-11-20",
    "title": "MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI",
    "authors": "Zijun Jiang, Yangdi Lyu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240938",
    "source": "IEEE",
    "abstract": "Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven promising in efficient storage and computation on edge devices. To further reduce the accuracy drop while increasing speedup, layer-wise mixed-precision quantization (MPQ) becomes a popular solution. However, existing algorithms for exploring MPQ schemes are limited in flexibility and efficiency. Comprehending the complex impacts of different MPQ schemes on post-training quantization and quantization-aware training results is a challenge for conventional methods. Furthermore, an end-to-end framework for the optimization and deployment of MPQ models is missing in existing work.In this paper, we propose the MiCo framework, a holistic MPQ exploration and deployment framework for edge AI applications. The framework adopts a novel optimization algorithm to search for optimal quantization schemes with the highest accuracies while meeting latency constraints. Hardware-aware latency models are built for different hardware targets to enable fast explorations. After the exploration, the framework enables direct deployment from PyTorch MPQ models to bare-metal C codes, leading to end-to-end speedup with minimal accuracy drops.",
    "title_zh": "MiCo：面向边缘人工智能的端到端混合精度神经网络协同探索框架",
    "abstract_zh": "量化神经网络（QNN）采用极低比特宽度的数据，在边缘设备上实现了高效的存储与计算。为了在提升加速比的同时进一步减少精度损失，逐层混合精度量化（MPQ）成为一种流行解决方案。然而，现有的MPQ方案探索算法在灵活性和效率方面存在局限。传统方法难以全面理解不同MPQ方案对训练后量化及量化感知训练结果的复杂影响。此外，现有研究中缺乏一个端到端的MPQ模型优化与部署框架。\n\n本文提出MiCo框架，这是一个面向边缘人工智能应用的完整MPQ探索与部署框架。该框架采用一种新颖的优化算法，在满足延迟约束的前提下，搜索具有最高精度的最优量化方案。针对不同硬件目标构建了硬件感知的延迟模型，从而实现快速探索。在完成探索后，框架可直接将PyTorch中的MPQ模型部署为裸机C代码，实现从模型设计到实际部署的端到端加速，同时保持最小的精度损失。"
  },
  {
    "date": "2025-11-20",
    "title": "Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA",
    "authors": "Jindong Li, Tenglong Li, Ruiqi Chen, Guobin Shen, Dongcheng Zhao, Qian Zhang, Yi Zeng",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11241002",
    "source": "IEEE",
    "abstract": "Deploying large language models (LLMs) on embedded devices remains a significant research challenge due to the high computational and memory demands of LLMs and the limited hardware resources available in such environments. While embedded FPGAs have demonstrated performance and energy efficiency in traditional deep neural networks, their potential for LLM inference remains largely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily relied on large, expensive cloud-grade hardware and have only shown promising results on relatively small LLMs, limiting their real-world applicability. In this work, we present Hummingbird, a novel FPGA accelerator designed specifically for LLM inference on embedded FPGAs. Hummingbird is smaller—targeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP, and 42% power savings over existing research. Hummingbird is stronger—targeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB memory constraint of embedded FPGAs through offloading strategies. Finally, Hummingbird is faster—achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on the KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization, outperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth utilization baseline. We further demonstrate the viability of industrial applications by deploying Hummingbird on a cost-optimized Spartan UltraScale FPGA, paving the way for affordable LLM solutions at the edge.",
    "title_zh": "蜂鸟：一种在嵌入式FPGA上实现的更小更快的大语言模型加速器",
    "abstract_zh": "在嵌入式设备上部署大语言模型（LLMs）仍是一个重大的研究挑战，主要源于LLMs对计算和内存的高需求，以及嵌入式环境中有限的硬件资源。尽管嵌入式FPGA在传统深度神经网络中已展现出出色的性能与能效，但其在LLM推理方面的潜力尚未得到充分探索。目前将LLMs部署到FPGA上的尝试大多依赖于大型且昂贵的云级硬件，且仅在相对较小的LLM上取得了有前景的结果，限制了其在实际场景中的应用。本文提出Hummingbird——一种专为嵌入式FPGA设计的新型LLM推理加速器。Hummingbird更小巧：针对KV260和ZCU104等嵌入式FPGA，实现比现有研究减少67%的查找表（LUT）、39%的数字信号处理单元（DSP）以及42%的功耗。Hummingbird更强大：支持LLaMA3-8B模型并可处理更长的上下文序列，通过创新的卸载策略突破了嵌入式FPGA常见的4GB内存限制。Hummingbird更快：在KV260和ZCU104上分别实现了4.8 tokens/s和8.6 tokens/s的推理速度，模型带宽利用率高达93–94%，显著优于此前针对LLaMA2-7B的基准表现（4.9 tokens/s，84%带宽利用率）。此外，我们进一步验证了其在工业应用中的可行性，成功将Hummingbird部署于成本优化的Spartan UltraScale FPGA，为边缘端提供经济高效的LLM解决方案铺平了道路。"
  },
  {
    "date": "2025-11-20",
    "title": "DIVE: Dynamic Information-Guided Variable Expansion for Deeper Analog Circuit Optimization",
    "authors": "Zhuohua Liu, Weilun Xie, Yuxuan Zhang, Chen Wang, Yuanqi Hu, Wei W. Xing",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240976",
    "source": "IEEE",
    "abstract": "In analog circuit design, transistor sizing remains a critical challenge due to high-dimensional parameter spaces and expensive simulations. While Bayesian optimization shows promise, existing methods struggle with the \"curse of dimensionality.\" Inspired by expert designers’ workflow of focusing on key parameters first before gradually optimizing secondary parameters, we introduce DIVE (Dynamic Information-guided Variable Expansion), a framework that reformulates parameter optimization as an information efficiency maximization problem. Unlike approaches that explore all parameters simultaneously, DIVE progressively includes design variables with the highest information content. Our framework introduces three innovations: (1) constraint-aware weighted mutual information analysis that evaluates parameters’ contributions to objectives and constraints; (2) an adaptive variable inclusion mechanism that determines when to expand the optimization space; and (3) a mutual information-guided kernel learning strategy for Gaussian process models. Evaluations across multiple analog circuits demonstrate that DIVE achieves a 1.61×-21.11× reduction in required simulations while delivering up to 2.69× spec improvements over the state-of-the-art methods. By progressing from simple to complex parameter spaces based on information theory, DIVE establishes a new paradigm for reaching deeper optima by mimicking experienced designers’ design philosophy in circuit optimization. Our code is available<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>.",
    "title_zh": "DIVE：用于深度模拟电路优化的动态信息引导变量扩展",
    "abstract_zh": "在模拟电路设计中，由于参数空间维度高且仿真成本昂贵，晶体管尺寸优化仍是一个关键挑战。尽管贝叶斯优化展现出巨大潜力，但现有方法在应对“维度灾难”方面仍面临困难。受资深设计师先聚焦关键参数、再逐步优化次要参数的工作流程启发，我们提出了DIVE（动态信息引导变量扩展）框架，将参数优化重新定义为信息效率最大化的任务。与同时探索所有参数的传统方法不同，DIVE采用渐进式策略，优先引入信息量最高的设计变量。本框架包含三项创新：（1）考虑约束条件的加权互信息分析，用于评估各参数对目标函数和约束条件的贡献；（2）自适应变量引入机制，用于判断何时扩展优化空间；（3）基于互信息引导的高斯过程核学习策略。在多个模拟电路上的实验表明，DIVE在所需仿真次数上减少了1.61×至21.11×，同时相较当前最先进方法实现了最高达2.69×的规格性能提升。通过依据信息论从简单到复杂的参数空间逐步推进，DIVE建立了一种新范式，模仿经验丰富的设计师的设计哲学，在电路优化中实现更深层次的最优解。我们的代码已公开<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>。"
  },
  {
    "date": "2025-11-20",
    "title": "DeepCell: Self-Supervised Multiview Fusion for Circuit Representation Learning",
    "authors": "Zhengyuan Shi, Chengyu Ma, Ziyang Zheng, Lingfeng Zhou, Hongyang Pan, Wentao Jiang, Fan Yang, Xiaoyan Yang, Zhufei Chu, Qiang Xu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240883",
    "source": "IEEE",
    "abstract": "We introduce DeepCell, a novel circuit representation learning framework that effectively integrates multiview information from both And-Inverter Graphs (AIGs) and Post-Mapping (PM) netlists. At its core, DeepCell employs a self-supervised Mask Circuit Modeling (MCM) strategy, inspired by masked language modeling, to fuse complementary circuit representations from different design stages into unified and rich embeddings. To our knowledge, DeepCell is the first framework explicitly designed for PM netlist representation learning, setting new benchmarks in both predictive accuracy and reconstruction quality. We demonstrate the practical efficacy of DeepCell by applying it to critical EDA tasks such as functional Engineering Change Orders (ECO) and technology mapping. Extensive experimental results show that DeepCell significantly surpasses state-of-the-art open-source EDA tools in efficiency and performance. The code is available at https://github.com/cure-lab/DeepCell.",
    "title_zh": "DeepCell：用于电路表示学习的自监督多视角融合",
    "abstract_zh": "我们提出 DeepCell，这是一种新颖的电路表示学习框架，能够有效整合来自与非门图（AIG）和映射后网表（PM netlists）的多视角信息。DeepCell 的核心是受掩码语言建模启发的自监督掩码电路建模（MCM）策略，该策略将不同设计阶段的互补电路表示融合为统一且丰富的嵌入表示。据我们所知，DeepCell 是首个专为 PM 网表表示学习而设计的框架，在预测准确率和重构质量方面均树立了新的基准。通过将其应用于功能工程变更单（ECO）和工艺映射等关键 EDA 任务，我们验证了 DeepCell 的实际有效性。大量实验结果表明，DeepCell 在效率和性能上显著优于当前最先进的开源 EDA 工具。代码已公开，获取地址为 https://github.com/cure-lab/DeepCell。"
  },
  {
    "date": "2025-11-20",
    "title": "Augmented Co-Simulation for Fast Functional and System-Level Verification of HLS Accelerators",
    "authors": "Michele Fiorito, Serena Curzel, Fabrizio Ferrandi",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240905",
    "source": "IEEE",
    "abstract": "Functional verification of accelerator designs generated through High-Level Synthesis (HLS) requires users to write low-level testbenches and feed them to a Register-Transfer Level (RTL) simulator alongside the generated accelerator. Such a manual and error-prone process hinders early design space exploration due to the inherently slow RTL simulation phase, and it is unable to capture the interaction between the host application and the accelerated kernels. We propose two key improvements to the co-simulation process integrated within an HLS tool: a fast and accurate functional simulation methodology that calculates clock cycles during software execution of an annotated HLS intermediate representation, and a system-level co-simulation that couples RTL simulation with software execution of a host application through direct programming interfaces and inter-process communication. Both components augment the HLS verification process, enabling developers to extract kernels to be accelerated from a large application, quickly verify their expected performance after synthesis, and automatically validate their correctness by simulating a full host-accelerator system. Our augmented functional verification methodology is, on average, 7.0x faster than a state-of-the-art method and 36.2x faster than RTL simulation in terms of simulated cycles per second, with high accuracy and minimal memory overhead.",
    "title_zh": "面向HLS加速器快速功能与系统级验证的增强型协同仿真",
    "abstract_zh": "通过高层次综合（HLS）生成的加速器设计的功能验证，要求用户编写低层次的测试平台，并将其与生成的加速器一起输入到寄存器传输级（RTL）仿真器中进行仿真。这种手动且易出错的过程由于固有的RTL仿真阶段速度缓慢，严重阻碍了早期的设计空间探索，同时无法捕捉主机应用程序与加速内核之间的交互关系。我们针对集成在HLS工具中的协同仿真流程提出了两项关键改进：一是基于注释的HLS中间表示的软件执行过程中，快速而准确地计算时钟周期的功能仿真方法；二是通过直接编程接口和进程间通信，将RTL仿真与主机应用程序的软件执行相耦合的系统级协同仿真。这两个组件共同增强了HLS验证流程，使开发者能够从大型应用程序中提取待加速的内核，快速验证其综合后的预期性能，并通过模拟完整的主机-加速器系统自动验证其正确性。我们的增强型功能验证方法，在每秒仿真周期数方面，平均比最先进的方法快7.0倍，比RTL仿真快36.2倍，同时保持高精度并仅有极小的内存开销。"
  },
  {
    "date": "2025-11-20",
    "title": "From Concept to Practice: an Automated LLM-aided UVM Machine for RTL Verification",
    "authors": "Junhao Ye, Yuchen Hu, Ke Xu, Dingrong Pan, Qichun Chen, Jie Zhou, Shuai Zhao, Xinwei Fang, Xi Wang, Nan Guan, Zhe Jiang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240679",
    "source": "IEEE",
    "abstract": "Verification presents a major bottleneck in Integrated Circuit (IC) development, consuming nearly 70% of the total development effort. While the Universal Verification Methodology (UVM) is widely used in industry to improve verification efficiency through structured and reusable testbenches, constructing these testbenches and generating sufficient stimuli remain challenging. These challenges arise from the considerable manual coding effort required, repetitive manual execution of multiple EDA tools, and the need for in-depth domain expertise to navigate complex designs. Here, we present UVM<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>, an automated verification framework that leverages Large Language Models (LLMs) to generate UVM testbenches and iteratively refine them using coverage feedback, significantly reducing manual effort while maintaining rigorous verification standards. To evaluate UVM<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>, we introduce a benchmark suite comprising Register Transfer Level (RTL) designs of up to 1.6K lines of code. The results show that UVM<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> reduces testbench setup time by up to 38.82× compared to experienced engineers, and achieve average code and function coverage of 87.44% and 89.58%, outperforming state- of-the-art solutions by 20.96% and 23.51%, respectively.",
    "title_zh": "从概念到实践：一种自动化LLM辅助的UVM RTL验证机",
    "abstract_zh": "验证在集成电路（IC）开发中构成了主要瓶颈，耗时几乎占总开发工作量的70%。尽管业界广泛采用通用验证方法学（UVM）通过结构化和可重用的测试平台来提升验证效率，但构建这些测试平台以及生成充分的激励信号仍然面临巨大挑战。这些挑战源于大量需要手动编写的代码、重复执行多种EDA工具的繁琐操作，以及对复杂设计深入领域知识的需求。本文提出UVM²，一个基于大语言模型（LLM）的自动化验证框架，能够自动生成UVM测试平台，并利用覆盖率反馈进行迭代优化，在显著降低人工投入的同时，仍能保持严格的验证标准。为评估UVM²的性能，我们构建了一个基准测试套件，包含最多达1.6K行代码的寄存器传输级（RTL）设计。实验结果表明，与经验丰富的工程师相比，UVM²将测试平台搭建时间缩短了最高达38.82倍，平均代码覆盖率和功能覆盖率分别达到87.44%和89.58%，相较于当前最先进的解决方案分别提升了20.96%和23.51%。"
  },
  {
    "date": "2025-11-20",
    "title": "LambdaGo: A Functional Extension of the Go Programming Language",
    "authors": "Vlad Olteanu, Ciprian Oprişa",
    "publish": "2025 29th International Conference on System Theory, Control and Computing (ICSTCC)",
    "url": "https://doi.org/10.1109/icstcc66753.2025.11240319",
    "source": "IEEE",
    "abstract": "Networked programs and the tools to build them have become subjects of great interest with the rise of distributed computing and web services. A programming language built for such tasks is the Go programming language, which has a focus on high parallelism, a feature needed to serve large amounts of network requests. We created a functional extension to the Go programming language that allows the user to create pure functions that can be called from pre-existing Go programs. Our solution brings the strong points of functional programming such as function purity, partial function application, and higher-order functions while preserving the strengths already present in the Go programming language. The experimental results show that the LambdaGo programs perform as well as their Go counterparts in some instances while incurring some performance overhead in others due to the added abstractions.",
    "title_zh": "LambdaGo：Go编程语言的功能性扩展",
    "abstract_zh": "随着分布式计算和网络服务的兴起，网络化程序及其构建工具已成为人们广泛关注的焦点。为应对这些任务而设计的一种编程语言是Go语言，它特别注重高并发性，这一特性对于处理大量网络请求至关重要。我们为Go语言创建了一个功能扩展，使用户能够从现有的Go程序中调用纯函数。我们的解决方案在保留Go语言原有优势的同时，引入了函数式编程的诸多优点，如函数纯度、部分函数应用以及高阶函数等。实验结果表明，在某些情况下，LambdaGo程序的性能与对应的Go程序相当；而在其他情况下，则由于增加了抽象层而产生一定的性能开销。"
  },
  {
    "date": "2025-11-20",
    "title": "(Invited Paper) AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing",
    "authors": "Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240818",
    "source": "IEEE",
    "abstract": "Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit’s design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.",
    "title_zh": "（特邀论文）AnaFlow：基于智能体的LLM工作流，用于推理驱动的可解释且样本高效模拟电路尺寸设计",
    "abstract_zh": "模拟/混合信号电路是电子设备与物理世界进行交互的关键。然而，其设计过程至今仍主要依赖手工完成，导致设计周期长且容易出错。尽管近年来基于人工智能的强化学习和生成式AI技术为自动化这一任务带来了新方法，但大量耗时的仿真需求成为制约整体效率的关键瓶颈。此外，现有方法生成的设计方案缺乏可解释性，也阻碍了这些工具的广泛应用。为解决上述问题，本文提出了一种新型的智能体（agentic）AI框架，用于实现高效且可解释的模拟电路尺寸优化。该框架采用多智能体协同工作流程，由基于大型语言模型（LLM）的专用智能体共同协作，解析电路拓扑结构、理解设计目标，并通过人类可读的推理过程，迭代优化电路参数以达成预期目标。其自适应的仿真策略实现了智能化控制，显著提升了样本效率。所提出的AnaFlow框架已在两种不同复杂度的电路上得到验证，能够完全自动地完成尺寸优化任务，这与纯贝叶斯优化或强化学习方法形成鲜明对比。系统能够从过往优化历史中学习，避免重复错误并加速收敛。其固有的可解释性使其成为模拟设计空间探索的强大工具，标志着模拟EDA领域进入一个新范式——AI智能体作为透明化的设计助手，深度参与并辅助电路设计。"
  },
  {
    "date": "2025-11-20",
    "title": "Design of Machine Learning Accelerators as RISC-V Extensions using an Open Source Tool Flow",
    "authors": "Batuhan Sesli, Muhammad Sabih, Frank Hannig, Jürgen Teich",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240834",
    "source": "IEEE",
    "abstract": "The fast-evolving nature of machine learning applications demands agile hardware development to keep pace with innovation. Harnessing the customizability of the open-source RISC-V instruction set architecture (ISA), we present an automated methodology to accelerate activation functions in quantized long short-term memory (LSTM) networks through custom functional units as hardware extensions. One common approach to accelerate the computation of activation functions is to use approximation techniques, such as function tables, which enable a fast lookup but are, in turn, memory-intensive. To address this challenge, we analyze execution profiles of LSTM networks and present a flexible method for generating and exploring table-based function approximation. In order to reflect memory constraints, we propose splitting the input range of computationally expensive activation functions, such as sigmoid and hyperbolic tangent, into intervals using different quantization granularities for each subinterval. Our open-source design flow includes simulation-based verification, synthesis, placement, routing, and compilation. The results highlight the potential of table-based acceleration by addressing trade-offs between memory demand and accuracy, and provide an efficient hardware/software co-design solution for AI applications.",
    "title_zh": "使用开源工具流设计作为RISC-V扩展的机器学习加速器",
    "abstract_zh": "机器学习应用的快速演进要求硬件开发具备敏捷性以跟上创新步伐。借助开源RISC-V指令集架构（ISA）的可定制性，我们提出了一种自动化方法，通过将自定义功能单元作为硬件扩展，加速量化长短期记忆（LSTM）网络中的激活函数计算。一种常见的加速激活函数计算的方法是采用近似技术，例如函数查表法，虽然能够实现快速查找，但会带来较高的内存开销。为应对这一挑战，我们分析了LSTM网络的执行特征，并提出了一种灵活的方法，用于生成和探索基于表格的函数近似方案。为了反映内存约束，我们建议将计算开销较大的激活函数（如Sigmoid和双曲正切函数）的输入范围划分为多个区间，并对每个子区间采用不同的量化粒度。我们的开源设计流程包括基于仿真的验证、综合、布局、布线以及编译。实验结果表明，基于表格的加速方法在内存需求与精度之间实现了有效权衡，为人工智能应用提供了一种高效的软硬件协同设计解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: CMOS 2.0 - Redefining the Future of Scaling",
    "authors": "Moritz Brunion, Navaneeth Kunhi Purayil, Francesco Dell’Atti, Sebastian Lam, Refik Bilgic, Mehdi Tahoori, Luca Benini, Julien Ryckaert",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240655",
    "source": "IEEE",
    "abstract": "We propose to revisit the functional scaling paradigm by capitalizing on two recent developments in advanced chip manufacturing, namely 3D wafer bonding and backside processing. This approach leads to the proposal of the CMOS 2.0 platform. The main idea is to shift the CMOS roadmap from geometric scaling to fine-grain heterogeneous 3D stacking of specialized active device layers to achieve the ultimate Power-Performance-Area and Cost gains expected from future technology generations. However, the efficient utilization of such a platform requires devising architectures that can optimally map onto this technology, as well as the EDA infrastructure that supports it. We also discuss reliability concerns and eventual mitigation approaches. This paper provides pointers into the major disruptions we expect in the design of systems in CMOS 2.0 moving forward.",
    "title_zh": "特邀论文：CMOS 2.0——重新定义未来缩放之路",
    "abstract_zh": "我们提议通过利用先进芯片制造领域的两项最新进展——3D晶圆键合和背面工艺——重新审视功能缩放范式，从而提出CMOS 2.0平台。其核心思想是将CMOS技术路线图从几何缩放转向细粒度异构3D堆叠专用有源器件层，以实现未来技术世代所预期的终极功耗-性能-面积-成本（PPAC）提升。然而，要高效利用这一平台，必须设计能够最优适配该技术的架构，以及相应的EDA基础设施支持。本文还讨论了可靠性问题及相应的缓解策略。本论文为未来CMOS 2.0系统设计可能面临的重大变革提供了重要指引。"
  },
  {
    "date": "2025-11-20",
    "title": "NUA-Timer: Pre-Synthesis Timing Prediction Under Non-Uniform Input Arrival Times",
    "authors": "Ziyi Wang, Fangzhou Liu, Tsung-Yi Ho, David Z. Pan, Bei Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240652",
    "source": "IEEE",
    "abstract": "Accurate and swift pre-synthesis timing estimation is crucial for early-stage timing optimization and design space exploration. Recent advances in machine learning have shown significant promise in improving pre-synthesis timing prediction accuracy. However, existing learning-driven methods have overlooked the complexities introduced by the trending hierarchical design paradigm, specifically non-uniform input arrival times (NUIAT). In this paper, we present NUA-Timer, a novel pre-synthesis timing prediction framework designed to address the unique challenges posed by NUIAT in hierarchical timing prediction. To capture the complex long-range timing dependencies under varying NUIAT, NUA-Timer employs a novel bidirectional propagation neural network (BPN), which enables the quantification of timing dependencies using a correlation matrix. Furthermore, we introduce a tailored loss function that leverages post-synthesis critical path labels, thereby aligning the correlation matrix with actual post-synthesis timing dependencies. Comprehensive experiments on both synthetic and open-source designs demonstrate the superiority of our method compared to the state-of-the-art (SOTA) pre-synthesis timing evaluators.",
    "title_zh": "NUA-Timer：非均匀输入到达时间下的预综合时序预测",
    "abstract_zh": "精确且快速的综合前时序估算对于早期阶段的时序优化和设计空间探索至关重要。近年来，机器学习技术在提升综合前时序预测精度方面展现出巨大潜力。然而，现有的基于学习的方法忽视了日益流行的分层设计范式所带来的复杂性，特别是非均匀输入到达时间（NUIAT）问题。本文提出了一种名为NUA-Timer的新颖综合前时序预测框架，旨在解决NUIAT在分层时序预测中带来的独特挑战。为捕捉在不同NUIAT条件下复杂的长距离时序依赖关系，NUA-Timer采用了一种新型的双向传播神经网络（BPN），该网络能够通过相关矩阵量化时序依赖关系。此外，我们设计了一种定制化的损失函数，利用综合后关键路径标签，使相关矩阵与实际的综合后时序依赖关系保持一致。在合成设计和开源设计上的大量实验表明，与当前最先进的综合前时序评估方法相比，我们的方法具有显著优势。"
  },
  {
    "date": "2025-11-20",
    "title": "Blockchains: An overview of the technology, circuit primitives, and future opportunities",
    "authors": "Vikram B. Suresh, Sanu K. Mathew",
    "publish": "IEEE Solid-State Circuits Magazine",
    "url": "https://doi.org/10.1109/mssc.2025.3595831",
    "source": "IEEE",
    "abstract": "Blockchain technology, generally associated with cryptocurrencies, has unique properties that have the potential to disrupt a wide variety of domains spanning finance, governance, health care and supply chains. The research and development of hardware and IC design for blockchain applications have so far been limited to accelerating cryptocurrency mining operations. Researchers have demonstrated significant enhancement in the energy efficiency of cryptocurrency mining systems using novel circuit technologies, such as dynamic latches, three-phase nonoverlapped clocks, optimized hash engines, and voltage stacking. However, the applications of blockchain technology are much wider, and underpinning this powerful technology are silicon embedded root-of-trust circuits that deliver the security assurances that form the foundation of blockchains. These hardware primitives are responsible for generating secure keys/IDs, encrypting/decrypting blockchain data, and efficiently handling verifiable compute algorithms. The emerging applications of blockchain technology present a great opportunity for the IC design community to engage and innovate across circuit design, architecture, and algorithm co-optimizations. In this article, we present an overview of blockchain technology and its applications, followed by a discussion of the state of the art in mining ASIC circuits, foundational circuit primitives for crypto wallets, and future circuit research.",
    "title_zh": "区块链：技术概览、电路原语及未来机遇",
    "abstract_zh": "区块链技术通常与加密货币相关联，具有独特的属性，有望在金融、治理、医疗保健和供应链等多个领域引发广泛变革。迄今为止，针对区块链应用的硬件及集成电路（IC）设计的研究与开发主要局限于加速加密货币挖矿操作。研究人员已通过采用新型电路技术（如动态锁存器、三相非重叠时钟、优化的哈希引擎以及电压堆叠技术），显著提升了加密货币挖矿系统的能效。然而，区块链技术的应用远不止于此，其核心在于嵌入硅基的可信根（root-of-trust）电路，这些电路提供了区块链安全性的基础保障。这些硬件基本单元负责生成安全密钥/身份标识、加密/解密区块链数据，并高效处理可验证的计算算法。随着区块链技术的新兴应用不断涌现，为集成电路设计领域带来了巨大的机遇，促使该领域在电路设计、架构以及算法协同优化方面开展深入创新。本文首先概述区块链技术及其应用，随后讨论当前挖矿专用集成电路（ASIC）的技术进展、加密钱包的基础电路原语，以及未来电路研究的方向。"
  },
  {
    "date": "2025-11-20",
    "title": "ExactMap: Enhancing Delay Optimization in Parallel ASIC Technology Mapping",
    "authors": "Zhenxuan Xie, Lixin Liu, Tianji Liu, Evangeline F.Y. Young",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240698",
    "source": "IEEE",
    "abstract": "ASIC technology mapping consists of mapping a technology-independent Boolean network into an equivalent circuit utilizing cells from a specified library, a process that is vital in electronic design automation (EDA). However, existing algorithms in the literature are sequential in nature and often neglect to account for the actual delay of the cells during the mapping process, resulting in significant discrepancy between estimated and actual delay. In this paper, we propose an ASIC technology mapper that considers load information of intermediate solutions. This approach enables the identification of critical nodes and a better selection of delay-oriented cells for those nodes. Furthermore, we introduce a dual recovery method for delay and area to enhance performance. Finally, these innovations are integrated within a configuration-level parallelism framework on GPU. Experimental results on different technology libraries demonstrate that our method achieves on average 32% reduction in delay with 3% area penalty compared to the public synthesis tool ABC. Additionally, our approach provides a significant speedup of 65.33×.",
    "title_zh": "ExactMap：在并行ASIC技术映射中提升延迟优化",
    "abstract_zh": "ASIC技术映射是指将与技术无关的布尔网络映射到使用特定单元库中单元构成的等效电路，这一过程在电子设计自动化（EDA）中至关重要。然而，现有文献中的算法大多为串行处理，且通常忽略单元的实际延迟信息，导致估算延迟与实际延迟之间存在显著差异。本文提出了一种考虑中间解负载信息的ASIC技术映射方法，该方法能够识别关键节点，并为这些节点更优地选择延迟导向型单元。此外，我们引入了一种用于延迟和面积的双重优化恢复机制，以进一步提升性能。最后，上述创新被整合进基于GPU的配置级并行计算框架中。在多个技术库上的实验结果表明，与公开的综合工具ABC相比，本方法平均可实现32%的延迟降低，仅带来3%的面积开销；同时，整体运行速度提升了65.33倍。"
  },
  {
    "date": "2025-11-20",
    "title": "ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing",
    "authors": "Ondrej Vlcek, Vojtech Mrazek",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240776",
    "source": "IEEE",
    "abstract": "Approximate computing offers promising energy efficiency benefits for error-tolerant applications, but discovering optimal approximations requires extensive design space exploration (DSE). Predicting the accuracy of circuits composed of approximate components without performing complete synthesis remains a challenging problem. Current machine learning approaches used to automate this task require retraining for each new circuit configuration, making them computationally expensive and time-consuming.This paper presents ApproxGNN, a construction methodology for a pre-trained graph neural network model predicting QoR and HW cost of approximate accelerators employing approximate adders from a library. This approach is applicable in DSE for assignment of approximate components to operations in accelerator. Our approach introduces novel component feature extraction based on learned embeddings rather than traditional error metrics, enabling improved transferability to unseen circuits. ApproxGNN models can be trained with a small number of approximate components, supports transfer to multiple prediction tasks, utilizes precomputed embeddings for efficiency, and significantly improves accuracy of the prediction of approximation error. On a set of image convolutional filters, our experimental results demonstrate that the proposed embeddings improve prediction accuracy (mean square error) by 50% compared to conventional methods. Furthermore, the overall prediction accuracy is 30% better than statistical machine learning approaches without fine-tuning and 54% better with fast fine-tuning. The proposed methodology effectively addresses the challenge of transferring knowledge across different circuit designs without requiring expensive retraining. We provide our implementation, including the graph generation tools and pretrained models, as an open-source library to facilitate further research in this area.",
    "title_zh": "ApproxGNN：一种用于近似计算设计空间探索中参数预测的预训练图神经网络",
    "abstract_zh": "近似计算为容错应用提供了显著的能效优势，但发现最优近似方案需要进行广泛的设计空间探索（DSE）。在不执行完整综合的情况下预测由近似组件构成的电路精度，仍是极具挑战性的问题。当前用于自动化该任务的机器学习方法，针对每种新的电路配置都需要重新训练，导致计算成本高且耗时。本文提出ApproxGNN，一种基于预训练图神经网络模型的构建方法，可预测采用库中近似加法器的近似加速器的QoR（质量与资源权衡）和硬件成本。该方法适用于DSE中将近似组件分配给加速器内运算的过程。我们的方法引入了一种基于学习嵌入的新组件特征提取方式，而非传统的误差度量，从而显著提升了对未见电路的迁移能力。ApproxGNN模型仅需少量近似组件即可完成训练，支持多种预测任务的迁移，利用预先计算的嵌入以提高效率，并大幅提升了近似误差预测的准确性。在一组图像卷积滤波器上的实验结果表明，所提出的嵌入方法相比传统方法将预测精度（均方误差）提高了50%。此外，在无需微调的情况下，整体预测精度比统计机器学习方法高出30%；经过快速微调后，精度提升达54%。所提出的框架有效解决了不同电路设计间知识迁移的难题，而无需昂贵的重新训练过程。我们已将实现代码（包括图生成工具和预训练模型）开源发布，以促进该领域的进一步研究。"
  },
  {
    "date": "2025-11-20",
    "title": "Differentiable Physical Optimization",
    "authors": "Yufan Du, Zizheng Guo, Runsheng Wang, Yibo Lin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240839",
    "source": "IEEE",
    "abstract": "Gate sizing and buffer insertion are crucial for VLSI physical optimization; however, conventional decoupled approaches often yield suboptimal solutions due to uncoordinated resource allocation. Existing simultaneous methods resort to oversimplified timing models or heuristic assumptions, failing to unify the two tasks mathematically rigorously. We present a differentiable physical optimization framework integrating both techniques with GPU acceleration. Key innovations include timing-aware buffer tree skeleton construction, physics-aware modeling, and discrete-aware optimization algorithms. Experiments demonstrate 23% total negative slack (TNS) improvement and 12% worst negative slack (WNS) improvement with similar power consumption and 30× speedup versus CPU-based optimization flow. This work establishes a new paradigm for co-optimizing interdependent physical design tasks with rigorous modeling and efficient computation.",
    "title_zh": "可微分物理优化",
    "abstract_zh": "门尺寸调整与缓冲器插入是VLSI物理优化中的关键步骤；然而，传统的解耦方法由于资源分配缺乏协调，常常导致次优结果。现有的联合优化方法往往依赖过于简化的时序模型或启发式假设，无法在数学上严格统一这两个任务。本文提出一种可微分的物理优化框架，结合了门尺寸调整与缓冲器插入技术，并采用GPU加速。其主要创新包括：面向时序的缓冲树骨架构建、考虑物理特性的建模方法以及兼顾离散特性的优化算法。实验表明，该方法在保持功耗相近的情况下，实现了23%的总负 slack（TNS）改善和12%的最差负 slack（WNS）改善，同时相比基于CPU的优化流程提升了30倍的速度。本工作为协同优化相互依赖的物理设计任务建立了一种新的范式，兼具严格的建模基础与高效的计算能力。"
  },
  {
    "date": "2025-11-20",
    "title": "IssueCourier: Multi-Relational Heterogeneous Temporal Graph Neural Network for Open-Source Issue Assignment",
    "authors": "Chunying Zhou, Xiaoyuan Xie, Gong Chen, Peng He, Bing Li",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2025.3634192",
    "source": "IEEE",
    "abstract": "Issue assignment plays a critical role in open-source software (OSS) maintenance, which involves recommending the most suitable developers to address the reported issues. Given the high volume of issue reports in large-scale projects, manually assigning issues is tedious and costly. Previous studies have proposed automated issue assignment approaches that primarily focus on modeling issue report textual information, developers’ expertise, or interactions between issues and developers based on historical issue-fixing records. However, these approaches often suffer from performance limitations due to the presence of incorrect and missing labels in OSS datasets, as well as the long tail of developer contributions and the changes in developer activity as the project evolves. To address these challenges, we propose IssueCourier, a novel Multi-Relational Heterogeneous Temporal Graph Neural Network approach for issue assignment. Specifically, we formalize five key relationships among issues, developers, and source code files to construct a heterogeneous graph. Then, we further adopt a temporal slicing technique that partitions the graph into a sequence of time-based subgraphs to learn stage-specific patterns. Furthermore, we provide a benchmark dataset with relabeled ground truth to address the problem of incorrect and missing labels in existing OSS datasets. Finally, to evaluate the performance of IssueCourier, we conduct extensive experiments on our benchmark dataset. The results show that IssueCourier can improve over the best baseline up to 45.49% in top-1 and 31.97% in MRR.",
    "title_zh": "IssueCourier：用于开源项目问题分配的多关系异构时序图神经网络",
    "abstract_zh": "问题分配在开源软件（OSS）维护中起着至关重要的作用，其核心在于为报告的问题推荐最合适的开发者进行修复。由于大规模项目中存在海量的问题报告，人工分配问题不仅耗时费力，而且成本高昂。以往的研究提出了多种自动化的问题分配方法，主要聚焦于建模问题报告的文本信息、开发者的专业能力，或基于历史修复记录中问题与开发者之间的交互关系。然而，这些方法常常因开源数据集中存在的错误标签和缺失标签，以及开发者贡献分布的长尾现象、项目演进过程中开发者活跃度的变化等因素，导致性能受限。为应对上述挑战，我们提出了一种名为 IssueCourier 的新型多关系异构时序图神经网络方法，用于解决问题分配问题。具体而言，我们形式化了问题、开发者与源代码文件之间五种关键关系，构建了一个异构图结构；随后引入一种时序切片技术，将图划分为一系列基于时间的子图，以学习不同阶段的特征模式。此外，我们还构建了一个经过重新标注真实标签的基准数据集，以缓解现有 OSS 数据集中标签不准确和缺失的问题。最后，为评估 IssueCourier 的性能，我们在该基准数据集上进行了大量实验。结果表明，IssueCourier 在 top-1 准确率上相比最佳基线最高提升 45.49%，在 MRR（平均倒数排名）指标上最高提升 31.97%。"
  },
  {
    "date": "2025-11-20",
    "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances",
    "authors": "Chase Stokes, Kylie Lin, Cindy Xiong Bearfield",
    "publish": "IEEE Transactions on Visualization and Computer Graphics",
    "url": "https://doi.org/10.1109/tvcg.2025.3633872",
    "source": "IEEE",
    "abstract": "A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.",
    "title_zh": "撰写、排名或评分：比较研究可视化 affordance 的方法",
    "abstract_zh": "越来越多的研究关注可视化表现力（visualization affordances），揭示了特定的设计选择如何影响读者对信息可视化结果的理解与认知。然而，要准确映射设计选择与读者结论之间的关系，通常需要耗费大量人力的众包研究，生成大量自由回答文本以供分析。为应对这一挑战，我们探索了替代性的可扩展研究方法，以评估图表的表现力。本研究测试了四种来自人类受试者实验的引出方法：自由回答、可视化排序、结论排序和显著性评分，并比较它们在获取读者对折线图、散点图和热力图理解方面的有效性。总体而言，我们发现尽管没有任何一种方法能完全复现自由回答中观察到的表现力特征，但结合使用排序与评分方法可在大规模研究中作为有效的替代方案。两种排序方法均受到参与者对特定图表类型的偏好以及所提结论之间比较的影响；而评分结论显著性则未能捕捉到其他方法中观察到的不同图表类型间的细微差异。为进一步补充本研究，我们引入了一个案例研究，利用GPT-4o探讨大型语言模型（LLMs）在生成类人化图表解读方面的潜力。这与近期学术界对利用大语言模型作为人类受试者的代理以提升数据收集与分析效率的兴趣相契合。结果显示，GPT-4o在显著性评分方法中表现最佳，但在其他方面存在严重局限。总体而言，不同引出方法（包括GPT-4o）之间表现出的性能差异，凸显了在研究设计中应有意识地选择并组合方法，同时权衡其优劣的重要性。"
  },
  {
    "date": "2025-11-20",
    "title": "A Complete Modeling Methodology for Full-chip Parasitic Extraction",
    "authors": "Yipei Xu, Zhisheng Zeng, Qing He",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240982",
    "source": "IEEE",
    "abstract": "As the semiconductor process node continues to shrink, on-chip parasitics become more and more important. However, a comprehensive methodology for the extraction of parasitic parameters on a full chip has rarely been discussed in the literature. In this work, we propose a full-flow RC extraction methodology for modern circuit design, as well as a systematic benchmark framework. Considering the shielding effect, we propose a geometric distribution of the \"ring\" structure and three types of corresponding patterns. We adopt a spatial temporal range tree (STR tree) to obtain potential aggressors and use event-based sweeping lines to get the true aggressor for wire segmentation and environment search. To approximate the process effect on the conductor’s resistivity, we assume a fixed metal density to obtain the corresponding resistance per square (RPSQ) and then experiment to obtain the optimal metal density. The experimental results show the effectiveness of our methodology and demonstrate that our extractor performs better than OpenRCX and PEX on multiple metrics.",
    "title_zh": "全芯片寄生参数提取的完整建模方法",
    "abstract_zh": "随着半导体工艺节点的不断缩小，片上寄生参数的影响变得越来越重要。然而，关于全芯片寄生参数提取的综合性方法在文献中却很少被讨论。本文提出了一种适用于现代电路设计的完整RC提取流程，以及一个系统化的基准测试框架。考虑到屏蔽效应，我们提出了“环形”结构的几何分布及其三种相应的布线模式。采用时空范围树（STR树）来识别潜在的干扰源，并利用基于事件的扫描线算法确定真实的干扰源，以实现导线分割与环境搜索。为了近似工艺对导体电阻率的影响，我们假设金属密度固定，从而获得对应的每方块电阻（RPSQ），并通过实验确定最优金属密度。实验结果表明，所提方法具有良好的有效性，且在多项指标上优于OpenRCX和PEX。"
  },
  {
    "date": "2025-11-20",
    "title": "G-Contour: GPU Accelerated Contour Tracing For Large-Scale Layouts",
    "authors": "Shuo Yin, Jiahao Xu, Jiaxi Jiang, Mingjun Li, Yuzhe Ma, Tsung-Yi Ho, Bei Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240932",
    "source": "IEEE",
    "abstract": "Contour tracing is a fundamental operation in computer vision and image processing, with applications ranging from object recognition to shape analysis. In the field of electronic design automation (EDA), contour tracing plays a critical role in layout processing tasks such as lithography simulation and mask optimization. In this paper, we present G-Contour, the first GPU-accelerated contour tracing framework, designed to efficiently handle large-scale layouts. G-Contour incorporates several GPU-accelerated image processing algorithms based on parallel geometry techniques to achieve significant speedups over traditional CPU-based methods. We evaluate G-Contour in real-world VLSI applications involving large-scale layout processing. The experimental results demonstrate that G-Contour achieves a speedup of over 90× compared to state-of-the-art CPU-based contour tracing frameworks. Moreover, G-Contour could be a versatile tool that extends beyond VLSI applications, with potential applicability in various domains of computer vision and image processing, making it a valuable resource for both researchers and practitioners.",
    "title_zh": "G-Contour：用于大规模布局的GPU加速轮廓追踪",
    "abstract_zh": "轮廓追踪是计算机视觉和图像处理中的基本操作，广泛应用于物体识别、形状分析等领域。在电子设计自动化（EDA）领域，轮廓追踪在版图处理任务中起着关键作用，例如光刻仿真和掩模优化。本文提出G-Contour，这是首个基于GPU加速的轮廓追踪框架，旨在高效处理大规模版图数据。G-Contour结合了多种基于并行几何技术的GPU加速图像处理算法，在速度上显著优于传统的CPU方法。我们在涉及大规模版图处理的真实世界VLSI应用中对G-Contour进行了评估，实验结果表明，与当前最先进的CPU基轮廓追踪框架相比，G-Contour实现了超过90倍的加速效果。此外，G-Contour具有广泛的适用性，不仅限于VLSI领域，还可拓展至计算机视觉和图像处理的多个应用场景，因此对研究人员和实践者均具有重要价值。"
  },
  {
    "date": "2025-11-20",
    "title": "HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis",
    "authors": "Kangwei Xu, Bing Li, Grace Li Zhang, Ulf Schlichtmann",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240891",
    "source": "IEEE",
    "abstract": "In high-level synthesis (HLS), C/C++ programs with synthesis directives are used to generate circuits for FPGA implementations. However, hardware-specific and platform-dependent characteristics in circuit implementations can introduce behavioral discrepancies between the original C/C++ programs and the circuits after high-level synthesis. Existing methods for testing behavioral discrepancies in HLS are still immature, and the testing workflow requires significant human efforts. To address this challenge, we propose HLSTester, a large language model (LLM) aided testing framework that efficiently detects behavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance prompt quality, existing C/C++ testbenches are used to guide the LLM to generate HLS-compatible versions, effectively eliminating certain traditional C/C++ syntax that are incompatible with HLS tools. Key variables are pinpointed through a backward slicing technique in both C/C++ and HLS programs to monitor their runtime spectra, enabling an in-depth analysis of the discrepancy symptoms. Then, a testing input generation mechanism is introduced to integrate dynamic mutation with insights from an LLM-based progressive reasoning chain. In addition, repetitive hardware testing is skipped by a redundancy-aware technique for the generated test inputs. Experimental results demonstrate that the proposed LLM-aided testing framework significantly accelerates the testing workflow while achieving higher testbench simulation pass rates compared with the traditional method and the direct use of LLMs on the same HLS programs.",
    "title_zh": "HLSTester：基于大语言模型的高效高阶综合行为差异测试方法",
    "abstract_zh": "在高层次综合（HLS）中，带有综合指令的C/C++程序被用于生成适用于FPGA实现的电路。然而，电路实现过程中存在的硬件特定性与平台依赖性特征，可能导致原始C/C++程序与经高层次综合后的电路之间出现行为差异。目前针对HLS中行为差异的测试方法仍不成熟，测试流程需要大量人工参与。为应对这一挑战，我们提出HLSTester——一种由大语言模型（LLM）辅助的测试框架，能够高效检测HLS中的行为差异。为减少LLM幻觉并提升提示质量，本方法利用现有的C/C++测试用例引导LLM生成与HLS兼容的版本，有效消除传统C/C++语法中与HLS工具不兼容的部分。通过在C/C++程序和HLS程序中分别采用反向切片技术精确定位关键变量，并监控其运行时谱图，从而深入分析差异表现。随后，引入一种测试输入生成机制，将动态变异与基于LLM的渐进式推理链所得洞察相结合。此外，通过一种冗余感知技术，跳过对已生成测试输入的重复硬件测试，进一步提高效率。实验结果表明，所提出的LLM辅助测试框架显著加速了测试流程，在测试用例仿真通过率方面也优于传统方法以及直接在相同HLS程序上使用LLM的方案。"
  },
  {
    "date": "2025-11-20",
    "title": "Clay: High-level ASIP Framework for Flexible Microarchitecture-Aware Instruction Customization",
    "authors": "Weijie Peng, Youwei Xiao, Yuyang Zou, Zizhang Luo, Yun Liang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240669",
    "source": "IEEE",
    "abstract": "Application-specific instruction-set processors (ASIPs) pro-vide energy-efficient acceleration for embedded systems and IoT devices. The free and open RISC-V ISA promotes open-source ASIP solutions to accelerate diverse application domains. Existing ASIP tools generate hardware and software artifacts from high-level architecture description languages (ADLs), however, they only support the in-pipeline coupling strategy on specific processors. As a result, they suffer from two critical limitations: they restrict instruction extensions to stateless behavior, preventing hardware implementation of efficient control flow like loops, and they impose rigid microarchitectural constraints that limit register file and memory interactions. These restrictions create a fundamental bottleneck in application acceleration and prevent the efficient deployment of custom instructions across different processors.We introduce Clay, an open-source high-level ASIP framework that overcomes these limitations. Clay introduces a unified instruction extension interface that abstracts different coupling strategies as microarchitecture-agnostic actions and microarchitectural attributes. Clay ADL (CADL) combines the interface actions and high-level syntax to describe general instruction behavior, which can be stateful. We further propose a microarchitecture-aware synthesis flow that selects the best coupling strategy for each custom instruction and schedules the optimal implementation with microarchitectural attributes modeled as constraints. Our evaluation of diverse workloads demonstrates that Clay delivers substantial performance improvements across two RISC-V processors, our custom Clay-core and the open-source Rocket-core.",
    "title_zh": "克莱伊：用于灵活微架构感知指令定制的高级ASIP框架",
    "abstract_zh": "应用特定指令集处理器（ASIPs）为嵌入式系统和物联网设备提供了高效的能效加速。开放自由的RISC-V指令集架构（ISA）推动了开源ASIP解决方案的发展，从而加速了多样化应用领域的演进。现有的ASIP工具能够从高级架构描述语言（ADL）生成硬件与软件代码，但它们仅支持在特定处理器上采用流水线内耦合策略。因此，这些工具存在两个关键局限：其一，将指令扩展限制为无状态行为，无法实现如循环等高效控制流的硬件化；其二，施加了僵化的微架构约束，限制了寄存器文件与内存之间的灵活交互。这些限制构成了应用加速的根本瓶颈，阻碍了自定义指令在不同处理器间的高效部署。\n\n我们提出了Clay——一个开源的高层次ASIP框架，旨在克服上述局限。Clay引入了一种统一的指令扩展接口，将不同的耦合策略抽象为与微架构无关的操作和微架构属性。Clay架构描述语言（CADL）结合了该接口的操作与高级语法，用于描述具有状态性的通用指令行为。此外，我们提出了一种面向微架构的综合流程，能够为每条自定义指令选择最优的耦合策略，并基于建模为约束条件的微架构属性，调度出最佳实现方案。对多种工作负载的评估表明，Clay在两款RISC-V处理器——我们自研的Clay-core以及开源的Rocket-core上均实现了显著的性能提升。"
  },
  {
    "date": "2025-11-20",
    "title": "Prompt, Fab, Flex: Agentic LLMs for Flexible Electronics Design",
    "authors": "Farshad Firouzi, Bahar Farahani, Polykarpos Vergos, Deepesh Sahoo, Nathaniel Bleier, Krishnendu Chakrabarty",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240714",
    "source": "IEEE",
    "abstract": "Flexible Electronics (FE) have emerged as a promising platform for extreme edge applications that demand attributes tailored to the application domain, such as ultra-low cost, low power consumption, mechanical flexibility, biocompatibility, and environmental sustainability. While advances in printed and flexible device technologies have demonstrated the feasibility of sensing, computing, and communication on deformable substrates, the design and implementation of FE-based systems remain limited by traditional Electronic Design Automation (EDA) workflows, which are complex, time-intensive, and largely inaccessible to non-experts. In parallel, recent progress in Large Language Models (LLMs) has enabled automation across multiple stages of integrated circuit design; however, existing approaches exclusively target conventional silicon technologies and are not designed to address the unique constraints of FE. This work introduces the first LLM-driven framework for end-to-end hardware design automation in flexible electronics. The proposed methodology supports Register-Transfer Level (RTL) generation, logic synthesis, and cross-layer Power–Performance–Area (PPA) Design Space Exploration (DSE) for bespoke Machine Learning (ML) classifiers. Experimental results demonstrate the feasibility and effectiveness of the approach in generating resource-efficient hardware designs optimized for FE, thereby lowering barriers to adoption and accelerating the development of personalized, application-specific FEs.",
    "title_zh": "提示，Fab，Flex：用于柔性电子设计的自主型大语言模型",
    "abstract_zh": "柔性电子（Flexible Electronics, FE）已成为极端边缘应用中极具前景的平台，能够满足特定应用领域对超低成本、低功耗、机械柔韧性、生物相容性以及环境可持续性等特性的需求。尽管印刷与柔性器件技术的进步已证明在可变形基底上实现传感、计算和通信的可行性，但传统电子设计自动化（Electronic Design Automation, EDA）流程在柔性电子系统的设计与实现方面仍存在诸多局限：这些流程复杂、耗时长，且对非专业用户极不友好。与此同时，大型语言模型（Large Language Models, LLMs）近年来在集成电路设计多个阶段实现了自动化，然而现有方法均聚焦于传统的硅基技术，未能针对柔性电子所特有的约束条件进行优化。本文首次提出一种基于大语言模型的端到端硬件设计自动化框架，专为柔性电子领域量身打造。该方法支持寄存器传输级（Register-Transfer Level, RTL）代码生成、逻辑综合，以及跨层次的功耗-性能-面积（Power–Performance–Area, PPA）设计空间探索（Design Space Exploration, DSE），以定制化机器学习（Machine Learning, ML）分类器为目标。实验结果表明，该方法在生成资源高效、面向柔性电子优化的硬件设计方面具有可行性和有效性，显著降低了技术门槛，加速了个性化、专用化柔性电子产品的研发进程。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Resource Management on Heterogeneous Chiplets Systems",
    "authors": "Wanli Chang, Yili Guo, Weijie Wang, Yaqi Yao, Fuyang Zhao, Yinjie Fang, Kuan Jiang, Liyun Shang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240955",
    "source": "IEEE",
    "abstract": "Chiplets promise customized design for different sets of applications with heterogeneity, where there are various kinds of resources to manage. Examples are GPUs, NPUs, and CPUs for computation of AI and general tasks, memory, communication, as well as logical resources such as I/Os. There can be strong contention in access of resources across chiplets by applications, which sometimes have discrepant requirements. Resolving such contention is critical to fully exploit the resources on heterogeneous chiplets and satisfy the needs of dynamic application workloads. This talk will discuss a set of methods that manage these resources during the design phase as well as at runtime, which should interact with the architecture exploration and contribute to the entire chiplets systems design altogether.",
    "title_zh": "特邀论文：异构芯粒系统中的资源管理",
    "abstract_zh": "芯粒（Chiplets）为不同应用场景提供了定制化设计的可能，具有异构特性，需要管理多种类型的资源。例如，用于人工智能和通用计算任务的GPU、NPU和CPU，以及内存、通信资源，还有诸如I/O等逻辑资源。由于应用程序在跨芯粒访问资源时可能存在强烈竞争，而这些应用的需求又各不相同，因此解决这种资源争用问题对于充分挖掘异构芯粒上的资源潜力、满足动态应用工作负载的需求至关重要。本次演讲将探讨一系列在设计阶段及运行时管理这些资源的方法，这些方法应与架构探索相协同，共同促进整个芯粒系统的设计优化。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Multi-Agent Generative Synthesis for Analog/RF Circuit: from Scalable Topology Generation to Efficient Inverse Design",
    "authors": "Shikai Wang, Qiufeng Li, Houbo He, Jian Gao, Zining Wang, Yu Sun, Xuan Zhang, Taiyun Chi, Weidong Cao",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240792",
    "source": "IEEE",
    "abstract": "The exponential growth of information and computational workloads has created unprecedented demands for high-productivity development of computer hardware built on foundational semiconductor integrated circuits (ICs). Yet, the lack of effective design automation techniques makes developing analog ICs–indispensable in ubiquitous computer systems–a significant bottleneck for overall design productivity and cost efficiency across the IC ecosystem. Excitingly, recent advances in generative AI present transformative opportunities to tackle the complexity and large-scale challenges of modern analog/radio-frequency (RF) IC design. This work introduces a first-of-its-kind multi-agent generative synthesis framework for analog/RF circuits. Specifically, our approach formulates analog synthesis as a multi-stage generative AI problem: first generating a novel topology conditioned on high-level textual descriptions, and then producing high-quality device parameters to meet given design specifications for the generated circuit topology. This novel paradigm offers distinct advantages over traditional methods, including controllable novelty in topology generation and significantly improved inverse design efficiency. Beyond topology generation and inverse design, our method enables broader applications such as synthetic dataset generation and privacy-preserving data sharing–emerging challenges in data-driven electronic design automation (EDA) due to the computationally intensive and confidential nature of analog/RF circuit design. This work paves the way for next-generation generative AI-driven multi-agent synthesis in analog/RF EDA.",
    "title_zh": "特邀论文：面向模拟/射频电路的多智能体生成合成：从可扩展拓扑生成到高效逆向设计",
    "abstract_zh": "信息量和计算工作负载的指数级增长，对基于基础半导体集成电路（IC）的计算机硬件高生产率开发提出了前所未有的需求。然而，缺乏有效的设计自动化技术，使得模拟IC——在无处不在的计算机系统中不可或缺——的开发成为整个IC生态系统中设计生产力和成本效率的重大瓶颈。令人振奋的是，生成式人工智能的最新进展为应对现代模拟/射频（RF）IC设计所面临的复杂性和大规模挑战提供了变革性的机遇。本文提出了一种开创性的多智能体生成式综合框架，用于模拟/RF电路设计。具体而言，我们的方法将模拟电路综合建模为一个多阶段生成式AI问题：首先根据高层次的文本描述生成新颖的电路拓扑结构，然后针对所生成的拓扑结构，生成满足给定设计规格的高质量器件参数。这一新范式相较于传统方法具有显著优势，包括在拓扑生成中实现可控的新颖性，以及逆向设计效率的大幅提升。除了拓扑生成与逆向设计之外，该方法还拓展了更广泛的应用场景，如合成数据集生成和隐私保护的数据共享——这些正是由于模拟/RF电路设计具有计算密集且高度机密的特性，在数据驱动的电子设计自动化（EDA）领域中日益凸显的新兴挑战。本研究为下一代生成式AI驱动的多智能体模拟/RF EDA综合技术铺平了道路。"
  },
  {
    "date": "2025-11-20",
    "title": "e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving",
    "authors": "Jiaqi Yin, Zhan Song, Chen Chen, Yaohui Cai, Zhiru Zhang, Cunxi Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240719",
    "source": "IEEE",
    "abstract": "E-graphs have attracted growing interest in many fields, particularly in logic synthesis and formal verification. E-graph extraction is a challenging NP-hard combinatorial optimization problem. It requires identifying optimal terms from exponentially many equivalent expressions, serving as the primary performance bottleneck in e-graph based optimization tasks. However, traditional extraction methods face a critical trade-off: heuristic approaches offer speed but sacrifice optimality, while exact methods provide optimal solutions but face prohibitive computational costs on practical problems. We present e-boost, a novel framework that bridges this gap through three key innovations: (1) parallelized heuristic extraction that leverages weak data dependence to compute DAG costs concurrently, enabling efficient multi-threaded performance without sacrificing extraction quality; (2) adaptive search space pruning that employs a parameterized threshold mechanism to retain only promising candidates, dramatically reducing the solution space while preserving near-optimal solutions; and (3) initialized exact solving that formulates the reduced problem as an Integer Linear Program with warm-start capabilities, guiding solvers toward high-quality solutions faster.Across the diverse benchmarks in formal verification and logic synthesis fields, e-boost demonstrates 558× runtime speedup over traditional exact approaches (ILP) and 19.04% performance improvement over the state-of-the-art extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost produces 7.6% and 8.1% area improvements compared to conventional synthesis tools with two different technology mapping libraries. e-boost is available at https://github.com/Yu-Maryland/e-boost.",
    "title_zh": "e-boost：基于自适应启发式与精确求解的增强型E图提取",
    "abstract_zh": "e-graphs 在多个领域引起了越来越多的关注，尤其是在逻辑综合与形式化验证方面。e-graph 提取是一个具有挑战性的 NP-hard 组合优化问题，需要从指数级数量的等价表达式中识别出最优项，这成为基于 e-graph 优化任务中的主要性能瓶颈。然而，传统提取方法面临一个关键权衡：启发式方法虽然速度快，但牺牲了最优性；而精确方法虽能提供最优解，却在实际问题上面临难以承受的计算开销。我们提出了 e-boost，一种通过三项关键创新弥合这一差距的新框架：（1）并行化的启发式提取，利用弱数据依赖性实现 DAG 成本的并发计算，从而在不牺牲提取质量的前提下，实现高效的多线程性能；（2）自适应搜索空间剪枝，采用参数化阈值机制仅保留有前景的候选解，大幅缩减解空间的同时仍能保持接近最优的解决方案；（3）初始化精确求解，将简化后的问题建模为带热启动能力的整数线性规划（ILP），引导求解器更快地收敛到高质量解。在形式化验证和逻辑综合领域的多样化基准测试中，e-boost 相较于传统的精确方法（ILP）实现了 558 倍的运行时加速，并比当前最先进的提取框架 SmoothE 提升了 19.04% 的性能。在真实的逻辑综合任务中，e-boost 相较于传统综合工具，在两种不同的技术映射库下分别实现了 7.6% 和 8.1% 的面积优化。e-boost 已开源，项目地址为 https://github.com/Yu-Maryland/e-boost。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Hardware-Software Co-Design for Highly Optimized, Customized, and Reliable AI Systems",
    "authors": "Jörg Henkel, Mehdi Tahoori, Heba Khdr, Hassan Nassar, Vincent Meyers, Deming Chen, Selin Yildirim, Yingbing Huang, Nirmal R. Saxena, Saurabh Hukerikar, Srivi Dhruvanarayan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240678",
    "source": "IEEE",
    "abstract": "Over the past decade, AI has been rapidly integrated into our daily life, coming in every shape and size and working across systems from big clouds to IoT. As a result, AI systems are increasingly requiring enhancements in model efficiency, hardware acceleration, and memory systems to satisfy stringent constraints on efficiency, reliability, and security. However, advancing across these fronts is challenging as compute demand outpaces Moore’s-law efficiency, hardening into an AI compute wall and an AI energy wall. Breaking through requires a unified AI co-design loop that co-optimizes algorithms and hardware, including efficient AI-to-hardware mapping, so that ongoing goals (accuracy, sparsity, latency) align with concrete hardware choices (precision modes, interconnects, memory hierarchies) and AI-specific execution and memory-reuse patterns. This paper details the principal co-design challenges, presents complementary strategies, and outlines a practical roadmap toward highly optimized, efficient, reliable, and secure AI systems.",
    "title_zh": "特邀论文：面向高度优化、定制化与可靠人工智能系统的软硬件协同设计",
    "abstract_zh": "过去十年中，人工智能（AI）已迅速融入我们的日常生活，以各种形态和规模存在于从大型云平台到物联网（IoT）的各类系统中。因此，AI系统日益需要在模型效率、硬件加速和内存系统方面进行提升，以满足对效率、可靠性和安全性方面的严格要求。然而，要在这几个领域持续进步面临巨大挑战，因为计算需求的增长已超越摩尔定律所描述的效率提升速度，逐渐形成“AI算力墙”和“AI能耗墙”。突破这一瓶颈，需要建立一个统一的AI协同设计循环，实现算法与硬件的联合优化，包括高效的AI到硬件映射，使持续追求的目标（如精度、稀疏性、延迟）与具体的硬件选择（如精度模式、互连结构、内存层次）以及针对AI特性的执行方式和内存重用模式相匹配。本文详细阐述了主要的协同设计挑战，提出了互补的应对策略，并勾勒出一条通往高度优化、高效、可靠且安全的AI系统的实用发展路线。"
  },
  {
    "date": "2025-11-20",
    "title": "XSSnake: A Practical Study in Synthetic XSS Payload Generation",
    "authors": "Maria-Gabriela Fodor, Teodor Gorghe",
    "publish": "2025 29th International Conference on System Theory, Control and Computing (ICSTCC)",
    "url": "https://doi.org/10.1109/icstcc66753.2025.11240360",
    "source": "IEEE",
    "abstract": "Cross-Site Scripting remains a prevalent vulnerability in modern web applications, despite the widespread adoption of output encoding libraries, hardened browser environments, and policy-driven security mechanisms such as Content Security Policy. A core challenge lies in the ever-expanding and rapidly evolving nature of the attack surface. As HTML parsing rules and JavaScript semantics continue to evolve, new contexts emerge where malicious scripts can be injected and executed. Simultaneously, attackers continually refine their filter-evasion techniques, further complicating defense efforts. Security testing workflows commonly rely on static payload collections of known attack strings curated over time. While these datasets are crucial, they often fail to keep pace with emerging attack strategies, potentially overlooking novel or subtle injection vectors. This paper highlights the resulting gap in evaluation coverage and explores the potential of automatically generated payloads as a forward-looking research avenue. In particular, we present early-stage experiments in synthesizing such payloads and discuss their dual role: not only can they help identify vulnerabilities introduced by previously unseen attack patterns, but they can also serve to augment training datasets for machine learning-based detection systems, where real-world exploit data is often limited.",
    "title_zh": "XSSnake：合成 XSS 攻击载荷生成的实用研究",
    "abstract_zh": "跨站脚本（Cross-Site Scripting）漏洞在现代Web应用中依然普遍存在，尽管输出编码库的广泛使用、浏览器环境的强化以及内容安全策略（CSP）等基于策略的安全机制已得到普遍部署。其核心挑战在于攻击面不断扩展且快速演变。随着HTML解析规则和JavaScript语义的持续演进，新的恶意脚本注入与执行上下文不断涌现。与此同时，攻击者也在不断优化绕过过滤机制的技术，进一步增加了防御难度。当前的安全测试流程通常依赖于长期积累的已知攻击字符串静态payload集合。虽然这些数据集至关重要，但往往难以跟上新兴攻击策略的发展，可能遗漏新型或隐蔽的注入路径。本文指出了由此带来的评估覆盖缺口，并探讨了自动生成payload作为前瞻性研究方向的潜力。具体而言，我们展示了合成此类payload的初步实验成果，并讨论了其双重作用：不仅有助于发现由此前未见攻击模式引入的安全漏洞，还可用于扩充机器学习驱动检测系统的训练数据集——而这类系统在实际应用中常面临真实exploit数据不足的问题。"
  },
  {
    "date": "2025-11-20",
    "title": "Optimization of Smart Contract Reentrancy Vulnerabilities Based on Static Analysis",
    "authors": "Zilga Heritiana Randriamiarison, Hajarisena Razafimahatratra, Nicolas Raft Razafindrakoto, Yassine Rhazali",
    "publish": "2025 29th International Conference on System Theory, Control and Computing (ICSTCC)",
    "url": "https://doi.org/10.1109/icstcc66753.2025.11240264",
    "source": "IEEE",
    "abstract": "A smart contract is a program deployed on a blockchain network and becomes immutable once deployed. Reentrancy bugs are among the most significant vulnerabilities in blockchain technology. Numerous approaches have been proposed to detect and analyze them. However, minimizing reentrancy errors in smart contracts remains a challenge. The aim of this paper is to optimize the handling of reentrancy bugs in smart contracts across different versions of Solidity. It also proposes a new method for syntactic and lexical code detection. Our approach is based on static analysis, and the tool can detect different versions of smart contract code. We used regex to identify external calls in the contract and Control Flow Graph (CFG) to identify the components of the code. A detection algorithm was developed to identify reentrancy vulnerabilities. This approach enhances contract reliability and reduces risk prior to deployment. Our method has been evaluated using True Positive Rate (TPR) and False Positive Rate (FPR) metrics. We tested it on the SmartBugs benchmark suite and contracts from Etherscan. The contracts analyzed include two solidity versions: 0.4.x and 0.8.x. Thanks to the integration of multiple tools, our solution supports testing across different solidity versions.",
    "title_zh": "基于静态分析的智能合约重入漏洞优化",
    "abstract_zh": "智能合约是部署在区块链网络上的一种程序，一旦部署便不可更改。重入漏洞（Reentrancy bugs）是区块链技术中最严重的安全漏洞之一。尽管已有大量方法被提出用于检测和分析此类漏洞，但如何最大限度地减少智能合约中的重入错误仍然是一个挑战。本文旨在优化不同版本Solidity中智能合约的重入漏洞处理，并提出一种新的语法与词法代码检测方法。我们的方法基于静态分析，能够检测多种版本的智能合约代码。通过正则表达式（regex）识别合约中的外部调用，利用控制流图（CFG）分析代码结构组件，并设计了一种检测算法以识别重入漏洞。该方法显著提升了合约的可靠性，降低了部署前的风险。我们采用真阳性率（TPR）和假阳性率（FPR）作为评估指标，对所提方法进行了测试，实验数据来自SmartBugs基准套件以及Etherscan上的真实合约。分析涵盖两个Solidity版本：0.4.x 和 0.8.x。得益于多种工具的集成，本方案支持跨不同Solidity版本的全面测试。"
  },
  {
    "date": "2025-11-20",
    "title": "Kokkidio: Fast, expressive, portable code, based on Kokkos and Eigen",
    "authors": "Lennart Steffen, Reinhard Hinkelmann",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3635265",
    "source": "IEEE",
    "abstract": "<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkidio</i> is a newly developed C++ template library that bridges the performance portability framework <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkos</i> and its strength in utilising GPUs with the expressive syntax and CPU optimisations of the linear algebra library <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Eigen</i>. The unified abstractions provided by <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkidio</i> enable both simple data management as well as clear, succinct compute code in kernel functors, where a novel iteration/functor parameter performs target-specific grouping of operations. When compiled for CPUs, this approach distributes contiguous data segments to OpenMP threads, while preserving <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Eigen</i>’s loop abstractions to enable explicit vectorisation, thus utilising both thread- and data-level parallelism. Compiling the same source code for accelerators, such as GPUs, leverages <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkos</i>’ parallel dispatch instead, achieving compatibility with all prevalent GPU backends (CUDA, ROCm, SYCL, ...). A comprehensive evaluation across GPUs and CPUs by all major vendors shows <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkidio</i> providing significantly improved performance portability over <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkos</i>:With the fraction of best observed runtime as the efficiency metric, <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkidio</i> achieves a near-optimal harmonic mean of 95% efficiency across all microbenchmarks and tested hardware, offering a marked improvement over to <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkos</i>’ 59%, thus effectively merging <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Kokkos</i>’ GPU and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Eigen</i>’s CPU performance, powerful math functions, and legibility into one framework.",
    "title_zh": "Kokkidio：基于Kokkos和Eigen的快速、高效且可移植的代码",
    "abstract_zh": "Kokkidio 是一个新开发的 C++ 模板库，它将性能可移植性框架 Kokkos 与 GPU 利用能力，与线性代数库 Eigen 的表达性语法及 CPU 优化特性相结合。Kokkidio 提供的统一抽象机制，既简化了数据管理，又在内核函数中实现了清晰简洁的计算代码；其中引入了一种新颖的迭代/函数参数机制，能够针对不同目标平台对操作进行分组。当编译为 CPU 使用时，该方法将连续的数据段分发给 OpenMP 线程，同时保留 Eigen 的循环抽象以支持显式向量化，从而充分利用线程级和数据级并行性。当同一源代码被编译用于加速器（如 GPU）时，则利用 Kokkos 的并行调度机制，实现与所有主流 GPU 后端（CUDA、ROCm、SYCL 等）的兼容性。通过对各大厂商的 GPU 和 CPU 平台进行全面评估，结果显示：相较于 Kokkos，Kokkidio 在性能可移植性方面有显著提升。以最佳观测运行时间占比作为效率指标，Kokkidio 在所有微基准测试和所测硬件上实现了接近最优的调和平均效率 95%，远超 Kokkos 的 59%。这有效融合了 Kokkos 的 GPU 性能优势、Eigen 的 CPU 性能、强大的数学函数以及出色的代码可读性，形成一个统一高效的编程框架。"
  },
  {
    "date": "2025-11-20",
    "title": "Software-Style Hardware Debugging: A Hardware Generation, Simulation and Debugging Framework",
    "authors": "Weiran Liu, Shixuan Chen, Chun Yang, Xianhua Liu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240882",
    "source": "IEEE",
    "abstract": "Hardware generation frameworks (HGFs) leverage high-level descriptions to introduce agile methods into hardware design. However, HGFs still rely on traditional RTL verification workflows, limiting design-verification iteration efficiency. This paper presents ComoPy, an HGF supporting host language native simulation and debugging, introducing software debugging techniques to hardware verification. The framework enables fine-grained tracing and breakpoints directly on HDL source lines, and implements software-style hot-reloading for incremental debug-fix iterations. Furthermore, it supports switching between fast low-level RTL simulation and high-level HDL execution. Case studies on the Sodor-clone processor and a database accelerator featuring systolic array show timely debugging responses, with only a 6% performance loss compared to Verilator simulation.",
    "title_zh": "软件风格的硬件调试：一种硬件生成、仿真与调试框架",
    "abstract_zh": "硬件生成框架（HGFs）利用高层次描述将敏捷方法引入硬件设计。然而，HGFs 仍依赖传统的 RTL 验证流程，限制了设计-验证迭代的效率。本文提出 ComoPy，一种支持主机语言原生仿真与调试的 HGF，将软件调试技术引入硬件验证。该框架可在 HDL 源代码行上实现细粒度的跟踪和断点设置，并实现了类似软件的热重载功能，以支持增量式调试与修复迭代。此外，它还支持在快速低层级 RTL 仿真与高层级 HDL 执行之间灵活切换。针对 Sodor 克隆处理器以及采用阵列结构的数据库加速器的案例研究显示，ComoPy 能够提供及时的调试响应，其性能损耗仅比 Verilator 仿真高出 6%。"
  },
  {
    "date": "2025-11-20",
    "title": "Achieving Simultaneous Buffering and Steiner Tree Synthesis via Harmonic Based Reinforcement Learning",
    "authors": "Lin Chen, Yuxuan Li, Hu Ding",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240816",
    "source": "IEEE",
    "abstract": "Buffer insertion is a critical technique for delay reduction, and in particular the construction of Steiner trees with buffers has garnered great attention in recent years. However, constructing Steiner tree and buffer insertion are often addressed as separate tasks, due to the large computational complexity for integrating them concurrently. Considering the interdependence between tree topology and buffer placement, it should gain significant benefit for circuit design if we can combine them jointly in the optimization. In this paper, we introduce a novel reinforcement learning-based approach to simultaneously achieve the construction of Steiner trees and buffer insertion, where the key idea is employing an appropriately designed harmonic function in the optimization. We also conduct a set of experiments to validate the efficiency and effectiveness of the proposed method in depth.",
    "title_zh": "基于谐波强化学习的同步缓冲与Steiner树综合实现",
    "abstract_zh": "缓冲插入是降低延迟的关键技术，近年来，带有缓冲器的Steiner树构建尤其受到广泛关注。然而，由于将两者同时整合存在巨大的计算复杂性，因此Steiner树构造与缓冲插入通常被当作独立任务来处理。考虑到树拓扑结构与缓冲器布局之间的相互依赖关系，若能在优化过程中联合考虑二者，将对电路设计带来显著效益。本文提出一种基于强化学习的新方法，能够同时实现Steiner树的构建与缓冲插入，其核心思想是在优化过程中引入一个合理设计的调和函数。此外，我们还进行了一系列实验，深入验证了所提方法在效率与有效性方面的优越性。"
  },
  {
    "date": "2025-11-20",
    "title": "Automatic Microarchitecture-Aware Custom Instruction Design for RISC-V Processors",
    "authors": "Evgenii Rezunov, Niko Zurstraßen, Lennart M. Reimann, Rainer Leupers",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240781",
    "source": "IEEE",
    "abstract": "An Application-Specific Instruction Set Processor (ASIP) is a specialized microprocessor that provides a trade-off between the programmability of a General Purpose Processor (GPP) and the performance and energy-efficiency of dedicated hardware accelerators. ASIPs are often derived from off-the-shelf GPPs extended by custom instructions tailored towards a specific software workload. One of the most important challenges of designing an ASIP is to find said custom instructions that help to increase performance without being too costly in terms of area and power consumption. To date, solving this challenge is relatively labor-intensive and typically performed manually.Addressing the lack of automation, we present Custom Instruction Designer for RISC-V Extensions (CIDRE), a front-to-back tool for ASIP design. CIDRE automatically analyzes hot spots in RISC-V applications and generates custom instruction suggestions with a corresponding nML description. The nML description can be used with other electronic design automation tools to accurately assess the cost and benefits of the found suggestions. In a RISC-V benchmark study, we were able to accelerate embedded benchmarks from Embench and MiBench by up to 2.47x with less than 24% area increase. The entire process was conducted completely automatically.",
    "title_zh": "面向RISC-V处理器的自动微架构感知定制指令设计",
    "abstract_zh": "应用特定指令集处理器（ASIP）是一种专用微处理器，它在通用处理器（GPP）的可编程性与专用硬件加速器的性能和能效之间取得了平衡。ASIP通常基于现成的通用处理器，并通过添加针对特定软件工作负载量身定制的自定义指令进行扩展。设计ASIP时面临的最重要挑战之一，是找到能够提升性能但又不会在面积和功耗方面造成过大开销的自定义指令。迄今为止，解决这一挑战仍相对费时，且通常依赖人工完成。\n\n为应对自动化不足的问题，我们提出了RISC-V扩展的自定义指令设计工具——CIDRE（Custom Instruction Designer for RISC-V Extensions）。CIDRE是一款从头到尾的ASIP设计工具，能够自动分析RISC-V应用程序中的热点代码，并生成相应的自定义指令建议，同时提供对应的nML描述。该nML描述可与其它电子设计自动化（EDA）工具结合使用，以精确评估所提建议带来的成本与收益。\n\n在一项针对RISC-V的基准测试研究中，我们成功将Embench和MiBench嵌入式基准程序的执行速度提升了最高达2.47倍，而面积增加不超过24%。整个流程完全自动化完成。"
  },
  {
    "date": "2025-11-20",
    "title": "PCBFormer: Understanding 3D Structure of RealWorld PCB Traces for S-Parameter Prediction",
    "authors": "Taejin Paik, Jaemin Park, Taehee Kim, Daniel Hyunsuk Jung, Doyun Kim",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240981",
    "source": "IEEE",
    "abstract": "As signal frequency increases, signal integrity, a measure of how well a signal is transferred from one component to another, becomes critical in modern electronic products. Thorough signal integrity analysis is essential in design process and S-parameters are often used to model electromagnetic characteristics of a channel such as PCB traces. However, numerous iterations are inevitable due to design changes in components of a product such as different placement on PCBs. Traditional approaches to S-parameter extraction rely on computationally expensive electromagnetic simulations, becoming a bottleneck in the design process. To tackle this issue, we present PCBFormer, a novel deep learning framework that predicts S-parameters of PCBs with high accuracy and efficiency. Our framework effectively captures multiple traces’ electromagnetic interactions across multiple layers in 3D PCB structure, taking each layer’s properties into account. For realistic PCB examples with 10 traces and 25 layers, PCBFormer achieves 0.86 R2 score across 210 S-parameters and DC to 1GHz frequency range.",
    "title_zh": "PCBFormer：理解真实世界PCB走线的三维结构以预测S参数",
    "abstract_zh": "随着信号频率的升高，信号完整性——即信号从一个元器件到另一个元器件传输质量的衡量标准——在现代电子产品中变得至关重要。在设计过程中，全面的信号完整性分析是必不可少的，而S参数常被用来建模如PCB走线等信道的电磁特性。然而，由于产品中组件的设计变更（如PCB上的不同布局），不可避免地需要多次迭代。传统的S参数提取方法依赖于计算成本高昂的电磁仿真，已成为设计流程中的瓶颈。为解决这一问题，我们提出了PCBFormer，一种新颖的深度学习框架，能够以高精度和高效率预测PCB的S参数。该框架有效捕捉了三维PCB结构中多层之间多个走线之间的电磁相互作用，并充分考虑了每一层的特性。对于包含10条走线和25层的真实PCB实例，PCBFormer在DC至1GHz频率范围内对210个S参数的预测达到了0.86的R²得分。"
  },
  {
    "date": "2025-11-20",
    "title": "ASTRA: Automatic Sizing of Transistors with Reasoning Agents",
    "authors": "Wei W. Xing, Baowen Ou, Yuxuan Zhang, Zhuohua Liu, Yuanqi Hu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240675",
    "source": "IEEE",
    "abstract": "Advancing technology nodes have significantly increased the complexity of transistor sizing in analog circuit design. Although artificial intelligence (AI) techniques show potential, their lack of integrated domain expertise often leads to slow convergence in practical applications. We propose ASTRA (Automatic Sizing of Transistors with Reasoning Agents), a novel optimization framework that implements the Model Context Protocol (MCP) to create structured reasoning pathways between Large Language Models (LLMs), domain knowledge bases, and Bayesian Optimization (BO). ASTRA introduces a two-stage process: first, MCP-guided design initialization that leverages Retrieval-Augmented Generation (RAG) to quickly identify feasible regions using gm/ID methodology; and second, BO-based optimization focused on critical transistors, identified through LLM reasoning with data-driven validation. A key innovation of ASTRA is its ability to seamlessly integrate with and enhance virtually any existing transistor sizing algorithm at minimal additional cost. Unlike purely data-driven or black-box LLM approaches, ASTRA maintains traceable decision processes that can be verified and refined. Evaluated on three real-world analog circuits, ASTRA enhances multiple classical optimization methods, achieving up to 4.35× fewer simulation iterations and 2.36× performance improvements, demonstrating its effectiveness as a general open-source framework for advancing analog circuit sizing. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "title_zh": "ASTRA：基于推理代理的晶体管自动尺寸设计",
    "abstract_zh": "随着技术节点的不断演进，模拟电路设计中的晶体管尺寸优化复杂度显著提升。尽管人工智能（AI）技术展现出巨大潜力，但其普遍缺乏与领域知识的深度融合，常导致实际应用中收敛速度缓慢。本文提出一种名为 ASTRA（基于推理代理的晶体管自动尺寸优化）的新颖优化框架，通过引入模型上下文协议（Model Context Protocol, MCP），构建了大型语言模型（LLMs）、领域知识库与贝叶斯优化（BO）之间的结构化推理路径。ASTRA 采用两阶段流程：第一阶段为 MCP 引导的设计初始化，利用检索增强生成（RAG）技术，结合 gm/ID 方法快速定位可行解区域；第二阶段为基于 BO 的优化过程，聚焦于由 LLM 推理并经数据驱动验证识别出的关键晶体管。ASTRA 的一项关键创新在于，能够以极低的额外成本无缝集成并增强几乎任何现有的晶体管尺寸优化算法。与纯数据驱动或黑箱式 LLM 方法不同，ASTRA 保持了可追溯的决策过程，支持验证与迭代优化。在三个真实世界模拟电路上的评估表明，ASTRA 能有效提升多种经典优化方法的性能，最多减少 4.35 倍的仿真迭代次数，并实现 2.36 倍的性能提升，充分证明其作为通用开源框架在推动模拟电路尺寸优化方面的有效性。<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"
  },
  {
    "date": "2025-11-20",
    "title": "Contents",
    "authors": "N/A",
    "publish": "2025 Systems and Technologies of the Digital HealthCare (STDH)",
    "url": "https://doi.org/10.1109/stdh66836.2025.11227514",
    "source": "IEEE",
    "abstract": null,
    "title_zh": "内容",
    "abstract_zh": "None"
  },
  {
    "date": "2025-11-20",
    "title": "Analog Design Experiments With AI—Part 1 [The Analog Mind]",
    "authors": "Behzad Razavi",
    "publish": "IEEE Solid-State Circuits Magazine",
    "url": "https://doi.org/10.1109/mssc.2025.3611213",
    "source": "IEEE",
    "abstract": "This article aims to evaluate the present abilities of ChatGPT for the analysis of analog circuits. We pose 30 questions on one- and two-transistor circuits to ChatGPT and examine its responses. The overall result indicates that ChatGPT answers about 60% of the questions incorrectly.",
    "title_zh": "模拟设计实验与人工智能——第一部分 [模拟思维]",
    "abstract_zh": "本文旨在评估ChatGPT在模拟电路分析方面的现有能力。我们向ChatGPT提出了30个关于单晶体管和双晶体管电路的问题，并对其回答进行了分析。总体结果显示，ChatGPT有约60%的问题回答错误。"
  },
  {
    "date": "2025-11-20",
    "title": "TriGen: A Semantic-Feedback Collaborative LLM Test Case Generation Model",
    "authors": "Peng Han, Yuan Chen, Junjie Wang",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3635218",
    "source": "IEEE",
    "abstract": "Regarding the problems of semantic understanding bias and uncontrollable generation in the generation of test cases driven by natural language requirements, this paper proposes TriGen - a controllable and traceable test case generation model for Chinese requirements. To break through the limitations of end-to-end black-box generation, TriGen is based on DeepSeek-7B and adopts a modular decoupling architecture, dividing the generation process into five stages: semantic enhancement, test type mapping, test procedure generation, step-by-step feedback optimization, and structured output. It realizes the explicit modeling of the generation logic and process tracing. Through efficient parameter fine-tuning of DeepSeek-7B using LoRA, the aim is to adapt to the multi-stage generation mechanism of TriGen, clarify the semantic goals and output formats of each stage, and achieve model specialization adaptation in low-resource environments. Further, a multi-dimensional closed-loop feedback mechanism is introduced to support local correction and quality iteration. To support research in Chinese scenarios, a high-quality Chinese software requirement dataset containing 2120 labeled samples is constructed, covering scenarios such as functional descriptions and interaction logic. Experimental results show that TriGen significantly outperforms the baseline method in both automatic evaluation and human evaluation. The semantic-level precision, recall, and F1 score reach 0.825, 0.819, and 0.801 respectively, with an hallucination rate controlled at 0.183, and the human rating is 0.94. It provides an interpretable, iterative, and deployable technical path for the automatic generation of test cases in Chinese requirements.",
    "title_zh": "TriGen：一种语义反馈协同的大型语言模型测试用例生成模型",
    "abstract_zh": "针对自然语言需求驱动的测试用例生成中存在的语义理解偏差与生成不可控等问题，本文提出了一种面向中文需求的可控且可追溯的测试用例生成模型——TriGen。为突破端到端黑箱生成模式的局限，TriGen基于DeepSeek-7B模型，采用模块化解耦架构，将生成过程划分为五个阶段：语义增强、测试类型映射、测试步骤生成、逐步反馈优化以及结构化输出，实现了生成逻辑的显式建模与过程可追溯。通过使用LoRA高效地对DeepSeek-7B进行参数微调，旨在适配TriGen的多阶段生成机制，明确各阶段的语义目标与输出格式，实现在低资源环境下的模型专业化适配。此外，引入多维度闭环反馈机制，支持局部修正与质量迭代优化。为支撑中文场景的研究，构建了一个高质量的中文软件需求数据集，包含2120个标注样本，覆盖功能描述、交互逻辑等多种典型场景。实验结果表明，TriGen在自动评估与人工评估中均显著优于基线方法：语义层面的精确率、召回率和F1分数分别达到0.825、0.819和0.801，幻觉率控制在0.183，人工评分高达0.94。该研究为中文需求场景下测试用例的自动化生成提供了一条可解释、可迭代、可部署的技术路径。"
  },
  {
    "date": "2025-11-20",
    "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs",
    "authors": "Chenchen Zhao, Zhengyuan Shi, Xiangyu Wen, Chengjie Liu, Yi Liu, Yunhao Zhou, Yuxiang Zhao, Hefei Feng, Yinan Zhu, Gwok-Waa Wan, Xin Cheng, Weiyu Chen, Yongqi Fu, Chujie Chen, Chenhao Xue, Ying Wang, Yibo Lin, Jun Yang, Ning Xu, Xi Wang, Qiang Xu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240653",
    "source": "IEEE",
    "abstract": "The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages—ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at https://github.com/cure-lab/MMCircuitEval.",
    "title_zh": "MMCircuitEval：一个全面的多模态电路导向基准，用于评估大语言模型",
    "abstract_zh": "多模态大语言模型（MLLMs）的出现为电子设计自动化（EDA）领域的自动化与性能提升带来了广阔前景。然而，由于现有基准测试范围有限，全面评估这些模型在电路设计中的表现仍面临挑战。为填补这一空白，我们提出了MMCircuitEval——首个专门设计用于全面评估MLLM在多样化EDA任务中表现的多模态基准。MMCircuitEval包含3614个精心筛选的问题-答案（QA）对，覆盖数字与模拟电路，并贯穿EDA关键阶段，从基础知识和规格说明到前端与后端设计。所有QA对均源自教材、技术题库、数据手册及真实世界的技术文档，并经过专家严格审核以确保准确性与相关性。本基准的独特之处在于，它按设计阶段、电路类型、考察能力（知识、理解、推理、计算）以及难度等级对问题进行分类，从而支持对模型能力与局限性的深入分析。大量实证评估揭示了现有LLMs在后端设计和复杂计算任务中存在显著性能差距，凸显了构建针对性训练数据集与建模方法的迫切需求。MMCircuitEval为推动MLLM在EDA领域的发展提供了基础资源，有助于其在实际电路设计流程中的集成应用。本基准现已开放获取，地址为：https://github.com/cure-lab/MMCircuitEval。"
  },
  {
    "date": "2025-11-20",
    "title": "FabThink: A Wafer Analysis Multimodal LLM via Chain-of-Thought-Driven Retrieval Augmentation",
    "authors": "Yuqi Jiang, Qian Jin, Xudong Lu, Jinyuan Deng, Hao Geng, Hanming Wu, Qi Sun, Cheng Zhuo",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240663",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) incorporates external knowledge to support Large Language Models (LLMs) in generating more accurate, fact-based answers. However, standard RAG methods applied in LLMs lack adaptation to specific domains, limiting their effectiveness in handling complex and specialized knowledge in wafer manufacturing, such as defect root cause analysis, which leads to lower retrieval accuracy and increased large model hallucinations. We propose a wafer-domain-tailored Multimodal Large Language Model (MLLM), FabThink, which aims to optimize the RAG process through a unique multimodal Chain-of-Thought (CoT) framework to address the above issues. Specifically, we propose a \"logical decomposition, cross-modal integration, multi-turn retrieval\" strategy to refine the process of solving complex queries and enhance the precision of document retrieval. In addition, we introduce an adaptive weighted ranking for critical document selection and fine-tune a text generator to answer wafer-related questions. Experimental results on fab data show that FabThink excels in detection, retrieval, and generation tasks, strongly supporting defect analysis in the integrated circuits (IC) domain.",
    "title_zh": "FabThink：一种通过思维链驱动的检索增强实现晶圆分析的多模态大模型",
    "abstract_zh": "检索增强生成（Retrieval-Augmented Generation, RAG）通过引入外部知识，帮助大型语言模型（LLM）生成更准确、基于事实的答案。然而，现有的标准RAG方法在应用于LLM时缺乏对特定领域的适应性，难以有效处理晶圆制造等复杂专业领域中的知识，例如缺陷根因分析，导致检索准确率较低，并增加了大模型产生幻觉的风险。为此，我们提出了一种面向晶圆制造领域的多模态大语言模型（Multimodal Large Language Model, MLLM）——FabThink，旨在通过独特的多模态思维链（Chain-of-Thought, CoT）框架优化RAG流程，以解决上述问题。具体而言，我们设计了“逻辑分解、跨模态融合、多轮检索”策略，以精细化复杂查询的求解过程，提升文档检索的精准度。此外，我们引入自适应加权排序机制用于关键文档的选择，并对文本生成器进行微调，使其能够更准确地回答晶圆相关问题。在晶圆厂数据上的实验结果表明，FabThink在检测、检索和生成任务中均表现优异，显著支持集成电路（IC）领域的缺陷分析。"
  },
  {
    "date": "2025-11-20",
    "title": "SO3-Cell: Standard Cell Layout Automation Framework for Simultaneous Optimization of Topology, Placement, and Routing",
    "authors": "Chung-Kuan Cheng, Andrew B. Kahng, Byeonggon Kang, Seokhyeong Kang, Jakang Lee, Bill Lin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240677",
    "source": "IEEE",
    "abstract": "We propose SO3-Cell, the first automatic standard cell layout generation framework that optimizes three key steps simultaneously using Mixed-Integer Linear Programming (MILP). SO3-Cell simultaneously performs circuit topology optimization, transistor placement, and internal cell routing to achieve an optimized layout solution. Our optimization objective is to minimize metal usage while enhancing cell layout flexibility within a given area.We introduce design space pruning techniques to mitigate the complexity of larger designs, such as a full adder, a reset flip-flop (FF), and a 2-bit FF. We successfully generate a layout for a 44-transistor 2-bit FF within 25,862 seconds, demonstrating the scalability and robustness of the SO3-Cell framework. We evaluate the block-level PPA impact of the proposed cell-layout improvements, demonstrating a 35.0% reduction in power, a 2.2% increase in frequency, and a 31.1% reduction in area.",
    "title_zh": "SO3-Cell：用于拓扑、布局与布线协同优化的标准单元版图自动化框架",
    "abstract_zh": "我们提出了SO3-Cell，这是首个采用混合整数线性规划（MILP）同时优化三个关键步骤的自动标准单元版图生成框架。SO3-Cell能够同步完成电路拓扑优化、晶体管布局以及单元内部布线，从而实现最优的版图解决方案。我们的优化目标是在给定面积内最小化金属使用量，同时提升单元版图的灵活性。为缓解大型设计（如全加器、复位触发器（FF）和2位触发器）带来的复杂性，我们引入了设计空间剪枝技术。我们成功在25,862秒内生成了一个44个晶体管的2位触发器版图，充分展示了SO3-Cell框架的可扩展性和鲁棒性。我们评估了所提出的单元版图改进在模块级的PPA（功耗、性能、面积）影响，结果表明：功耗降低35.0%，频率提升2.2%，面积减少31.1%。"
  },
  {
    "date": "2025-11-20",
    "title": "IncreGPUSTA: GPU-Accelerated Incremental Static Timing Analysis for Iterative Design Flows",
    "authors": "Haichuan Liu, Zizheng Guo, Runsheng Wang, Yibo Lin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240636",
    "source": "IEEE",
    "abstract": "Static timing analysis (STA) plays an essential role in VLSI design optimization. While CPU-based incremental STA methods reduce computational overhead by selectively updating affected circuit regions, and GPU-accelerated engines improve full-circuit analysis throughput, effectively combining these approaches has remained challenging. Existing solutions offer only partial incrementality, either switching to CPU processing for small modifications or handling solely delay value changes without supporting structural updates. We introduce IncreGPUSTA, a novel GPU-accelerated incremental STA algorithm with dual-CSR data structures and incremental levelization that efficiently processes timing updates for both localized and structural modifications. Experimental results on industrial benchmarks demonstrate speedups of up to 3.06× over GPU full Timer and up to 72.50× over CPU incremental Timer for million-scale designs.",
    "title_zh": "IncreGPUSTA：面向迭代设计流程的GPU加速增量式静态时序分析",
    "abstract_zh": "静态时序分析（STA）在VLSI设计优化中起着至关重要的作用。尽管基于CPU的增量式STA方法通过选择性地更新受影响的电路区域来降低计算开销，而基于GPU的加速引擎则提升了全电路分析的吞吐量，但如何有效结合这两种方法仍面临挑战。现有的解决方案仅提供部分增量性：要么在修改较小时切换到CPU处理，要么仅支持延迟值的变化，而不支持结构上的更新。本文提出了一种新型的GPU加速增量式STA算法——IncreGPUSTA，该算法采用双CSR数据结构与增量式层级化方法，能够高效处理局部修改和结构变更带来的时序更新。在工业级基准测试中的实验结果表明，对于百万级规模的设计，IncreGPUSTA相比GPU全电路分析工具最高可实现3.06倍的加速，相比CPU增量式分析工具最高可达72.50倍的加速。"
  },
  {
    "date": "2025-11-20",
    "title": "Revisit Choice Network for Synthesis and Technology Mapping",
    "authors": "Chen Chen, Jiaqi Yin, Cunxi Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240920",
    "source": "IEEE",
    "abstract": "Choice network construction is a critical technique for alleviating structural bias issues in Boolean optimization, equivalence checking, and technology mapping. Previous works on lossless synthesis utilize independent optimization to generate multiple snapshots, and use simulation and SAT solvers to identify functionally equivalent nodes. These nodes are then merged into a subject graph with choice nodes. However, such methods often neglect the quality of these choices—raising the question of whether they truly contribute to effective technology mapping. This paper introduces CRISTAL, a novel methodology and framework to constructing Boolean choice networks. Specifically, CRISTAL introduces a novel flow of choice network-based synthesis and mapping, includes representative logic cone search, structural mutation for generating diverse choice structures via equality saturation, and priority-ranking choice selection along with choice network construction and validation. Through these techniques, CRISTAL constructs fewer but higher-quality choices. Our experimental results demonstrate that CRISTAL outperforms the state-of-the-art Boolean choice network construction implemented in ABC in the post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime reduction on large-scale cases, across a diverse set of combinational circuits from the IWLS 2005, ISCAS’89, and EPFL benchmark suites.",
    "title_zh": "重新审视用于综合与技术映射的选择网络",
    "abstract_zh": "选择网络构建是缓解布尔优化、等价性检查及技术映射中结构偏差问题的关键技术。以往的无损合成方法通常采用独立优化生成多个快照，再通过仿真和SAT求解器识别功能等价的节点，并将这些节点合并到包含选择节点的主图中。然而，这类方法往往忽视了所生成选择的质量——这引发了疑问：这些选择是否真正有助于高效的技术映射？本文提出CRISTAL，一种新颖的方法论与框架，用于构建布尔选择网络。具体而言，CRISTAL引入了一种基于选择网络的合成与映射新流程，包括代表性逻辑锥搜索、通过等式饱和实现多样化选择结构生成的结构变异，以及优先级排序的选择选取机制，结合选择网络的构建与验证。通过这些技术，CRISTAL能够构建数量更少但质量更高的选择。实验结果表明，在ABC中实现的当前最先进布尔选择网络构建方法的基础上，CRISTAL在后映射阶段表现更优：在延迟导向模式下，平均实现3.85%/8.35%（面积/延迟）的降低；在面积导向模式下，分别实现0.11%/2.74%的改善；在大规模电路案例上，运行时间减少63.77%。实验覆盖了来自IWLS 2005、ISCAS’89和EPFL基准套件的多种组合电路。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Unitho: A Unified Multi-Task Framework for Computational Lithography",
    "authors": "Qian Jin, Yumeng Liu, Yuqi Jiang, Qi Sun, Cheng Zhuo",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240871",
    "source": "IEEE",
    "abstract": "Reliable, generalizable data foundations are critical for enabling large-scale models in computational lithography. However, essential tasks—mask generation, rule violation detection, and layout optimization—are often handled in isolation, hindered by scarce datasets and limited modeling approaches. To address these challenges, we introduce Unitho, a unified multi-task large vision model built upon the Transformer architecture. Trained on a large-scale industrial lithography simulation dataset with hundreds of thousands of cases, Unitho supports end-to-end mask generation, lithography simulation, and rule violation detection. By enabling agile and high-fidelity lithography simulation, Unitho further facilitates the construction of robust data foundations for intelligent EDA. Experimental results validate its effectiveness and generalizability, with performance substantially surpassing academic baselines.",
    "title_zh": "特邀论文：Unitho：一种用于计算光刻的统一多任务框架",
    "abstract_zh": "可靠的、可泛化的数据基础对于推动计算光刻中大规模模型的发展至关重要。然而，关键任务如掩模生成、规则违例检测和版图优化通常被孤立处理，受限于数据集稀缺和建模方法有限。为解决这些挑战，我们提出了Unitho——一种基于Transformer架构的统一多任务大视觉模型。该模型在包含数十万案例的大规模工业光刻仿真数据集上进行训练，能够实现端到端的掩模生成、光刻仿真及规则违例检测。通过支持敏捷且高保真的光刻仿真，Unitho进一步助力构建智能EDA所需的稳健数据基础。实验结果验证了其有效性和泛化能力，性能显著优于现有学术基准。"
  },
  {
    "date": "2025-11-20",
    "title": "ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings",
    "authors": "Xiaomeng Yang, Jian Gao, Yanzhi Wang, Xuan Zhang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240890",
    "source": "IEEE",
    "abstract": "Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13×) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.",
    "title_zh": "ZeroSim：基于统一Transformer嵌入的零样本模拟电路评估",
    "abstract_zh": "尽管基于学习的模拟电路设计自动化近年来在拓扑生成、器件尺寸优化和版图综合等任务上取得了进展，但高效性能评估仍是主要瓶颈。传统的SPICE仿真耗时较长，而现有的机器学习方法通常需要针对特定拓扑重新训练，或依赖人工进行子结构分割以进行微调，限制了其可扩展性和适应性。本文提出ZeroSim——一种基于Transformer的性能建模框架，旨在实现对已训练拓扑在新型参数配置下的鲁棒分布内泛化，以及对未见过的拓扑实现无需任何微调的零样本泛化。我们采用了三项关键技术策略：（1）构建包含360万实例的多样化训练数据集，覆盖超过60种放大器拓扑；（2）采用统一的拓扑嵌入机制，结合全局感知标记与分层注意力，以增强对新电路的泛化能力；（3）引入拓扑条件化的参数映射方法，确保结构表示在参数变化下保持一致性。实验结果表明，ZeroSim显著优于多层感知机、图神经网络及普通Transformer等基线模型，在不同放大器拓扑上均能实现高精度的零样本预测。此外，当集成至基于强化学习的参数优化流程中时，ZeroSim相较传统SPICE仿真实现了惊人的13倍加速，充分彰显其在各类模拟电路设计自动化任务中的实际应用价值。"
  },
  {
    "date": "2025-11-20",
    "title": "LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism",
    "authors": "Yimin Wang, Yue Jiet Chong, Xuanyao Fong",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240722",
    "source": "IEEE",
    "abstract": "Large language model (LLM) inference has been a prevalent demand in daily life and industries. The large tensor sizes and computing complexities in LLMs have brought challenges to memory, computing, and databus. This paper proposes a computation/memory/communication co-designed non-von Neumann accelerator by aggregating processing-in-memory (PIM) and computational network-on-chip (NoC), termed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC based on the data dynamicity to maximize data locality. Model partition and mapping are optimized by heuristic design space exploration. Dedicated fine-grained parallelism and tiling techniques enable high-throughput dataflow across the distributed resources in PIM and NoC. The architecture is evaluated on Llama 1B/8B/13B models and shows ~2.55× throughput (tokens/sec) improvement and ~71.94× energy efficiency (tokens/Joule) boost compared to the A100 GPU.",
    "title_zh": "LEAP：基于平衡数据流与细粒度并行的可扩展PIM-NoC架构上的大模型推理",
    "abstract_zh": "大语言模型（LLM）推理在日常生活和工业应用中已成为普遍需求。由于LLM具有庞大的张量规模和复杂的计算特性，给内存、计算能力和数据总线带来了巨大挑战。本文提出了一种计算/内存/通信协同设计的非冯·诺依曼加速器——LEAP，该架构通过集成存内计算（PIM）与计算型网络-on-chip（NoC）来实现高效处理。针对LLM中的矩阵乘法操作，根据数据动态性将其分配至PIM或NoC，以最大化数据局部性。通过启发式的设计空间探索优化模型划分与映射策略。专用的细粒度并行化与分块技术，实现了PIM与NoC中分布式资源间高吞吐的数据流传输。在Llama 1B/8B/13B模型上的评估结果表明，与A100 GPU相比，LEAP实现了约2.55倍的吞吐量提升（tokens/sec），以及高达71.94倍的能量效率增益（tokens/Joule）。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: 2025 ICCAD CAD Contest Problem C: Incremental Placement Optimization Beyond Detailed Placement: Simultaneous Gate Sizing, Buffering, and Cell Relocation",
    "authors": "Yi-Chen Lu, Rongjian Liang, Wen-Hao Liu, Haoxing Ren",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240847",
    "source": "IEEE",
    "abstract": "Late-stage placement optimization is where real PPA trade-offs surface, and where conventional heuristic passes tend to get trapped in small, local neighborhoods. We frame an invited \"Problem C\" contest that treats this stage as a global, multi-operator search over gate sizing, buffer/inverter-pair insertion, and legal cell relocation, with strict reproducibility and legality. Our core belief grounded in production experience is that GPU batching and differentiable guidance expand the tractable search space: you can score and steer thousands of coordinated moves per iteration, not just a handful, and do so under tight runtime budgets. Submissions must produce a replayable ECO changelist and a final legal DEF; a standardized evaluation flow computes timing, power, and wirelength and combines them with displacement and runtime into the contest score. The specification is designed to encourage pragmatic use of gradient signals and tensorized batching without mandating any single method, enabling participants to leverage novel GPU tools to deliver industrially deployable PPA gains.",
    "title_zh": "特邀论文：2025 ICCAD CAD竞赛问题C：超越详细布局的增量式布局优化：门尺寸调整、缓冲插入与单元重定位的协同优化",
    "abstract_zh": "后期布局优化是实际PPA权衡显现的阶段，也是传统启发式方法容易陷入局部小范围搜索的瓶颈所在。我们为此特别设立了一项受邀的“问题C”竞赛，将该阶段视为对门尺寸调整、缓冲器/反相器对插入以及合法单元重定位的全局性、多算子搜索问题，并严格要求结果的可重现性和合法性。基于生产实践的经验，我们的核心信念是：GPU批处理与可微引导能够显著扩展可探索的搜索空间——每轮迭代中，你可以评估并引导数千个协同操作，而不仅仅是少数几个，并且在严格的运行时间预算下完成。参赛作品必须生成可回放的ECO变更列表和最终合法的DEF文件；标准化的评估流程将计算时序、功耗和布线长度，并将其与位移量和运行时间综合，形成竞赛评分。该规范旨在鼓励参赛者务实利用梯度信号和张量化批处理，但不强制采用任何特定方法，从而让参与者能够充分发挥新型GPU工具的优势，实现工业级可部署的PPA提升。"
  },
  {
    "date": "2025-11-20",
    "title": "Making the Best Switch: Encoding Strategy Management for Efficient TFHE Circuit Evaluation",
    "authors": "Mingfei Yu, Gabrielle De Micheli, Giovanni De Micheli",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240699",
    "source": "IEEE",
    "abstract": "This work addresses the synthesis of efficient torus fully homomorphic encryption (TFHE) circuits for private Boolean function evaluation through encoding strategy management. Modern TFHE implementations support multiple plaintext encoding spaces, each offering distinct trade-offs between computational cost and the expressiveness enabled by larger plaintext domains. Smartly switching between encoding strategies to maximize evaluation efficiency remains challenging due to the lack of algorithmic support for determining when and where such transitions should occur. To address this, we propose a synthesis framework that enables encoding-switch-aware TFHE circuit generation. Our approach leverages the structural properties of the exclusive-or sum of products (ESOP) representation to partition Boolean functions into encoding-aligned regions, enabling cost-effective evaluation while minimizing switch overhead. Experimental results demonstrate that our encoding-aware synthesis technique significantly accelerates homomorphic Boolean function evaluation – achieving up to 53.46% and 23.34% average evaluation time reduction on general-purpose Boolean benchmarks – compared to advanced synthesis baselines lacking explicit encoding-switch management. This work lays the groundwork for systematic encoding strategy management in TFHE circuits and highlights the role of logic-level design automation in advancing efficient homomorphic evaluation.",
    "title_zh": "最佳切换：高效TFHE电路评估中的编码策略管理",
    "abstract_zh": "本文针对私有布尔函数评估中的高效环面全同态加密（TFHE）电路合成问题，提出通过编码策略管理来优化实现。现代TFHE实现支持多种明文编码空间，每种编码空间在计算成本与由更大明文域带来的表达能力之间提供不同的权衡。由于缺乏算法层面的支持以确定何时何地进行编码策略切换，因此智能地在不同编码策略间切换以最大化评估效率仍具挑战性。为解决该问题，我们提出一种合成框架，能够生成具备编码切换感知能力的TFHE电路。我们的方法利用异或积之和（ESOP）表示的结构特性，将布尔函数划分为与编码对齐的区域，从而在降低切换开销的同时实现低成本评估。实验结果表明，与缺乏显式编码切换管理的先进合成基线相比，所提出的编码感知合成技术显著加速了同态布尔函数的评估——在通用布尔基准测试中，平均评估时间分别减少了高达53.46%和23.34%。本工作为TFHE电路中系统化的编码策略管理奠定了基础，并凸显了逻辑级设计自动化在推动高效同态计算方面的重要作用。"
  },
  {
    "date": "2025-11-20",
    "title": "MuSTNet: SAT-based Exact Multi-Stage Transistor Network Synthesis with Placement Awareness",
    "authors": "Jiun-Cheng Tsai, Wei-Min Hsu, Kuei-Lin Wu, Hsuan-Ming Huang, Jen-Hang Yang, Heng-Liang Huang, Yen-Ju Su, Charles H.-P. Wen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240651",
    "source": "IEEE",
    "abstract": "Optimizing power, performance, and area in IC designs remains a key focus. However, the limited functionality of standard cell libraries restricts further optimization. A promising solution involves developing customized complex gates that integrate multiple basic gate functions into a single gate at the transistor level. While prior research has extensively explored transistor network optimization of the complex gates, most studies still focus on 1-stage networks, limiting the potential for deeper optimization. Furthermore, existing methodologies often neglect considering transistor placement during network synthesis, potentially leading to suboptimal area even if the transistor count is reduced. To address these limitations, we propose MuSTNet, a SAT-based exact synthesis framework that minimizing transistor networks by incorporating two key innovations: (1) multi-stage hierarchy for deeper optimization, and (2) transistor placement constraints to simultaneously minimize both transistor count and physical area. Experimental results demonstrate that MuSTNet surpasses previous studies, achieving an 18% reduction in transistor count for 4-input P-class functions and a 12.9% reduction for multi-output functions. When applied to an industrial library benchmark, MuSTNet yields a 6.3% area reduction compared to the approach neglecting placement constraints. Moreover, MuSTNet has been applied to complex gate generation, allowing simultaneous functional and topological optimization at the transistor level. Compared to traditional cell-level design, this approach reduces transistor count by 12.3% and area by 21.4%, demonstrating its potential in advanced IC design.",
    "title_zh": "MuSTNet：基于SAT的精确多阶段晶体管网络综合方法（具备布局感知）",
    "abstract_zh": "在集成电路设计中，优化功耗、性能和面积仍然是核心关注点。然而，标准单元库功能有限，制约了进一步的优化空间。一种有前景的解决方案是开发定制化的复杂门电路，将多个基本逻辑门功能集成于单一门电路的晶体管层级。尽管已有研究广泛探索了复杂门电路的晶体管网络优化，但大多数工作仍局限于单级网络结构，限制了更深层次的优化潜力。此外，现有方法在进行网络综合时往往忽略晶体管布局问题，即使减少了晶体管数量，也可能导致物理面积不理想。为解决上述局限性，本文提出 MuSTNet——一种基于SAT的精确综合框架，通过两项关键创新实现晶体管网络的最小化：（1）采用多级层次结构以实现更深层次的优化；（2）引入晶体管布局约束，在减少晶体管数量的同时，同步优化物理面积。实验结果表明，MuSTNet 在 4 输入 P 类函数上实现了 18% 的晶体管数量减少，在多输出函数上实现了 12.9% 的减少。在工业级标准单元库基准测试中，相比忽略布局约束的方法，MuSTNet 实现了 6.3% 的面积缩减。此外，MuSTNet 已成功应用于复杂门电路的生成，实现了晶体管层级上的功能与拓扑结构的协同优化。与传统单元层级设计相比，该方法使晶体管数量减少 12.3%，面积降低 21.4%，充分展现了其在先进集成电路设计中的巨大潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "VeriSAT: the Hardware Design of Modern SAT Solver",
    "authors": "Yue Tao, Shaowei Cai",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240752",
    "source": "IEEE",
    "abstract": "VeriSAT is the first modern SAT solver implemented entirely in synthesizable SystemVerilog, leveraging FPGA architecture for hardware acceleration. This paper introduces the design of VeriSAT, focusing on hardware-specific optimizations that significantly improve performance over traditional software-based solvers. By rethinking key SAT components for hardware, VeriSAT introduces custom data structures and parallelized processes to accelerate solving efficiency.Central to VeriSAT's architecture are hardware-optimized data structures. A linked-list based literal-watching mechanism, enhanced with cached watching literals, reduces latency in unit propagation. Additionally, a concurrent propagation tree enables simultaneous traversal for faster conflict detection. The solver's pipelined clause learning further boosts throughput, allowing rapid conflict analysis without compromising performance.Extensive benchmarking demonstrates that VeriSAT outperforms two other FPGA-based solvers, SAT-Hard and SAT-Accel, by a factor of 1044x and is 18x faster, respectively, on curated benchmark instances. Additionally, VeriSAT shows a 30x speedup over the popular CPU-based MiniSat solver for specific datasets, validating the effectiveness of our design optimizations.VeriSAT represents a significant leap forward in FPGA-based SAT solving, offering unprecedented efficiency through its hardware-tailored design. This work lays the foundation for future advancements in hardware-accelerated SAT solvers and paves the way for more scalable, high-performance solutions in both academic and industrial applications.",
    "title_zh": "VeriSAT：现代SAT求解器的硬件设计",
    "abstract_zh": "VeriSAT 是首个完全采用可综合的 SystemVerilog 实现的现代 SAT 求解器，利用 FPGA 架构实现硬件加速。本文介绍了 VeriSAT 的设计，重点阐述了针对硬件特性的优化策略，这些优化显著提升了其相对于传统软件求解器的性能表现。通过重新设计关键 SAT 组件以适配硬件特性，VeriSAT 引入了定制化的数据结构和并行化处理流程，大幅提高了求解效率。\n\nVeriSAT 架构的核心是经过硬件优化的数据结构。基于链表的字面量监视机制，结合缓存的监视字面量，有效降低了单位传播过程中的延迟。此外，支持并发传播的树形结构能够同时进行多路径遍历，从而加快冲突检测速度。求解器采用流水线化的子句学习机制，进一步提升了吞吐量，在不牺牲性能的前提下实现了快速冲突分析。\n\n大量基准测试表明，在精心挑选的测试实例上，VeriSAT 相较于另外两个基于 FPGA 的求解器 SAT-Hard 和 SAT-Accel，分别实现了 1044 倍和 18 倍的性能提升。同时，对于特定数据集，VeriSAT 的速度比广泛使用的 CPU 版本 MiniSat 快出 30 倍，充分验证了所提出设计优化的有效性。\n\nVeriSAT 标志着基于 FPGA 的 SAT 求解技术迈出了重要一步，凭借其高度定制化的硬件设计，实现了前所未有的高效性。本工作为未来硬件加速 SAT 求解器的发展奠定了基础，也为学术界与工业界提供了更具可扩展性、更高性能的解决方案新路径。"
  },
  {
    "date": "2025-11-20",
    "title": "M3: Mamba-assisted Multi-Circuit Optimization via Model-based RL with Effective Scheduling",
    "authors": "Youngmin Oh, Jinje Park, Taejin Paik, Seunggeun Kim, Suwan Kim, Yoon Hyeok Lee, David Z. Pan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240790",
    "source": "IEEE",
    "abstract": "Recent advances in neural network architectures, such as the Transformer, have enabled a shift from task-specific models to unified foundation models, significantly enhancing generalization and scalability. In contrast, analog circuit design has traditionally relied on bespoke optimization models tailored to individual circuits. To address this gap, we propose M3, a novel unified reinforcement learning (RL) that concurrently optimizes multiple circuits with different topologies to meet target specifications, without requiring task-specific adjustments. The M3 framework employs the Mamba architecture, a recently emerging model regarded as a potential alternative to the Transformer. It combines model-based RL with a dynamic scheduling mechanism that adapts RL parameters to balance exploration (seeking novel designs) and exploitation (refining existing ones). Experimental results demonstrate that M3 successfully trains RL agents capable of simultaneously optimizing multiple circuits, having different topologies, to achieve target performance levels while prior RL-based methods are unable to do. This approach highlights the potential of developing a unified model for optimizing circuits across varying topologies and target specifications<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>.",
    "title_zh": "M3：基于模型的强化学习与有效调度的Mamba辅助多电路优化",
    "abstract_zh": "近年来，神经网络架构（如Transformer）的进展推动了从任务特定模型向统一基础模型的转变，显著提升了模型的泛化能力和可扩展性。相比之下，模拟电路设计传统上依赖于针对每个电路量身定制的优化模型。为弥合这一差距，我们提出了M3——一种新颖的统一强化学习（RL）框架，能够同时优化具有不同拓扑结构的多个电路，在无需进行任务特异性调整的情况下满足目标规格要求。M3框架采用近期兴起的Mamba架构，该架构被视为Transformer的一种潜在替代方案。它结合了基于模型的强化学习与动态调度机制，能够自适应地调整强化学习参数，以平衡探索（寻找新颖设计）与利用（优化现有设计）之间的关系。实验结果表明，M3成功训练出能够同时优化多种拓扑结构电路的强化学习智能体，使其达到目标性能水平，而以往基于强化学习的方法则无法实现这一目标。该方法凸显了构建统一模型以优化不同拓扑结构和目标规格电路的巨大潜力<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>。"
  },
  {
    "date": "2025-11-20",
    "title": "RollPlace: Improving Macro Placement via Monte Carlo Rollout Search",
    "authors": "Qi Zhou, Guojun Liu, Guangzhi Qi, Ming Lu, Jiechu Liu, Zhongli Liu, Jianqun Yang, Xingji Li",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2025.3635566",
    "source": "IEEE",
    "abstract": "The application of Reinforcement Learning (RL) in Electronic Design Automation (EDA), particularly for chip placement, has attracted considerable attention in recent years. While existing machine learning (ML)-based approaches have achieved notable progress, they predominantly focus on generating optimal layouts in a single attempt, often producing solutions that require subsequent refinement. To address this limitation, we propose RollPlace, a novel and generalized macro placement framework. RollPlace adopts a two-stage optimization strategy: generating initial placement solutions via machine learning methods or heuristic-based strategies, and refining these layouts efficiently by adjusting specific macros derived from the initial stage. This strategy circumvents the sequential generation constraints inherent in traditional RL-based placement methods. Furthermore, RollPlace seamlessly integrates Monte Carlo Tree Search (MCTS) to balance exploration and exploitation, and employs a rollout mechanism for efficient local search. Extensive experiments on the ISPD 2005 benchmark demonstrate that RollPlace outperforms state-of-the-art methods. Additionally, end-to-end experimental results based on OpenROAD across 19 benchmarks show that RollPlace excels in multiple metrics. The proposed framework offers a robust and scalable solution for addressing the growing complexity of modern chip design challenges.",
    "title_zh": "RollPlace：通过蒙特卡洛展开搜索优化宏单元布局",
    "abstract_zh": "近年来，强化学习（Reinforcement Learning, RL）在电子设计自动化（Electronic Design Automation, EDA）领域的应用，尤其是在芯片布局方面，受到了广泛关注。尽管现有的基于机器学习（Machine Learning, ML）的方法已取得显著进展，但它们大多侧重于一次性生成最优布局，往往导致结果需要后续的进一步优化。为克服这一局限，我们提出了 RollPlace——一种新颖且通用的宏单元布局框架。RollPlace 采用两阶段优化策略：首先通过机器学习方法或基于启发式的策略生成初始布局方案，然后通过调整来自初始阶段的关键宏单元，高效地对布局进行精细化优化。该策略有效规避了传统基于RL的布局方法中存在的序列生成限制。此外，RollPlace 无缝集成了蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS），以实现探索与利用之间的良好平衡，并采用回溯（rollout）机制进行高效的局部搜索。在 ISPD 2005 基准测试上的大量实验表明，RollPlace 的性能优于当前最先进的方法。同时，在 OpenROAD 平台上的端到端实验结果也显示，基于19个基准测试，RollPlace 在多个关键指标上表现优异。所提出的框架为应对现代芯片设计日益复杂的挑战提供了一种强大且可扩展的解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "P2P-Chiplet: Partition and Placement Co-Optimization for Multi-Chiplet Architecture",
    "authors": "Qidie Wu, Jiangyuan Gu, Xuguang Yuan, Shaojun Wei, Shouyi Yin",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240768",
    "source": "IEEE",
    "abstract": "The rising cost and complexity of cutting-edge process nodes have impeded large monolithic System-on-Chip to follow Moore’s Law, forcing chip designers to embrace Multi-Chiplet architectures. Multi-chiplet designs achieve cost reduction while maintaining near-monolithic performance by disaggregating a large die into smaller chiplets and integrating them through advanced packaging. The payback of this Disaggregation-Integration paradigm critically depends on the efficacy of chiplet Partition and Placement framework. However, existing frameworks fail to harness the potential merits offered by Partition-Placement Co-Optimization. Serving as an input provider for placement, partition phase typically adjusts block-to-die assignments to guide subsequent placement. This sequential dependency implies an inherent Partition-Placement (P2P) Inconsistency problem: solutions optimal solely in partition or placement may finally cause an inferior solution. Hence, this paper proposes P2P-Chiplet, a Partition-Placement Co-Optimization framework for multi-chiplet designs. Firstly, an optimized ACG structure, named as HeteroACG, is introduced to aggregate topological partition and physical placement optimization spaces. Then, the sequential partition-placement flow is decomposed into interleaved fine-grained epochs and an alternating progressive optimization strategy is employed to preserve P2P Consistency and bring better co-optimized solutions. Finally, experimental results show that, compared with existing chiplet partition-placement frameworks, our proposed P2P Chiplet notably mitigates potential performance bottlenecks while effectively reducing costs within acceptable overhead.",
    "title_zh": "P2P芯片小核：多芯片小核架构中的分区与布局协同优化",
    "abstract_zh": "尖端制程节点不断上升的成本与复杂性已阻碍大型单片系统芯片继续遵循摩尔定律，迫使芯片设计者转向多芯粒（Multi-Chiplet）架构。多芯粒设计通过将大尺寸芯片分解为多个较小的芯粒，并借助先进封装技术进行集成，在保持接近单片性能的同时实现了成本降低。这一“解耦-集成”范式所带来的收益，关键取决于芯粒划分与布局框架的有效性。然而，现有框架未能充分挖掘划分与布局协同优化（Partition-Placement Co-Optimization）所蕴含的优势。在传统流程中，划分阶段作为布局阶段的输入提供方，通常通过调整模块到芯粒的分配来引导后续布局。这种串行依赖关系导致了固有的“划分-布局不一致”（Partition-Placement, P2P Inconsistency）问题：仅在划分或布局阶段达到最优的方案，最终可能导致整体性能下降。为此，本文提出一种名为 P2P-Chiplet 的划分-布局协同优化框架，用于多芯粒设计。首先，引入一种优化后的 ACG（Architecture Connectivity Graph）结构——HeteroACG，以聚合拓扑划分与物理布局的优化空间。其次，将传统的串行划分-布局流程分解为交错的细粒度迭代周期，并采用交替渐进式优化策略，有效维持 P2P 一致性，从而获得更优的协同优化结果。最后，实验结果表明，相较于现有的芯粒划分-布局框架，所提出的 P2P-Chiplet 框架显著缓解了潜在的性能瓶颈，同时在可接受的开销范围内实现了有效的成本降低。"
  },
  {
    "date": "2025-11-20",
    "title": "AI Analog Circuit Design Agents : On Knowledge Extraction and Transfer with Knowledge Graphs",
    "authors": "Karthik Somayaji N.S, Peng Li",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240770",
    "source": "IEEE",
    "abstract": "Emulating expert human designers’ cognitive strategies is a long-standing aspiration in analog design automation. A persistent challenge is achieving generalizability, specifically, how insights from optimizing certain circuits can effectively inform the design of related circuits with differing topologies. For the first time, we identify required key components towards enabling AI-based topology-agnostic design knowledge transfer and demonstrate how such methodology can be developed for analog sizing optimization utilizing large language models (LLMs). The proposed framework, called Analog Design Optimization with Knowledge Transfer (ADO-KT) consists of three key integrated steps: 1) LLM-enabled knowledge graph (KG) generation for structured extraction of prior analog design knowledge, 2) LLM-enabled knowledge graph refinement for quality KG generation, and 3) topology-agnostic knowledge transfer for LLM-assisted optimization of new circuits. ADO-KT enables seamless summarization and transfer of quality knowledge flow between circuits of varying types and topologies, effectively mirroring human designers’ strategies. Empirical evaluations demonstrate significant improvements in design quality and data efficiency, highlighting the potential of the proposed approach for automated analog circuit design with generalized knowledge reuse.",
    "title_zh": "AI模拟电路设计智能体：基于知识图谱的知识提取与迁移",
    "abstract_zh": "模仿专家人类设计师的认知策略，一直是模拟电路设计自动化领域长期追求的目标。一个持续存在的挑战是实现泛化能力，即如何将优化特定电路所获得的洞见，有效应用于拓扑结构不同的相关电路的设计中。本文首次识别出实现基于人工智能的拓扑无关设计知识迁移所需的关键组件，并展示了如何利用大语言模型（LLMs）开发此类方法，以实现模拟电路尺寸优化。所提出的框架——基于知识迁移的模拟电路优化（Analog Design Optimization with Knowledge Transfer, ADO-KT），包含三个关键集成步骤：1）通过大语言模型驱动的知识图谱（KG）生成，实现对已有模拟电路设计知识的结构化提取；2）通过大语言模型驱动的知识图谱优化，生成高质量的知识图谱；3）实现拓扑无关的知识迁移，支持大语言模型辅助新电路的优化。ADO-KT能够实现不同类型、不同拓扑电路之间高质量知识的无缝总结与迁移，有效模拟人类设计师的设计策略。实证评估表明，该方法在设计质量与数据效率方面均取得显著提升，凸显了该方法在可泛化知识复用基础上实现自动化模拟电路设计的巨大潜力。"
  },
  {
    "date": "2025-11-20",
    "title": "SNO: Securing Network Function Offloading on FPGA-based SmartNICs in Untrusted Clouds",
    "authors": "Yunkun Liao, Jingya Wu, Wenyan Lu, Hang Lu, Xiaowei Li, Guihai Yan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240873",
    "source": "IEEE",
    "abstract": "As network bandwidth outpaces host CPU compute capability, Smart Network Interface Cards (SmartNICs) are increasingly deployed to offload network functions from the host CPU. FPGA-based SmartNICs excel due to their programmability at hardware speed, enabling high-performance and customized offloading. Securing offloaded network functions on FPGA-based SmartNICs is a critical challenge in the cloud, as the sensitive user cannot fully trust the cloud service provider (CSP). CPU Trusted Execution Environments (TEEs) protect software code, not FPGA hardware circuits. Existing FPGA TEEs fail to provide packet I/O protection, System-on-Chip (SoC) CPU utilization, and user-friendly memory access interfaces. To address this gap, we introduce SNO, the first TEE for FPGA-based SmartNICs with the secure boot, the SNO Manager for attestation and network function lifecycle orchestration, and the SNO Guard for I/O encryption and authentication. SNO increases SoC CPU utilization (+6.6% for 8-CPU SoC) by co-locating the SNO Manager with the CSP software while isolating the security-critical components of SNO Manager inside SoC CPU TEE, reduces performance overhead by integrating a fully-pipelined AES-GCM engine and overlapped execution, and offers a user-friendly (86.9% user code reduction) streaming interface. The experimental results show that SNO introduces a relative latency overhead of 7.7–143.2% (corresponding to absolute overheads up to 96 nanoseconds) across five network functions, significantly offset by microsecond-level latency savings from offloading.",
    "title_zh": "SNO：在不受信任的云环境中基于FPGA的智能网卡实现网络功能卸载的安全保障",
    "abstract_zh": "随着网络带宽的增长速度超过主机CPU的计算能力，智能网卡（SmartNICs）正被越来越多地部署以将网络功能从主机CPU中卸载。基于FPGA的智能网卡因其可在硬件层面实现可编程性，从而支持高性能和定制化的功能卸载，表现出显著优势。然而，在云环境中，如何保障基于FPGA的智能网卡上卸载的网络功能的安全性，成为一项关键挑战，因为用户无法完全信任云服务提供商（CSP）。传统的CPU可信执行环境（TEEs）仅能保护软件代码，而无法保护FPGA硬件电路。现有的FPGA TEE方案在数据包I/O保护、片上系统（SoC）CPU利用率以及用户友好的内存访问接口方面仍存在明显不足。\n\n为弥补这一空白，我们提出了SNO——首个面向基于FPGA的智能网卡的可信执行环境。SNO包含三个核心组件：安全启动机制、用于身份验证与网络功能生命周期管理的SNO Manager，以及用于I/O加密与认证的SNO Guard。SNO通过将SNO Manager与CSP软件共置于SoC上，同时将SNO Manager中的安全关键组件隔离至SoC CPU的可信执行环境内，使SoC CPU利用率提升6.6%（针对8核SoC），有效缓解了资源竞争问题。此外，SNO通过集成全流水线化的AES-GCM加密引擎并采用重叠执行机制，显著降低了性能开销；同时提供用户友好的流式接口，使用户代码量减少达86.9%。实验结果表明，SNO在五种典型网络功能上引入的相对延迟开销为7.7%至143.2%（对应绝对延迟增加最高达96纳秒），但这些开销被功能卸载带来的微秒级延迟节省所大幅抵消，整体性能表现优异。"
  },
  {
    "date": "2025-11-20",
    "title": "MM-GRADE: A Multi-Modal EDA Tool Documentation QA Framework Leveraging Retrieval Augmented Generation",
    "authors": "Yuan Pu, Zhuolun He, Shutong Lin, Jiajun Qin, Xinyun Zhang, Hairuo Han, Haisheng Zheng, Yuqi Jiang, Cheng Zhuo, Qi Sun, David Pan, Bei Yu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240857",
    "source": "IEEE",
    "abstract": "The complexity of EDA tools necessitates the development of advanced documentation query answering systems to enhance user efficiency and reduce the associated learning curve. Recent innovations in the use of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for EDA tool documentation have demonstrated significant progress; however, these approaches typically lack the multi-modal capabilities required to effectively handle visual data, such as circuit layout and GUI screenshots provided through user input. To address the concern, we introduce a multi-modal RAG system that incorporates two domain-customized modules: a multi-modal retriever model finetuned by the customized bilevel hard negative mining (BHNM) strategy, and a vision large language model (VLLM) finetuned using a tailored extract-score-answer pipeline. Moreover, we have manually curated ORD-MMBench, a multi-modal QA benchmark comprising 120 high-quality question-document-answer triplets based on OpenROAD documentation. Experimental results demonstrate that our customized RAG framework outperforms state-of-the-art multi-modal RAG flows and models on ORD-MMBench.",
    "title_zh": "MM-GRADE：一种利用检索增强生成技术的多模态EDA工具文档问答框架",
    "abstract_zh": "EDA工具的复杂性要求开发先进的文档查询问答系统，以提升用户效率并降低学习门槛。近年来，利用大语言模型（LLMs）和检索增强生成（RAG）技术处理EDA工具文档方面取得了显著进展；然而，这些方法通常缺乏处理视觉数据（如电路版图、GUI截图等）所需的多模态能力，而这些数据常作为用户输入的一部分。为解决这一问题，我们提出了一种融合两个领域定制化模块的多模态RAG系统：一是通过自定义双层硬负样本挖掘（BHNM）策略微调的多模态检索模型，二是采用定制化的“提取-评分-回答”流程微调的视觉大语言模型（VLLM）。此外，我们手动构建了ORD-MMBench——一个基于OpenROAD文档的多模态问答基准数据集，包含120个高质量的问题-文档-答案三元组。实验结果表明，我们的定制化RAG框架在ORD-MMBench上优于当前最先进的多模态RAG流程与模型。"
  },
  {
    "date": "2025-11-20",
    "title": "Hyle: An HLS Framework for Hyperdimensional Computing Accelerators on FPGAs",
    "authors": "Caio Vieira, Antonio Carlos Schneider Beck",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240971",
    "source": "IEEE",
    "abstract": "Hyperdimensional computing (HDC) is an emerging brain-inspired machine learning paradigm that exploits unique properties of high-dimensional vectors. HDC establishes a standard set of operations implemented by all of its classes, with Fourier and binary being the most common classes. The first achieves high accuracy but is built upon complex numbers, hindering its adoption in accelerators, whereas the latter is widely adopted in hardware despite providing lower accuracy. To overcome previous problems, the new CGR class was proposed to fit between Fourier and binary classes, learning better than the binary class at affordable hardware implementation w.r.t. Fourier. This paper introduces Hyle, an HLS-based framework for building HDC accelerators in CGR and binary for FPGAs. Hyle is the first proposal to accelerate CGR. We show that the learning advantage of CGR over BSC can result in faster and smaller accelerators and reduce model size up to 8× at iso-accuracy compared to binary.",
    "title_zh": "Hyle：一种用于FPGA上超维度计算加速器的HLS框架",
    "abstract_zh": "超维计算（HDC）是一种新兴的类脑机器学习范式，利用高维向量的独特性质。HDC建立了一套所有其类别都实现的标准操作，其中傅里叶类和二值类是最常见的两类。前者虽然具有较高的精度，但基于复数构建，限制了其在加速器中的应用；而后者尽管在硬件中被广泛采用，但精度较低。为解决上述问题，新提出的CGR类介于傅里叶类和二值类之间，在硬件实现成本上远低于傅里叶类的同时，学习性能优于二值类。本文提出Hyle，一个基于高层次综合（HLS）的框架，用于在FPGA上构建CGR和二值类的HDC加速器。Hyle是首个针对CGR类的加速方案。我们证明，与二值类相比，CGR类在保持相同精度时，可实现更快速、更小型的加速器，并将模型大小减少高达8倍。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited Paper: Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications",
    "authors": "Yaman Jandali, Ruisi Zhang, Nojan Sheybani, Farinaz Koushanfar",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240746",
    "source": "IEEE",
    "abstract": "Privacy-preserving technologies have introduced a paradigm shift that allows for realizable secure computing in real-world systems. The significant barrier to the practical adoption of these primitives is the significant computational and communication overhead that is incurred when applied at scale. In this paper, we present an overview of our efforts to bridge the gap between this overhead and practicality for privacy-preserving learning systems using multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE). Through meticulous hardware/software/algorithm co-design, we show progress towards enabling LLM-scale applications in privacy-preserving settings. We show the efficacy of our solutions in several contexts, including DNN IP ownership, ethical LLM usage enforcement, and transformer inference.",
    "title_zh": "特邀论文：优化隐私保护原语以支持大语言模型规模的应用",
    "abstract_zh": "隐私保护技术引入了一种范式转变，使得在现实世界系统中实现可落地的可信计算成为可能。这些技术在实际应用中的主要障碍在于，当其大规模部署时会带来显著的计算和通信开销。本文中，我们概述了在使用多方计算（MPC）、零知识证明（ZKPs）以及全同态加密（FHE）等技术来弥合这一开销与实用性之间差距方面的努力。通过精心的软硬件协同设计与算法优化，我们展示了在隐私保护环境下实现大语言模型（LLM）规模应用的重要进展。我们的解决方案在多个场景中展现出有效性，包括深度神经网络（DNN）知识产权保护、伦理化大语言模型使用合规性强制执行，以及Transformer推理。"
  },
  {
    "date": "2025-11-20",
    "title": "DiSPlace: Diffusion-Sharing-Driven Transistor-Level Placement Beyond Standard-Cell Boundaries for DTCO",
    "authors": "Keyu Peng, Yinuo Wu, Zhengzhe Zheng, Hao Gu, Ziran Zhu, Chao Wang, Jun Yang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240688",
    "source": "IEEE",
    "abstract": "As the increasing demands of design technology co-optimization (DTCO) in advanced nodes, the rigid configurations of standard cells impose significant limitations on wirelength and area optimization. A more flexible alternative is to place transistors directly on the design canvas, allowing for precise transistor-level adjustments that reduce wirelength and minimize design area. In this paper, we propose DiSPlace, a novel diffusion-sharing-driven transistor-level placement algorithm beyond standard-cell boundaries to fully leverage DTCO. We first present an in-cell placement based transistor pairing method to pair PMOS and NMOS transistors with the same gate net, followed by incorporating Gaussian perturbations to generate an initial placement. Then, we propose the first diffusion-sharing-driven global placement framework. It begins with the construction of diffusion sharing nets to guide transistor placement, followed by an analytical model for simultaneously optimizing diffusion sharing, wirelength, and density. Besides, a nonlinear optimization with adaptive penalty adjustment is presented to solve the analytical model effectively and efficiently. Finally, we develop a satisfiability modulo theories (SMT)-based detailed placement method to optimize design area and wirelength while ensuring legal placement. A diffusion-sharing-aware partitioning technique is also developed to enhance the scalability and efficiency of the SMT-based method. Compared to a standard-cell-based placer and the state-of-the-art transistor-level placer, our algorithm achieves significant improvements, reducing wirelength by 18% and 11%, and design area by 24% and 4%, respectively. These results highlight the effectiveness of DiSPlace in achieving high-quality placements for transistor-level designs.",
    "title_zh": "DiSPlace：一种超越标准单元边界的扩散共享驱动晶体管级布局方法，用于设计与工艺协同优化（DTCO）",
    "abstract_zh": "随着先进制程节点中设计与技术协同优化（DTCO）需求的不断增长，标准单元固有的刚性布局在布线长度和面积优化方面带来了显著限制。一种更具灵活性的替代方案是将晶体管直接放置于设计画布上，从而实现晶体管级别的精确调整，有效缩短布线长度并最小化设计面积。本文提出了一种名为DiSPlace的新方法，该方法突破标准单元边界，基于扩散共享驱动的晶体管级布局算法，以充分挖掘DTCO的潜力。我们首先提出一种基于单元内部的晶体管配对方法，将具有相同栅极网络的PMOS与NMOS晶体管进行配对；随后引入高斯扰动生成初始布局。接着，我们提出了首个基于扩散共享驱动的全局布局框架：首先构建扩散共享网络以指导晶体管布局，然后建立一个联合优化扩散共享、布线长度和密度的解析模型。此外，我们设计了一种带有自适应惩罚调整的非线性优化方法，以高效且有效地求解该解析模型。最后，我们开发了一种基于可满足性模理论（SMT）的详细布局方法，在确保布局合法性的前提下，进一步优化设计面积和布线长度。同时，我们还提出一种面向扩散共享的分区技术，显著提升了SMT方法的可扩展性和效率。与基于标准单元的布局工具以及当前最先进的晶体管级布局工具相比，我们的算法分别实现了18%和11%的布线长度缩减，以及24%和4%的设计面积减少。这些结果充分证明了DiSPlace在实现高质量晶体管级布局方面的有效性。"
  },
  {
    "date": "2025-11-20",
    "title": "Enhancing Timing Closure via Spatially Embedded Graph Transformer with Low Power/Area Overhead",
    "authors": "Joonyoung Seo, Jonghyeon Nam, Howoo Jang, Yoonseok Jung, Seokhyeong Kang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240684",
    "source": "IEEE",
    "abstract": "As technology scales and operating frequencies increase, achieving timing closure in digital circuits has become significantly challenging. Traditional post-placement optimization often fails to account for routing variations, leading to persistent timing violations and lengthy design iterations. To address this issue, we introduce a Spatially Embedded Graph Transformer (SEGT) for accurate timing prediction at the placement stage, considering the effects of subsequent processes. By incorporating distance-aware self-attention and a positional encoding scheme tailored to the geometric structure of circuit designs, SEGT effectively captures complex interactions between non-adjacency nodes and the positional context of circuits. Compared to previous timing prediction networks, SEGT improves the R<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> score from 0.915 to 0.959 in post-routing stage delay prediction and from 0.857 to 0.938 in post-routing path slack prediction. Leveraging accurate timing predictions of SEGT, a preemptive timing closure framework is developed to handle routing-induced violations proactively before post-placement optimization. Our framework identifies post-routing timing violation paths with slacks and adjusts the required arrival time by precisely the amount needed to clear each violation, thereby reducing additional design overhead. Experimental results of the proposed timing closure enhancement framework with SEGT predictions showed a substantial reduction in total negative slack across test designs while also lowering power and area overhead compared to previous works, demonstrating significant timing improvement and superior design efficiency. By enhancing the accuracy of timing prediction, this work enables a proactive and efficient timing closure approach, bridging the gap between placement and routing stages in physical design.",
    "title_zh": "通过具有低功耗/面积开销的嵌入空间图Transformer提升时序收敛",
    "abstract_zh": "随着技术节点的不断演进和工作频率的持续提升，数字电路中的时序收敛问题变得愈发严峻。传统的布局后优化方法往往无法充分考虑布线带来的变化，导致时序违规问题反复出现，并引发冗长的设计迭代过程。为解决这一难题，本文提出一种**空间嵌入图Transformer（Spatially Embedded Graph Transformer, SEGT）**，用于在布局阶段实现高精度的时序预测，充分考虑后续布线过程的影响。SEGT通过引入距离感知的自注意力机制以及针对电路设计几何结构定制的位置编码方案，能够有效捕捉非邻接节点间的复杂交互关系及电路中各单元的位置上下文信息。\n\n与以往的时序预测网络相比，SEGT在布线后的延迟预测方面将R²得分从0.915提升至0.959，在布线后的路径余量（slack）预测方面则从0.857提升至0.938，显著提高了预测准确性。基于SEGT提供的精确时序预测结果，我们进一步构建了一种**前瞻式时序收敛框架**，能够在布局后优化之前主动识别由布线引起的时序违规路径，并精准调整各路径所需的到达时间，仅以恰好消除违规所需的程度进行修正，从而最大限度减少额外的设计开销。\n\n实验结果表明，采用SEGT预测的时序收敛增强框架，在多个测试设计上均显著降低了总负余量（total negative slack），同时相较于以往方法，有效降低了功耗与面积开销，充分体现了其在时序优化方面的显著成效与卓越的设计效率。本研究通过大幅提升时序预测的准确性，实现了从布局到布线阶段的主动、高效时序收敛，成功弥合了物理设计中布局与布线之间的关键鸿沟。"
  },
  {
    "date": "2025-11-20",
    "title": "Design Space Exploration of a Unified FPGA Accelerator for Elliptic-Curve-Based Functions in Attribute-Based Encryption",
    "authors": "Anawin Opasatian, Momoko Fukuda, Makoto Ikeda",
    "publish": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
    "url": "https://doi.org/10.1109/tvlsi.2025.3632677",
    "source": "IEEE",
    "abstract": "Attribute-based encryption (ABE) requires several elliptic-curve-based functions, including elliptic-curve scalar multiplication (ECSM), hashing to the curve, and pairing. Although these computations share underlying similarities, prior hardware designs have shown that distinct architectures yield better performance for each function. Consequently, it remains unclear which architecture offers optimal performance when supporting all required functions within a unified design. In this work, we present a design space exploration methodology in which the design is parameterized using a defined set of design parameters, and an automatic schedule generator is employed to estimate the cycle counts for each function. This allows us to identify the configuration that minimizes latency for both individual functions and complete cryptographic operations. The versatility of our approach is demonstrated through two case studies: 1) ECSM over Curve25519, Secp256k1, NIST-P256, and NIST-P384; and 2) multiple elliptic-curve functions over the BLS12-381 curve. In the first case, the optimal configuration implemented on a Virtex7 field-programmable gate array (FPGA) achieves up to a 15% latency reduction compared to state-of-the-art designs. In the second case, the unified accelerator in its optimal configuration supports all core ABE functions and achieves superior latency or throughput-per-area (TPA), and in some cases both, compared to existing designs.",
    "title_zh": "基于属性加密中椭圆曲线函数的统一FPGA加速器设计空间探索",
    "abstract_zh": "基于属性的加密（ABE）需要多种基于椭圆曲线的运算，包括椭圆曲线标量乘法（ECSM）、哈希到曲线以及双线性对运算。尽管这些计算在底层具有相似性，但以往的硬件设计表明，每种函数各自采用不同的架构可获得更优性能。因此，在统一设计中同时支持所有所需功能时，尚不清楚哪种架构能实现最佳性能。本文提出一种设计空间探索方法，通过一组预定义的设计参数对设计进行参数化，并利用自动调度生成器来估算各项函数的周期数。这使得我们能够识别出在单个函数及完整密码运算中均能最小化延迟的最佳配置。我们的方法灵活性通过两个案例研究得以验证：1）针对Curve25519、Secp256k1、NIST-P256和NIST-P384曲线的ECSM；2）针对BLS12-381曲线的多种椭圆曲线运算。在第一种情况下，最优配置在Virtex7现场可编程门阵列（FPGA）上实现，与现有最先进设计相比，延迟最高降低15%。在第二种情况下，最优配置下的统一加速器能够支持所有核心ABE功能，并在延迟、单位面积吞吐量（TPA）或两者方面均优于现有设计。"
  },
  {
    "date": "2025-11-20",
    "title": "A Unified Design Flow for Homogeneous and Heterogeneous 3D Integration with Fine-Pitch Hybrid Bonding",
    "authors": "Gyumin Kim, Heechun Park",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240974",
    "source": "IEEE",
    "abstract": "Fine-pitch hybrid bonding offers significant power, performance, and area (PPA) benefits for 3D ICs. However, existing electronic design automation (EDA) tools lack native support for multi-tier integration, prompting the use of pseudo-3D design flows based on conventional 2D tools. Still, these flows often overlook key 3D-specific constraints, such as via count limitations and overlap avoidance, resulting in 3D via overflow and design rule violations of 3D via pitch overlaps. Furthermore, in the context of heterogeneous 3D ICs with different technology nodes, prior works fail to incorporate critical design considerations, including timing-aware cell assignment, congestion management, and wirelength control. In this paper, we propose a unified 3D IC design framework addressing these limitations through four key techniques: (1) Generalized footprint contraction to support both heterogeneous and homogeneous 3D integrations; (2) A constraint-driven tier partitioning for pitch-based 3D via budgeting that reduces 3D via count by up to 62% and maintaining 3D via utilization between 45–89%; (3) A timing-aware gain to steer performance-critical cells to the faster die, improving final WNS and TNS by up to 62% and 74%, respectively. (4) A post-route 3D via legalization stage eliminates all overlapping 3D vias utilizing reinforcement learning (RL) that minimizes design quality degradation. Evaluations on various homogeneous and heterogeneous 3D integrations on representative benchmarks show that our framework consistently improves timing, wirelength, and power, while removing all 3D via violations compared to conventional pseudo-3D flows.",
    "title_zh": "一种面向同质与异质3D集成的统一设计流程及细间距混合键合技术",
    "abstract_zh": "细间距混合键合为3D集成电路（3D ICs）带来了显著的功耗、性能和面积（PPA）优势。然而，现有的电子设计自动化（EDA）工具缺乏对多层级集成的原生支持，导致人们不得不采用基于传统2D工具的伪3D设计流程。但这些流程常常忽略关键的3D特定约束，例如通孔数量限制和重叠规避问题，从而引发3D通孔溢出以及3D通孔节距重叠的设计规则违规。此外，在涉及不同工艺节点的异构3D IC中，以往的研究未能充分考虑一些关键设计因素，包括时序感知的单元分配、拥塞管理以及布线长度控制。本文提出了一种统一的3D IC设计框架，通过四项关键技术克服上述局限：（1）广义版图收缩技术，支持异构与同构3D集成；（2）基于约束的层级划分方法，实现基于节距的3D通孔预算，可将3D通孔数量减少高达62%，同时保持3D通孔利用率在45%–89%之间；（3）时序感知增益机制，引导性能关键单元布局到速度更快的芯片层，使最终的负 slack（WNS）和总负 slack（TNS）分别改善最多达62%和74%；（4）后布线阶段的3D通孔合法化处理，利用强化学习（RL）消除所有重叠的3D通孔，同时最小化设计质量的退化。在多个代表性基准测试平台上对多种同构与异构3D集成方案进行评估表明，与传统的伪3D流程相比，本框架在保持无3D通孔违规的前提下，持续提升了时序、布线长度和功耗表现。"
  },
  {
    "date": "2025-11-20",
    "title": "Open3DFlow: An Open-Source EDA Platform for 3D Chip Design with AI Enhancement",
    "authors": "Yifei Zhu, Dawei Feng, Zhenxuan Luan, Lei Ren, Weiwei Chen, Zhangxi Tan",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240886",
    "source": "IEEE",
    "abstract": "Driven by demands for high-performance, energy-efficient electronics, 3D integrated circuits (3D ICs) have emerged as a transformative solution. However, the absence of specialized electronic design automation (EDA) tools and standardized design flows for 3D chiplets remains a critical barrier. To address this, we present Open3DFlow, an open-source EDA platform for 3D IC design that integrates a seven-step workflow encompassing ASIC back-end processes and multi-physics analysis. It includes modeling of through silicon vias (TSVs), thermal, and signal integrity (SI) simulation. We validate Open3DFlow through implementing a 3D RISC-V CPU with a vertically stacked L2 cache in GlobalFoundries 0.18µm (GF180). It enables face-to-face (F2F) die coupling via a custom bonding layer derived from the original technology file. For scalability assessment, we highlight its compatibility with commercial-level PDKs through specific manual handling, while this methodology proves implementable in proprietary EDA tools, with open-source EDA exhibiting superior 3D-aware potential. Leveraging Open3DFlow’s open-source advantages that facilitate AI integration, we demonstrate automated bonding pad placement and Tcl script generation using large language models (LLMs), thus enhancing design productivity. This work spearheads an open-source framework for 3D IC co-design, orchestrating cross-domain expertise to transmute collective innovation into silicon manufacturing ecosystems.",
    "title_zh": "Open3DFlow：一个基于人工智能增强的开源3D芯片设计EDA平台",
    "abstract_zh": "在高性能、高能效电子设备需求的推动下，三维集成电路（3D IC）已成为一项变革性解决方案。然而，缺乏针对3D芯片小片（chiplets）的专用电子设计自动化（EDA）工具和标准化设计流程，仍是当前面临的关键障碍。为应对这一挑战，我们提出了Open3DFlow——一个开源的EDA平台，专用于3D IC设计，集成了涵盖ASIC后端流程与多物理场分析的七步工作流。该平台包含对硅通孔（TSVs）的建模，以及热分析和信号完整性（SI）仿真功能。我们通过在GlobalFoundries 0.18µm（GF180）工艺上实现一款采用垂直堆叠L2缓存结构的3D RISC-V CPU，验证了Open3DFlow的有效性。该平台支持通过基于原始工艺文件定制的键合层实现晶圆面对面（F2F）堆叠。在可扩展性评估方面，我们展示了其与商业级PDK兼容的潜力，尽管需进行特定的手动配置；同时表明该方法可移植至专有EDA工具中，而开源EDA在3D感知能力方面展现出更优前景。借助Open3DFlow的开源优势，我们成功实现了利用大语言模型（LLMs）自动完成键合焊盘布局及Tcl脚本生成，显著提升了设计效率。本研究开创了一种面向3D IC协同设计的开源框架，整合跨领域专业知识，将集体创新转化为可落地的硅制造生态系统。"
  },
  {
    "date": "2025-11-20",
    "title": "OA-LAMA: An Outlier-Adaptive LLM Inference Accelerator with Memory-Aligned Mixed-Precision Group Quantization",
    "authors": "Huangxu Chen, Yingbo Hao, Yi Zou, Xinyu Chen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240926",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) face significant deployment challenges due to their substantial memory and computational demands. While low-precision quantization offers a promising solution, the presence of activation outliers severely degrades model accuracy. Existing approaches either compromise hardware efficiency through misaligned memory access or sacrifice quantization granularity through rigid bit-width allocation, particularly when handling non-uniform tensor distributions across and within layers. This paper presents a hardware-software co-designed framework resulting in an outlier-adaptive LLM inference accelerator with memory-aligned mixed-precision group quantization, named OA-LAMA. The framework comprises three key innovations: First, an outlier-adaptive memory-aligned mixed-precision group (OAMAG) format with a novel outlier reordering technique is proposed to preserve accuracy while maintaining DRAM-aligned memory access. Second, a distribution-aware group allocation strategy is proposed to address inter-layer outlier ratio variance. Finally, we design the OA-LAMA hardware architecture with a three-level accumulation architecture and timing-balanced processing elements to support the OAMAG format efficiently. Evaluations demonstrate that OA-LAMA achieves better accuracy than state-of-the-art 4-bit quantization methods while delivering 1.21–3.09× performance improvement and 1.35–2.47× energy efficiency gains over leading LLM accelerators. OA-LAMA establishes new Pareto frontiers in accuracy-efficiency co-optimization for LLM inference. OA-LAMA is open-sourced at https://github.com/CLab-HKUST-GZ/ICCAD25_OA-LAMA.git.",
    "title_zh": "OA-LAMA：一种具有内存对齐混合精度分组量化能力的异常值自适应大模型推理加速器",
    "abstract_zh": "大型语言模型（LLMs）由于其巨大的内存和计算需求，在实际部署中面临严峻挑战。尽管低精度量化提供了一种有前景的解决方案，但激活值中的异常值会严重降低模型精度。现有方法要么因内存访问不对齐而牺牲硬件效率，要么因固定的位宽分配而降低量化粒度，尤其在处理层间与层内张量分布不均的情况下表现不佳。本文提出一种软硬件协同设计的框架，构建了一种面向异常值自适应的LLM推理加速器——OA-LAMA，该框架采用内存对齐的混合精度分组量化技术。本工作包含三大创新：第一，提出一种新型的异常值自适应、内存对齐的混合精度分组（OAMAG）格式，并引入独特的异常值重排序技术，可在保持DRAM对齐内存访问的同时有效保障模型精度；第二，设计了一种感知数据分布的分组分配策略，以应对不同层间异常值比例的变化；第三，构建了支持OAMAG格式的OA-LAMA硬件架构，采用三级累加结构与时间平衡的处理单元，实现高效计算。实验结果表明，OA-LAMA在精度上优于当前最先进的4比特量化方法，同时相比领先的LLM加速器实现了1.21–3.09倍的性能提升以及1.35–2.47倍的能效增益。OA-LAMA在LLM推理的精度-效率协同优化方面树立了新的帕累托前沿。相关代码已开源，地址为：https://github.com/CLab-HKUST-GZ/ICCAD25_OA-LAMA.git。"
  },
  {
    "date": "2025-11-20",
    "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs",
    "authors": "Benjamin Kubwimana, Qijing Huang",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00018",
    "source": "IEEE",
    "abstract": "Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources.To navigate these constraints, developers must balance multiple design factors—choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating to-ken budgets, and applying test-time scaling strategies—to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce.In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.",
    "title_zh": "边缘推理：边缘GPU上大语言模型部署的推理特性分析",
    "abstract_zh": "边缘智能范式正日益受到新兴自主系统（如机器人）的推动。除了在连接受限环境中保障隐私保护和运行韧性外，边缘部署相较于基于云的解决方案还具有显著的能效与成本优势。然而，在边缘GPU上部署大型语言模型（LLM）以执行推理任务时，仍面临来自严格延迟约束和有限计算资源的严峻挑战。为应对这些限制，开发者必须权衡多种设计因素——包括选择推理型与非推理型架构、确定合适的模型规模、分配令牌预算，以及应用测试时扩展策略——以满足目标延迟要求并优化准确性。然而，关于这些变量最优组合的指导仍然十分匮乏。\n\n在本研究中，我们提出了EdgeReasoning，一项全面的研究，旨在刻画推理型LLM在边缘GPU上的部署特性。我们系统地量化了不同LLM架构与模型规模下的延迟-准确率权衡关系。我们还系统评估了基于提示（prompt-based）和基于模型微调（model-tuning-based）的技术，以在保持性能质量的前提下减少推理阶段的令牌长度。此外，我们对不同并行度的测试时扩展方法进行了深入分析，以在严格的延迟预算下最大化准确性。通过上述分析，EdgeReasoning绘制出了可实现的准确率-延迟配置的帕累托前沿，为推理型LLM在边缘端的最优部署提供了系统性指导。"
  },
  {
    "date": "2025-11-20",
    "title": "Versatile Rewiring and Concurrent Resynthesis for High-Quality Customized Optimization",
    "authors": "Jiun-Hao Chen, Jie-Hong R. Jiang, Alan Mishchenko",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240927",
    "source": "IEEE",
    "abstract": "When traditional logic synthesis algorithms reach saturation, yet the demand for improved synthesis quality remains high, research focus shifts toward identifying synergistic synthesis algorithms and efficient deployment strategies. This paper proposes a novel rewiring method that can substantially restructure a circuit using truth tables, though it does not scale beyond 16 inputs. To address this limitation, the method is integrated with a new concurrent resynthesis framework that partitions the circuit into support-limited subcircuits, enabling the application of rewiring at scale. Experimental results demonstrate the effectiveness of the proposed approach in minimizing various cost functions, including node count, transistor count, and area after standard-cell mapping, achieving substantial improvements over strong baselines.",
    "title_zh": "多功能重布线与并发重综合以实现高质量的定制化优化",
    "abstract_zh": "当传统逻辑综合算法达到性能瓶颈，而对提升综合质量的需求依然强烈时，研究重点逐渐转向寻找具有协同效应的综合算法以及高效的部署策略。本文提出了一种新颖的重布线方法，能够利用真值表对电路进行大幅度重构，但该方法在输入规模超过16个时难以扩展。为克服这一局限性，本文将该方法与一种新的并行重综合框架相结合，将电路划分为受支持数量限制的子电路，从而实现重布线技术的大规模应用。实验结果表明，所提方法在最小化多种成本函数方面表现出色，包括节点数量、晶体管数量以及标准单元映射后的面积，在多个方面均显著优于现有强基线方法。"
  },
  {
    "date": "2025-11-20",
    "title": "Apollo: Automated Routing-Informed Placement for Large-Scale Photonic Integrated Circuits",
    "authors": "Hongjian Zhou, Haoyu Yang, Nicholas Gangi, Zhaoran Rena Huang, Haoxing Ren, Jiaqi Gu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240728",
    "source": "IEEE",
    "abstract": "As technology advances, photonic integrated circuits (PICs) are rapidly scaling in size and complexity, with modern designs integrating thousands of components to meet the demands of artificial intelligence (AI), high-performance computing, and chip-to-chip optical interconnects. However, the analog custom layout nature of photonics, the curvy waveguide structures, and single-layer routing resources impose stringent physical constraints, such as minimum bend radii and waveguide crossing penalties, which make manual layout the de facto standard. This manual process takes weeks to complete and is error-prone, which is fundamentally unscalable for large-scale PIC systems. Existing automation solutions have adopted force-directed placement on small benchmarks with tens of components, with limited routability and scalability. To fill this fundamental gap in the electronic-photonic design automation (EPDA) toolchain, we present Apollo, the first GPU-accelerated, routing-informed placement framework tailored for large-scale PICs. Apollo features an asymmetric bending-aware wirelength function with explicit modeling of waveguide routing congestion and crossings to preserve enough routing spacing for routability maximization. Meanwhile, conditional projection is employed to gradually enforce a variety of user-defined layout constraints, including alignment, spacing, etc. This constrained optimization is accelerated and stabilized by a custom blockwise adaptive Nesterov-accelerated optimizer, ensuring stable and high-quality convergence. To catalyze research in PIC layout automation, we also develop and open-source large-scale PIC benchmarks derived from real-world photonic tensor core designs. Compared to existing methods, Apollo can generate high-quality layouts for large-scale PICs with an average routing success rate of 94.79% across all benchmarks within minutes. By tightly coupling placement with physical-aware routing, Apollo establishes a new paradigm for automated PIC design—bringing intelligent, scalable layout synthesis to the forefront of next-generation EPDA. Our code is open-sourced at link<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*</sup>.",
    "title_zh": "阿波罗：面向大规模光子集成电路的自动化布线感知布局方法",
    "abstract_zh": "随着技术的进步，光子集成电路（PICs）在规模和复杂度上迅速增长，现代设计已集成数千个元件，以满足人工智能（AI）、高性能计算以及芯片间光互连的需求。然而，光子学固有的模拟定制布局特性、弯曲的波导结构以及单层布线资源，带来了严格的物理约束，如最小弯曲半径和波导交叉惩罚等，这使得手动布局成为事实上的标准。这种人工流程耗时数周且容易出错，对于大规模PIC系统而言根本无法实现可扩展性。现有的自动化解决方案仅在包含数十个组件的小型基准测试中采用力导向布局方法，其布线能力与可扩展性均有限。\n\n为填补电子-光子设计自动化（EPDA）工具链中的这一根本性空白，我们提出了Apollo——首个专为大规模PIC设计而打造的GPU加速、路由感知的布局框架。Apollo采用一种非对称的弯折感知线长函数，显式建模了波导布线拥塞与交叉情况，从而保留足够的布线空间以最大化可布线性。同时，通过条件投影机制，逐步施加多种用户定义的布局约束，包括对齐、间距等。该约束优化过程由一个自定义的分块自适应Nesterov加速优化器加速并稳定，确保收敛过程稳定且结果质量高。\n\n为进一步推动PIC布局自动化的研究，我们还基于真实世界的光子张量核心设计，开发并开源了大规模PIC基准数据集。与现有方法相比，Apollo能够在几分钟内为大规模PIC生成高质量布局，在所有基准测试中平均布线成功率高达94.79%。通过将布局与物理感知的布线紧密耦合，Apollo建立了一种全新的自动化PIC设计范式，将智能、可扩展的布局综合推向下一代EPDA的核心位置。我们的代码已在链接<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">*</sup>公开。"
  },
  {
    "date": "2025-11-20",
    "title": "CTDM: Resource-Efficient FPGA-Accelerated Simulation of Large-Scale NPU Designs",
    "authors": "Hyunje Jo, Han-Sok Suh, Hyungseok Heo, Jinseok Kim, Hyunsung Kim, Boeui Hong, Jungju Oh, Sunghyun Park, Jinwook Oh, Sunghwan Jo, Kangwook Lee, Jae-Sun Seo",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240632",
    "source": "IEEE",
    "abstract": "This paper proposes a novel approach to accelerate large Neural Processing Unit (NPU) simulations on FPGA through Chain-based Time-Division Multiplexing (CTDM) and its automatic compiler. CTDM replaces repeated logic patterns with a single logic pattern and register chains, which can take advantage of built-in shift register primitives. It reduces FPGA resource utilization more effectively than conventional multiplexer-based Time-Division Multiplexing (TDM) approaches by minimizing logic overhead and routing congestion. The automated CTDM compiler supports various hardware design languages (HDL) including Verilog, VHDL, high-level synthesis (HLS), and Chisel, as well as a wide range of FPGA devices—from small on-premise boards to server-grade hardware simulators like Synopsys ZeBu. To extend the applicability of CTDM to multi-FPGA systems, we propose a block interleaving technique that hides inter-FPGA link latency and fully utilizes the pipeline in a high-speed serial I/O channel. When applied to NVIDIA Deep Learning Accelerator (NVDLA), CTDM achieved a 66% and 82% reduction in LUT and FF utilization, respectively, and enabled the successful deployment of the largest variant of NVDLA on a single AMD U250 FPGA device. This demonstrated a 3,653× acceleration in NVDLA simulation time over the Synopsys VCS simulator on a CPU. This method has already been implemented for the simulation and verification of our proprietary NPUs. Notably, it enabled the simulation of a 4-die 1024 TFLOPS chiplet using 144 FPGAs on ZeBu 5 server.",
    "title_zh": "CTDM：面向大规模NPU设计的资源高效型FPGA加速仿真",
    "abstract_zh": "本文提出了一种通过基于链式时分复用（CTDM）及其自动化编译器，加速在FPGA上进行大规模神经处理单元（NPU）仿真的新方法。CTDM将重复的逻辑模式替换为单一逻辑模式与寄存器链结构，能够充分利用FPGA内部的移位寄存器原语。相比传统的基于多路选择器的时分复用（TDM）方法，CTDM通过最小化逻辑开销和布线拥塞，更有效地降低了FPGA资源占用。该自动化CTDM编译器支持多种硬件设计语言（HDL），包括Verilog、VHDL、高层次综合（HLS）以及Chisel，并兼容从小型本地开发板到高端服务器级硬件仿真器（如Synopsys ZeBu）等多种FPGA设备。为进一步拓展CTDM在多FPGA系统中的应用，我们提出了一种块交织技术，可隐藏跨FPGA链路的延迟，并充分挖掘高速串行I/O通道中的流水线潜力。当应用于NVIDIA深度学习加速器（NVDLA）时，CTDM实现了LUT资源减少66%、触发器（FF）资源减少82%的效果，并成功将最大版本的NVDLA部署于单个AMD U250 FPGA设备上。相较于CPU上的Synopsys VCS仿真器，该方法使NVDLA仿真速度提升了3,653倍。该技术已成功应用于我们自有NPU的仿真与验证。尤为突出的是，它实现了使用144个FPGA在ZeBu 5服务器上对一个4芯片堆叠、1024 TFLOPS的芯片组进行仿真。"
  },
  {
    "date": "2025-11-20",
    "title": "CIMTester: An Agile Golden-Result-Free BIST Compiler for Robust Compute-In-Memory",
    "authors": "Wenjie Ren, Meng Wu, Mingxuan Li, Peiyu Chen, Tianyu Jia, Le Ye",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240874",
    "source": "IEEE",
    "abstract": "Digital compute-in-memory (DCIM) is playing an increasingly vital role in efficient AI computing due to its significant efficiency advantages. However, the combination of memory and computation logics in DCIM presents challenges for testing, including extra coupling fault between memory and logic, high overhead for golden result generation and indirect fault location. In this paper, we present a golden-result-free BIST structure together with a computation-coupled CIM BIST algorithm to reduce testing overhead and improve test coverage. In addition, we present CIMTester, a DCIM BIST compiler to adapt to swiftly changing test requirements and DCIM macro sizes and numbers in chips. The template-based generator generates BIST RTL based on proposed BIST structure and modifies the templates according to the architecture parameters of test chip. CIMTester’s iterator analyzes diverse sharing strategy of BIST components to satisfy user specifications of area, test time and fault coverage. We implemented and evaluated a series of TSMC 22nm DCIM macros with the generated BIST circuits, which achieves up to 99.48% fault coverage with only less than 2.44% area overhead.",
    "title_zh": "CIMTester：一种面向计算内存的敏捷式无金标准内置自测编译器",
    "abstract_zh": "数字计算内存（DCIM）由于其显著的能效优势，在高效人工智能计算中正发挥着日益重要的作用。然而，DCIM将存储与计算逻辑集成在一起，给测试带来了挑战，包括存储与逻辑之间的额外耦合故障、黄金结果生成开销高以及故障定位间接等问题。本文提出了一种无需黄金结果的BIST结构，并结合一种计算耦合的CIM BIST算法，以降低测试开销并提升测试覆盖率。此外，我们还提出了CIMTester——一款专为快速变化的测试需求以及芯片中DCIM宏单元规模和数量而设计的DCIM BIST编译器。该编译器采用基于模板的生成机制，根据所提出的BIST结构生成BIST RTL代码，并依据测试芯片的架构参数动态调整模板。CIMTester的迭代器模块能够分析多种BIST组件共享策略，以满足用户对面积、测试时间及故障覆盖率的具体要求。我们在台积电22nm工艺下实现并评估了一系列DCIM宏单元，所生成的BIST电路在仅增加不到2.44%面积开销的情况下，实现了高达99.48%的故障覆盖率。"
  },
  {
    "date": "2025-11-20",
    "title": "ISO 26262-Aligned Functional Safety Verification Framework with Explainable Graph Neural Network",
    "authors": "Yutao Sun, Jiehua Huang, Xiangping Liao, Zhijun Wang, Liping Liang",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240753",
    "source": "IEEE",
    "abstract": "The growing complexity and integration of automotive electronic systems, driven by advancements in intelligent vehicles and autonomous driving, make functional safety (FuSa) verification critical for ensuring system reliability. Traditional fault injection (FI) methods face inefficiencies, scalability limitations, and interpretability gaps, hard to meet the stringent requirements of ISO 26262 standards for safety-critical automotive systems. This paper proposes an explainable Graph Neural Network(GNN)-based framework for FuSa verification in automotive electronics through three core contributions: graph neural networks for modeling circuit structures to identify critical fault nodes, gradient-driven feature importance analysis to optimize selective hardening with minimal resource overhead and GNNExplainer to visualize critical nodes and connections driving fault-criticality predictions through subgraph analysis. Validated across diverse circuits the framework achieves up to 99.6% precision and 99.8% F1-score in fault detection while significantly reducing the simulation time. Notably, the framework improves accuracy by approximately 5% compared to state-of-the-art (SOTA) methods while requiring only half the fault injection data. Integrated explainable artificial intelligence techniques provide transparent decision traces, ensuring compliance with ISO 26262 traceability requirements. Through feature selection, the framework achieves comparable accuracy with a minimal feature set, significantly reducing computational overhead while maintaining performance. By bridging AI-driven automation with rigorous safety certification, this work establishes a scalable, efficient, and interpretable solution for FuSa verification in automotive SoCs.",
    "title_zh": "符合ISO 26262的可解释图神经网络功能安全验证框架",
    "abstract_zh": "随着智能汽车与自动驾驶技术的不断发展，汽车电子系统的复杂性与集成度持续提升，功能安全（FuSa）验证已成为保障系统可靠性的重要环节。传统的故障注入（FI）方法存在效率低下、可扩展性差以及可解释性不足等问题，难以满足汽车安全关键系统对ISO 26262标准的严苛要求。本文提出一种基于可解释图神经网络（GNN）的功能安全验证框架，通过三大核心贡献实现突破：首先，利用图神经网络建模电路结构，识别出关键故障节点；其次，采用基于梯度的特征重要性分析，实现选择性加固优化，在极低资源开销下提升系统鲁棒性；最后，引入GNNExplainer技术，通过子图分析可视化驱动故障严重性预测的关键节点与连接关系。在多种电路设计上的验证结果表明，该框架在故障检测中最高可达99.6%的精确率和99.8%的F1分数，同时显著降低仿真耗时。尤为突出的是，相较于当前最先进的方法（SOTA），本框架在仅需一半故障注入数据的情况下，准确率提升了约5%。通过集成可解释人工智能技术，系统能够提供透明的决策追溯路径，确保符合ISO 26262标准所要求的可追溯性。此外，借助特征选择机制，框架在使用极少量特征的前提下仍能保持优异性能，大幅降低计算开销。本研究成功实现了人工智能驱动的自动化与严格安全认证之间的融合，为汽车SoC中的功能安全验证提供了一种可扩展、高效且可解释的解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "A Hybrid Hardware-Software Scheduling Scheme for Heterogeneous Many-Core Systems",
    "authors": "Limin Jiang, Yi Shi, Siyi Xu, Shan Cao, Zhiyuan Jiang",
    "publish": "IEEE Embedded Systems Letters",
    "url": "https://doi.org/10.1109/les.2025.3635097",
    "source": "IEEE",
    "abstract": "As heterogeneous many-core systems gain prominence in embedded domains, efficient scheduling mechanisms become increasingly vital. Existing solutions often suffer from trade-offs between real-time responsiveness, parallelism, and programmability. We propose a hybrid hardware-software scheduling framework for heterogeneous many-core systems, which ensures the software algorithm’s scheduling results while dynamically adjusting task execution at runtime based on hardware status. Task dependencies are analyzed offline using a domain-specific language (DSL) to generate optimized schedules. At runtime, a hardware scheduler performs fast dependency checks and task dispatching. Experimental results show efficient parallel scheduling with minimal overhead, positioning the framework as a practical solution for latency-sensitive applications such as wireless baseband processing.",
    "title_zh": "异构多核系统中的一种软硬件协同调度方案",
    "abstract_zh": "随着异构多核系统在嵌入式领域的重要性日益凸显，高效的调度机制变得愈发关键。现有解决方案往往在实时响应性、并行度和可编程性之间存在权衡。本文提出一种面向异构多核系统的软硬件协同调度框架，该框架在保证软件算法调度结果的同时，能够根据硬件状态动态调整任务执行。通过领域专用语言（DSL）对任务依赖关系进行离线分析，生成优化的调度方案；在运行时，由硬件调度器快速完成依赖检查与任务分发。实验结果表明，该框架实现了高效的并行调度，且开销极小，为无线基带处理等对延迟敏感的应用提供了切实可行的解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "Understanding Distributed Training of Large Language Models with Unified Virtual Memory",
    "authors": "Jane Rhee, Eunbi Jeong, Jiwon Lee, Myung Kuk Yoon",
    "publish": "2025 IEEE International Symposium on Workload Characterization (IISWC)",
    "url": "https://doi.org/10.1109/iiswc66894.2025.00016",
    "source": "IEEE",
    "abstract": "Keeping up with the soaring memory demands of Large Language Models (LLMs) is one of the biggest hurdles in modern system design. Given their size, distributed training across multiple GPUs is no longer optional but required. As distributed training takes center stage, unlocking parallelism across devices is more important than ever. Still, even the best parallelism techniques face roadblocks imposed by limited memory capacity. Unified Virtual Memory (UVM) o˛ers a compelling solution by automating data transfers between host and device via demand paging, thereby supporting memory oversubscription. Despite its potential, the integration of UVM with various parallelism strategies in distributed training has yet to be thoroughly investigated. This study explores new opportunities for leveraging UVM in distributed LLM training. Our analysis shows that finer-grained parallelism accelerates training performance under UVM, although the increased communication between small partitions demands careful attention. This approach also yields better memory efficiency with fewer page faults and contributes to improved scalability. As the first in-depth analysis of parallelism in distributed training with UVM, this work provides key insights into how UVM can be effectively integrated alongside diverse parallelism strategies for training large-scale LLMs.",
    "title_zh": "理解具有统一虚拟内存的大语言模型分布式训练",
    "abstract_zh": "跟上大型语言模型（LLMs）不断增长的内存需求，是现代系统设计中最大的挑战之一。由于模型规模庞大，跨多个GPU进行分布式训练已不再是可选方案，而是必须采取的手段。随着分布式训练成为主流，实现设备间并行计算的重要性愈发凸显。然而，即便最先进的并行技术也受限于有限的内存容量。统一虚拟内存（UVM）提供了一种极具吸引力的解决方案：通过按需分页自动完成主机与设备之间的数据传输，从而支持内存超分配。尽管UVM潜力巨大，但其在分布式训练中与各类并行策略的结合仍缺乏深入研究。本研究探索了在分布式LLM训练中利用UVM的新机遇。我们的分析表明，在UVM环境下，更细粒度的并行化能够提升训练性能，尽管这会带来小分区之间通信量的增加，需要格外关注。此外，该方法还能减少页面错误，提高内存效率，并增强系统的可扩展性。作为首次对UVM环境下分布式训练中并行性的深入分析，本工作为如何有效将UVM与多种并行策略协同应用于大规模LLM训练提供了关键洞见。"
  },
  {
    "date": "2025-11-20",
    "title": "SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning",
    "authors": "Junzhuo Zhou, Ziwen Wang, Haoxuan Xia, Yuxin Yan, Chengyu Zhu, Ting-Jung Lin, Wei Xing, Lei He",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240969",
    "source": "IEEE",
    "abstract": "Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltage-temperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multi-corner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4× overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learning-based approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.",
    "title_zh": "SetupKit：基于偏差增强插值与主动学习的高效多角落建立/保持时间特性分析",
    "abstract_zh": "精确的建立/保持时间表征对于现代芯片时序收敛至关重要，但其依赖于在多种工艺-电压-温度（PVT）条件下进行数百万次SPICE仿真，形成了一个重大瓶颈，通常持续数周甚至数月。现有方法在多PVT角场景下普遍存在搜索收敛缓慢、探索效率低的问题。我们提出SetupKit——一种创新框架，通过统计智能、电路分析与主动学习（Active Learning, AL）技术打破这一瓶颈。SetupKit融合三大核心创新：BEIRA（偏差增强插值搜索），基于统计误差建模，可有效克服收敛停滞问题，加速搜索进程；基于电路分析的初始搜索区间估计；以及采用高斯过程的主动学习策略。该主动学习组件能够智能地学习PVT与时序之间的相关性，主动引导昂贵的仿真任务至最具信息量的PVT角落，从而大幅减少多角表征中的冗余。在16个PVT角的22nm工业标准单元上进行评估，SetupKit相比传统方法实现了显著的2.4倍整体CPU时间节省（单核计算从720天降至290天），极大缩短了表征周期。SetupKit提供了一种系统化、基于学习的库表征方法，有效应对EDA领域的一项关键挑战，为更智能的仿真管理开辟了新路径。"
  },
  {
    "date": "2025-11-20",
    "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs",
    "authors": "Federico Nicolás Peccia, Frederik Haxel, Oliver Bringmann",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11241007",
    "source": "IEEE",
    "abstract": "RISC-V provides a flexible and scalable platform for applications ranging from embedded devices to high-performance computing clusters. Particularly, its RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI workloads. But writing software that efficiently utilizes the vector units of RISC-V CPUs without expert knowledge requires the programmer to rely on the autovectorization features of compilers or hand-crafted libraries like muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing the integration with the RISC-V RVV extension, thus heavily limiting the efficient deployment of complex AI workloads. In this paper, we present a workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V vector units. Instead of relying on hand-crafted libraries, we integrated the RVV extension into TVM’s MetaSchedule framework, a probabilistic program framework for tensor operation tuning. We implemented different RISC-V SoCs on an FPGA and tuned a wide range of AI workloads on them. We found that our proposal shows a mean improvement of 46% in execution latency when compared against the autovectorization feature of GCC, and 29% against muRISCV-NN. Moreover, the binary resulting from our proposal has a smaller code memory footprint, making it more suitable for embedded devices. Finally, we also evaluated our solution on a commercially available RISC-V SoC implementing the RVV 1.0 Vector Extension and found our solution is able to find mappings that are 35% faster on average than the ones proposed by LLVM. We open-sourced our proposal for the community to expand it to target other RISC-V extensions.",
    "title_zh": "基于概率程序的RISC-V向量扩展张量程序优化",
    "abstract_zh": "RISC-V 为从嵌入式设备到高性能计算集群的应用提供了灵活且可扩展的平台。特别是其 RISC-V 向量扩展（RVV）在加速人工智能（AI）工作负载方面引起了广泛关注。然而，若无专家知识，程序员难以高效利用 RISC-V CPU 的向量单元，通常只能依赖编译器的自动向量化功能，或使用手工优化的库（如 muRISCV-NN）。而更智能的方法，例如自动调优框架，却尚未与 RISC-V RVV 扩展实现有效集成，从而严重限制了复杂 AI 工作负载的高效部署。\n\n本文提出了一种基于 TVM 编译器的工作流，能够高效地将 AI 工作负载映射到 RISC-V 向量单元上。我们并未依赖手工优化的库，而是将 RVV 扩展集成至 TVM 的 MetaSchedule 框架——一个用于张量操作调优的概率化程序框架中。我们在 FPGA 上实现了多种 RISC-V 系统级芯片（SoC），并在这些平台上对大量 AI 工作负载进行了调优。实验结果表明，与 GCC 的自动向量化功能相比，我们的方法平均降低了 46% 的执行延迟；与 muRISCV-NN 相比，也降低了 29%。此外，我们方案生成的二进制代码具有更小的代码内存占用，因而更适合嵌入式设备。\n\n最后，我们还在一款商用 RISC-V SoC 上评估了该方案，该芯片实现了 RVV 1.0 向量扩展。结果表明，我们的方法所找到的映射平均比 LLVM 提出的方案快 35%。为促进社区发展，我们已将本方案开源，以便进一步扩展以支持其他 RISC-V 扩展。"
  },
  {
    "date": "2025-11-20",
    "title": "Diffusion-Model-Enhanced Layout Pattern Generation for Sub-3nm DFM",
    "authors": "Guanglei Zhou, Chen-Chia Chang, Junyao Zhang, Jingyu Pan, Yiran Chen",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240639",
    "source": "IEEE",
    "abstract": "Modern VLSI layout pattern generation for design for manufacturability (DFM) at sub-3 nm nodes faces two challenges: 1) the rapid evolution of intricate design rules; 2) the scarcity of high-quality, rule-compliant layout data during the development of new process technologies. To address these challenges, we introduce a diffusion-based framework that re-frames complex layout synthesis as a sequence of template-guided inpainting tasks, which significantly reduces training sample requirements for legal pattern generation. This approach leverages the knowledge of a pre-trained image foundation model to generate layout variations that satisfy complex 2D metal interconnect design rule constraints, and introduces a novel template-based denoising scheme to eliminate residual noisy pixels. Through few-shot fine-tuning, our approach uniquely produces legal layouts conforming to a full sign-off rule deck at sub-3nm nodes while delivering superior pattern diversity, offering a production-ready, data-efficient solution for next-generation technology node development.",
    "title_zh": "面向3纳米以下工艺的DFM增强型扩散模型布局模式生成",
    "abstract_zh": "面向3纳米以下节点的现代VLSI布局模式生成在可制造性设计（DFM）方面面临两大挑战：1）复杂设计规则的快速演进；2）在新工艺技术开发阶段高质量、符合规则的布局数据稀缺。为应对这些挑战，我们提出一种基于扩散模型的框架，将复杂的布局合成问题重新建模为一系列由模板引导的图像修复（inpainting）任务，从而显著降低合法布局生成所需的训练样本量。该方法利用预训练的图像基础模型的知识，生成满足复杂二维金属互连设计规则约束的布局变体，并引入一种新颖的基于模板的去噪机制，有效消除残留的噪声像素。通过少量样本微调，我们的方法能够独特地生成符合3纳米以下节点完整签核规则集的合规布局，同时展现出卓越的模式多样性，为下一代技术节点的开发提供了一种可投入生产的、数据高效的解决方案。"
  },
  {
    "date": "2025-11-20",
    "title": "Invited: IEEE DATC RDF-2025: Enabling an EDA Research Ecosystem",
    "authors": "Vidya A. Chhabria, Amur Ghose, Vikram Gopalakrishnan, Andrew B. Kahng, Sayak Kundu, Yiting Liu, Zhiang Wang, Bing-Yue Wu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240743",
    "source": "IEEE",
    "abstract": "Over the past year, IEEE CEDA DATC has continued to improve the DATC Robust Design Flow (RDF) while continuing to expand initiatives that advance open infrastructures and culture changes, serving the global community of EDA researchers and users. This invited paper focuses on three highlights: (1) establishment of an accessible, \"contrib-like\" GitHub resource that provides a more accessible environment for OpenROAD- and OpenROAD-flow-scripts-based research works; (2) the first-ever permission mechanism and benchmarking results for a commercial EDA P&R tool, published with permissions developed with the tool vendor (Siemens EDA); and (3) efforts that support a nascent \"ML EDA Commons\". The paper also provides brief reviews of the past year’s RDF developments and roadmap updates.",
    "title_zh": "受邀：IEEE DATC RDF-2025：构建EDA研究生态系统",
    "abstract_zh": "在过去一年中，IEEE CEDA DATC持续改进DATC稳健设计流程（RDF），同时不断拓展推动开放基础设施和文化变革的举措，致力于服务全球EDA研究者与用户群体。本文重点介绍三大亮点：（1）建立了一个易于访问的“类贡献型”GitHub资源平台，为基于OpenROAD及OpenROAD-flow-scripts的研究工作提供了更加便捷的环境；（2）首次发布了一款商用EDA物理设计工具（由Siemens EDA提供技术支持）的权限机制与基准测试结果，相关成果在获得厂商授权的前提下完成；（3）推动新兴“机器学习EDA公共社区”（ML EDA Commons）建设的努力。此外，本文还简要回顾了过去一年RDF的发展进展，并更新了未来路线图。"
  },
  {
    "date": "2025-11-20",
    "title": "CoXplorer: Multi-Staged Co-Exploration Framework for AI Model Compression and Accelerator Design",
    "authors": "Songchen Ma, Junyi Wu, Yonghao Tan, Pingcheng Dong, Peng Luo, Di Pang, Yu Liu, Xuejiao Liu, Luhong Liang, Kwang-Ting Cheng, Fengbin Tu",
    "publish": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
    "url": "https://doi.org/10.1109/iccad66269.2025.11240731",
    "source": "IEEE",
    "abstract": "The rapid evolution of artificial intelligence (AI) algorithms demands efficient computing chips, positioning algorithm-hardware co-design as a crucial optimization strategy. However, automating the co-design process remains challenging due to the lack of a unified exploration framework for both algorithmic and hardware domains, as existing tools - hardware design space exploration (DSE) and compression neural architecture search (Compression NAS) - operate independently, relying entirely on manual collaboration. This paper presents CoXplorer, a co-exploration framework that connects model-compression optimization space and architecture design space. We make three key contributions: (1) a multi-staged co-design space decomposition method that enables systematic exploration of compression-hardware design choices with reduced complexity, (2) an AC-Copilot toolchain enhanced with multi-grained performance modeling driven by hardware simulation-compilation hierarchical cooperation to fulfill various evaluation requirements of co-exploration, enabling balanced simulation accuracy-efficiency trade-offs, and (3) a co-exploration workflow with hierarchical and bottleneck-guided search for harmonizing optimization objectives of both model and hardware design spaces, resulting in improved search efficiency. We validate the CoXplorer on two edge chips, which achieve 53.7% throughput and 45.8% energy efficiency improvements for the CNN acceleration, and 7.5× speedup with 9.9× energy efficiency boost for the Transformer acceleration. A case study on large language model acceleration shows CoXplorer’s extensibility to emerging workloads, enhancing LLAMA2-7B inference throughput from 6.75 to 25.46 tokens/s via co-optimization with compression and near-memory computing architecture.",
    "title_zh": "CoXplorer：面向AI模型压缩与加速器设计的多阶段协同探索框架",
    "abstract_zh": "人工智能（AI）算法的快速演进对高效计算芯片提出了更高要求，推动了算法-硬件协同设计成为关键的优化策略。然而，由于缺乏统一的算法与硬件领域探索框架，自动化协同设计过程仍面临挑战。现有工具——硬件设计空间探索（DSE）和压缩型神经网络架构搜索（Compression NAS）——各自独立运行，完全依赖人工协作，难以实现高效整合。本文提出 CoXplorer，一个连接模型压缩优化空间与架构设计空间的协同探索框架。我们做出三项关键贡献：（1）一种多阶段协同设计空间分解方法，能够系统性地降低复杂度，实现对压缩与硬件设计选择的全面探索；（2）一个由硬件仿真-编译器分层协作驱动的多粒度性能建模增强型 AC-Copilot 工具链，可满足协同探索中多样化的评估需求，实现模拟精度与效率之间的平衡；（3）一种具有层次化与瓶颈导向特性的协同探索工作流，有效协调模型与硬件设计空间的优化目标，显著提升搜索效率。我们在两款边缘计算芯片上验证了 CoXplorer 的有效性，结果显示在 CNN 加速任务中分别实现了 53.7% 的吞吐量提升和 45.8% 的能效改进；在 Transformer 加速任务中实现 7.5 倍速度提升及 9.9 倍能效增益。针对大语言模型加速的案例研究进一步展示了 CoXplorer 对新兴负载的可扩展性：通过压缩与近内存计算架构的联合优化，LLAMA2-7B 推理吞吐量从 6.75 提升至 25.46 tokens/s。"
  }
]