[
  {
    "date": "2025-11-26",
    "title": "A Study on Metric-Human Alignment in Code Summarization by LLMs through Kolmogorov–Arnold Networks",
    "authors": "Georgian Nicolae, Corneliu Burileanu",
    "publish": "2025 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)",
    "url": "https://doi.org/10.1109/sped67700.2025.11252151",
    "source": "IEEE",
    "abstract": "BLEU, ROUGE, and METEOR are widely used metrics for evaluating code summaries generated by large language models (LLMs), but their correlation with human judgment remains uncertain. In this study, we analyze whether a combination of these metrics can better approximate human evaluation scores. Using multilayer perceptrons (MLPs), Gaussian Process Regression (GPR) and Kolmogorov–Arnold Networks (KANs), we model the relationship between aggregated metric scores and human ratings. While individual metrics show weak alignment with human preferences particularly regarding semantic adequacy and contextual relevance, our results suggest that learned combinations offer modest improvements. Importantly, the interpretability of KANs provides insight into how these metrics interact. Rather than proposing a definitive new metric, this work offers a step toward understanding the limitations of current metrics and motivates the development of more human aligned evaluation methods for code summarization.",
    "title_zh": "通过Kolmogorov–Arnold网络研究大语言模型在代码摘要中的度量与人类对齐问题",
    "abstract_zh": "BLEU、ROUGE 和 METEOR 是评估大语言模型（LLMs）生成代码摘要时广泛使用的指标，但它们与人类判断的相关性仍存疑。在本研究中，我们分析了这些指标的组合是否能更好地逼近人类评价得分。通过多层感知机（MLPs）、高斯过程回归（GPR）以及柯尔莫哥洛夫-阿诺德网络（KANs），我们建模了综合指标得分与人工评分之间的关系。尽管单一指标在语义充分性和上下文相关性等方面与人类偏好存在较弱的对应关系，我们的结果表明，通过学习得到的指标组合能够带来适度的性能提升。尤为重要的是，KAN 模型的可解释性为我们揭示了这些指标之间的交互机制。本文并未提出一个确定性的新指标，而是为理解现有指标的局限性迈出了一步，并推动开发更贴近人类认知的代码摘要评估方法。"
  },
  {
    "date": "2025-11-26",
    "title": "SecLLM: Enhancing Security Smell Detection in IaC with Large Language Models",
    "authors": "Gabriele De Vito, Fabio Palomba, Filomena Ferrucci",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3637505",
    "source": "IEEE",
    "abstract": "The emergence and expansion of Infrastructure-as-Code (IaC) paradigms have transformed cloud provisioning methodologies, while simultaneously introducing complex security smells. Traditional detectors are unable to handle the semantic challenges (i.e., understanding the meaning and context of code beyond its syntax) introduced by contemporary IaC ecosystems due to their reliance on heuristic rules and syntactic patterns. This work introduces SecLLM, an innovative framework that leverages the capabilities of Large Language Models (LLMs) to reconceptualize the detection of security smells through contextual semantic analysis. Our approach avoids conventional intermediate representations, exploiting natural language understanding to examine Ansible, Puppet, and Chef configurations through adaptive prompt engineering methodologies. The framework architecture supports multiple IaC infrastructures and LLMs while simultaneously enabling optimization of the underlying LLM responses for each of the nine security smell analyzed. Furthermore, SecLLM provides detailed feedback regarding the analyzed scripts that can help practitioners define their infrastructure’s risk profile and resolve the identified smells. The empirical validation demonstrates that SecLLM outperforms GLITCH, a state-of-the-art security smell identification tool, achieving precision improvements of 12-21 percentage points and F1-score gains of 17-32 percentage points in terms of detection accuracy while simultaneously maintaining low operational costs ($0.003-$0.015 per script). Additionally, we address the non-deterministic behavior inherent to LLMs through three mitigation strategies (confidence-based filtering, ensemble voting, and response caching) that achieve perfect reproducibility (Fleiss’ k = 1.00) and preserve detection accuracy.",
    "title_zh": "SecLLM：利用大语言模型增强基础设施即代码中的安全异味检测",
    "abstract_zh": "基础设施即代码（IaC）范式的兴起与扩展，彻底改变了云资源的配置方式，同时也引入了复杂的安全缺陷问题。传统检测工具由于依赖启发式规则和语法模式，难以应对现代IaC生态系统带来的语义挑战（即超越语法层面，理解代码的含义与上下文）。本文提出SecLLM——一种创新框架，利用大型语言模型（LLM）的能力，通过上下文语义分析重新定义安全缺陷的检测方法。该方法摒弃了传统的中间表示形式，借助自然语言理解能力，采用自适应提示工程策略，对Ansible、Puppet和Chef等配置文件进行分析。SecLLM的框架架构支持多种IaC平台与LLM，并能针对所分析的九类安全缺陷分别优化底层LLM的响应结果。此外，SecLLM可为用户提供建设性反馈，帮助实践者评估其基础设施的风险状况并修复发现的安全缺陷。\n\n实证验证表明，SecLLM在检测准确率方面显著优于当前最先进的安全缺陷识别工具GLITCH，精度提升达12至21个百分点，F1分数提高17至32个百分点，同时保持极低的运行成本（每脚本仅需0.003至0.015美元）。此外，针对LLM固有的非确定性行为，本文提出了三种缓解策略：基于置信度的过滤、集成投票机制以及响应缓存，成功实现了完全可复现的结果（Fleiss’ k = 1.00），且未牺牲检测准确性。"
  },
  {
    "date": "2025-11-26",
    "title": "Analyzing LLM-generated code according to four ISO/IEC 5055:2021 categories",
    "authors": "Rasmus Krebs, Somnath Mazumdar",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3637569",
    "source": "IEEE",
    "abstract": "The use of large language models (LLMs) for generating code in software development is on the rise. While LLMs demonstrate impressive capabilities, there remains a need to evaluate the quality of generated code beyond functional correctness. This paper addresses a gap in current research by evaluating Python code generated by nine state-of-the-art LLMs according to the four main code quality categories (maintainability, reliability, performance efficiency, and security) defined by the ISO/IEC 5055:2021 standard. The evaluation spans three popular application domains (high-performance computing, machine learning, and data processing). It employs a stratified prompting approach with varying levels (short, medium, and long) of detail. Using widely adopted static code analysis tools, the study identifies which LLMs perform best across these categories. The analysis involved selecting nine algorithms across three application domains, and the generated code was compared against a human developer using four static code analysis tools. Metrics were organized into the four ISO 5055 categories, and composite scores were calculated for each category following preprocessing to ensure accurate evaluation. The results showed that GPT-4-Turbo produced the most reliable, performance-efficient, and secure code, while Gemini excelled in generating maintainable Python code among the evaluated models. The study concludes that properly prompted and configured LLMs can produce code that meets or even exceeds human-developed code in terms of the four ISO categories. Future work will continue to refine and expand upon these methodologies on other programming languages.",
    "title_zh": "根据ISO/IEC 5055:2021的四个类别分析大语言模型生成的代码",
    "abstract_zh": "在软件开发中，使用大型语言模型（LLMs）生成代码的趋势日益增长。尽管LLMs展现出令人印象深刻的能力，但仍需在功能正确性之外评估生成代码的质量。本文填补了当前研究的空白，依据ISO/IEC 5055:2021标准定义的四大代码质量类别——可维护性、可靠性、性能效率和安全性——对九种前沿LLMs生成的Python代码进行了评估。该研究覆盖了三个主流应用领域：高性能计算、机器学习和数据处理，并采用分层提示策略，设置不同详细程度（短、中、长）的提示信息。研究利用广泛使用的静态代码分析工具，识别出在各项质量指标中表现最优的LLM。分析过程选取了三个应用领域中的九个算法，将生成代码与人类开发者编写的代码进行对比，通过四种静态代码分析工具进行评估。各项指标按ISO 5055标准的四个类别进行组织，并在预处理后计算每个类别的综合评分，以确保评估的准确性。结果表明，GPT-4-Turbo生成的代码在可靠性、性能效率和安全性方面表现最佳，而Gemini则在可维护性方面优于其他被评估的模型。研究结论指出，经过恰当提示和配置的LLM能够生成在四项ISO标准维度上达到甚至超过人类开发者水平的代码。未来工作将继续优化并拓展这些方法，应用于其他编程语言。"
  },
  {
    "date": "2025-11-26",
    "title": "Retraction Notice: They use of NLP in Association with AI Technology for the Purpose of Optimization in Various Fields",
    "authors": "Ankita Joshi, Saloni Bansal, Gopal Krishan Gard, Mohammed Al-Farouni, S Srinivasan, Surendra Shetty",
    "publish": "2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)",
    "url": "https://doi.org/10.1109/icacite60783.2024.11269090",
    "source": "IEEE",
    "abstract": "In the world of Natural Language Processing (NLP), developing structures that not simplest carry out nicely but additionally offer apparent reasoning for their selections is paramount. Traditional gadget mastering fashions, in particular the ones based on deep gaining knowledge of, have excelled in diverse NLP duties. However, the ones models frequently act as “black containers,” providing little to no perception into their selection-making processes. This opacity can undermine the don’t forget and reliability perceived with the useful resource of customers. To deal with this trouble, we advocate a unique generative rationalization framework that endeavors to bridge the distance between high overall performance and explainability in NLP systems. This frameworks designed to concurrently make kind picks and generate unique, terrific-grained causes for the ones choices, integrating an explainable difficulty and minimum chance schooling. By providing human-readable justifications for its predictions, our framework enhances the interpretability and trustworthiness of NLP fashions. Experimental validation on two newly curated datasets demonstrates the framework’s capacity to outperform existing baselines, significantly advancing the sector of explainable AI inside NLP.",
    "title_zh": "撤稿声明：自然语言处理与人工智能技术相结合在各领域优化中的应用",
    "abstract_zh": "在自然语言处理（NLP）领域，构建不仅性能出色，而且能够清晰解释其决策过程的系统至关重要。传统的机器学习模型，尤其是基于深度学习的模型，在各种NLP任务中表现优异。然而，这些模型往往如同“黑箱”，难以揭示其决策背后的逻辑，这种不透明性可能削弱用户对其可信度和可靠性的认可。为解决这一问题，我们提出一种全新的生成式解释框架，旨在弥合高性能与可解释性之间的差距。该框架设计用于同时做出判断并生成独特且细粒度的解释，融合了可解释性建模与最小风险训练机制。通过为预测结果提供人类可读的论证依据，我们的框架显著提升了NLP模型的可解释性与可信度。在两个新构建的数据集上的实验验证表明，该框架在性能上超越现有基线方法，显著推动了NLP领域可解释人工智能的发展。"
  },
  {
    "date": "2025-11-26",
    "title": "The Fault in Our Chips: A Survey on Edge AI Accelerator Vulnerabilities",
    "authors": "Shamik Kundu, Sanjay Das, Anand Menon, Swastik Bhattacharya, Arnab Raha, Kanad Basu",
    "publish": "IEEE Design &amp; Test",
    "url": "https://doi.org/10.1109/mdat.2025.3637710",
    "source": "IEEE",
    "abstract": "Edge AI accelerators, such as TPUs, NPUs, and custom low-power ASICs—are increasingly used in real-time, battery-operated systems for on-device deep learning. However, their deployment in resource-constrained and physically exposed environments raises serious concerns about hardware reliability and security. Faults in these systems can be transient (<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i>, due to voltage fluctuations or radiation), permanent (<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i>, aging or manufacturing defects), or even adversarially induced through targeted bit-flip attacks. This survey provides a comprehensive overview of hardware faults in AI accelerators, with a particular focus on edge deployment scenarios.We examine the fault models, detection and mitigation techniques, and unique challenges posed by limited on-chip resources and real-time constraints. Unlike prior surveys focusing mainly on random fault tolerance in general-purpose settings, our work highlights how faults at the edge can also be intentional attack vectors, blurring the line between reliability and security. The goal is to guide the development of resilient edge AI hardware capable of withstanding natural and malicious fault events.",
    "title_zh": "我们的芯片之弊：边缘AI加速器漏洞调查",
    "abstract_zh": "边缘AI加速器（如TPU、NPU以及定制的低功耗ASIC）在需要实时处理且依赖电池供电的设备中日益普及，广泛应用于本地深度学习任务。然而，这些加速器部署在资源受限且物理环境暴露的场景中时，引发了对硬件可靠性和安全性的严重担忧。系统中的故障可能为瞬态的（例如由电压波动或辐射引起）、永久性的（如老化或制造缺陷），甚至可能是通过针对性的位翻转攻击等恶意手段诱发的。本文综述全面探讨了AI加速器中的硬件故障问题，特别聚焦于边缘部署场景。我们分析了各类故障模型、检测与缓解技术，并深入讨论了有限片上资源和实时性约束带来的独特挑战。与以往主要关注通用环境下随机容错的综述不同，本研究强调边缘环境下的故障也可能成为有意的攻击向量，模糊了可靠性与安全性之间的界限。其目标是为开发能够抵御自然及恶意故障事件的鲁棒边缘AI硬件提供指导。"
  },
  {
    "date": "2025-11-26",
    "title": "Retraction Notice: An Enhanced Role of AI in SDLC in Implementing Maintenance System",
    "authors": "Yogendra Kumar, Ch.Raja Kishore Babu, Maha Medha Deorari, Laith Jasim, Komal Sharma, K Vinothkumar",
    "publish": "2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)",
    "url": "https://doi.org/10.1109/icacite60783.2024.11269082",
    "source": "IEEE",
    "abstract": "In the hastily evolving domain of software development, Artificial Intelligence (AI) is a pivotal pressure reworking software checking out from its traditional, exertions-intensive methodologies to greater sophisticated, computerized techniques. This research paper delves into the multifaceted effect of AI on software checking out, elucidating the way it now not only augments the performance and precision of testing however also appreciably reduces the time to market for software products. By integrating AI technology which include gadget mastering, deep studying, and herbal language processing, this study illustrates the ability of AI to revolutionize best assurance practices with the aid of facilitating smarter automated trying out techniques, improving check case generation, and improving fault detection mechanisms. Furthermore, the studies highlight AI’s instrumental function in addressing the complexities of modern software programs, in the end leading to the development of strong, blunders-loose software program solutions. The findings underscore the transformative capability of AI in software program trying out, heralding a brand-new generation of innovation and performance inside the software program development lifecycle.",
    "title_zh": "撤稿声明：人工智能在实施维护系统中的软件开发生命周期中作用的增强",
    "abstract_zh": "在快速演进的软件开发领域，人工智能（AI）正成为推动软件测试变革的关键力量，促使测试从传统的、依赖人力的方法转向更加先进和自动化的技术。本文深入探讨了人工智能对软件测试多方面的影响，阐明其不仅显著提升了测试的效率与准确性，还大幅缩短了软件产品的上市时间。通过整合机器学习、深度学习以及自然语言处理等AI技术，本研究展示了人工智能在革新质量保障实践方面的巨大潜力，具体体现在推动更智能的自动化测试方法、优化测试用例生成，以及增强缺陷检测机制等方面。此外，研究还强调了人工智能在应对现代软件系统复杂性方面的关键作用，最终有助于开发出更加稳健、几乎无错误的软件解决方案。研究结果凸显了人工智能在软件测试领域的变革性潜力，预示着软件开发生命周期将迎来一个创新与效率并重的新时代。"
  },
  {
    "date": "2025-11-26",
    "title": "Retraction Notice: Fully Automated Wheatstone Bridge-Based Transducer Problem Identification with Classifier for Verilog HDL",
    "authors": "Sunila Choudhary",
    "publish": "2023 3rd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)",
    "url": "https://doi.org/10.1109/icacite57410.2023.11269077",
    "source": "IEEE",
    "abstract": "This study describes how artificial neural networks (ANNs) were used to develop automated defect monitoring & diagnosis programs in a type of Wheatstone bridge-based actuators. The results indicate that the proposed fault detection is perfect for incorporating awareness within the devices in a form that is interacting with humans. The automated problem detector that is currently exhibited detects the 7 flaws that are frequently found in a category of such actuators. In this study, a two classifying system built on an ANN is used to teach the recommended automated defect detectors. The railway is identified using Verilog in able to differentiate among the outputs at either 1 of its failures, that is strong (1), and the result at the other 3 defects, who are lower (0).Verilog HDL is used to recognise a subway principles, i.e., mass & prejudices from the neurological connectivity toolkit of MATLAB & feedback voltage levels obtained from OrCAD apps, in sequence to differentiate among the outcome at whatever one of the failings, that is elevated (1), & the outcome at the remainder 3 defects, that also are small (0).",
    "title_zh": "撤稿声明：基于全自动化惠斯通电桥的传感器问题识别，采用分类器实现Verilog HDL设计",
    "abstract_zh": "本研究描述了如何利用人工神经网络（ANN）开发基于惠斯通电桥的执行器中自动缺陷监测与诊断程序。结果表明，所提出的故障检测方法非常适合在设备中融入一种能够与人类交互的智能感知能力。目前展示的自动化问题检测系统能够识别此类执行器中常见的7种缺陷。在本研究中，采用基于ANN的双分类系统来训练推荐的自动化缺陷检测器。通过Verilog实现铁路识别，以区分在任一故障情况下的输出：一种为高电平（1），另一种为其余三种故障情况下的低电平（0）。随后，使用Verilog HDL识别地铁原理，即从MATLAB神经网络连接工具包中获取的“质量”与“偏见”特征，以及从OrCAD软件获得的反馈电压水平，从而区分任一故障情况下的高输出（1）与其余三种故障情况下的低输出（0）。"
  },
  {
    "date": "2025-11-26",
    "title": "AI-Powered Unit Test Generation via Multi-LLM Chaining: A Case Study with GPT-4o, Gemini, and Claude-3.5",
    "authors": "Chandan Kumar, P Usha Sri, P V L Narasimha Naidu, P Bhuvaneswari, Mukesh Prasad, Sunil Kumar Singh",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3637221",
    "source": "IEEE",
    "abstract": "Software testing is a critical activity in the software development cycle since it confirms code correctness, reliability, and maintainability. Unit testing, a cornerstone of software testing, involves verifying the correctness of individual components of a program. Manually writing these tests is a resource-intensive and time-consuming activity. Researchers have proposed various automated test generation methods to reduce the possibility of this problem. Large Language Models (LLMs) have demonstrated remarkable efficacy in automatically generating unit tests in recent years. Although a single LLM configuration can produce a satisfactory response, there are possible risks to the effectiveness of the generated tests, such as the repetition of specific test cases, the lack of edge case coverage, and the omission of full assertions. We present a novel method, LLM Chaining, that uses the collaboration of several LLMs, namely Gemini, GPT-4o, and Claude-3.5 Sonnet, to collaborate and iteratively enhance tests. We started by looking into a single LLM writing unit test, but after seeing how well they performed, we looked into LLM chaining as a way to increase the accuracy and comprehensiveness of the tests that were produced. Gemini and GPT-4o yielded the most reliable results of all the configurations we tested. Under this configuration, the LLM system uses Gemini to generate the initial JUnit tests, then sends the generated results to GPT-4o for assimilation of accuracy and depth. The HumanEval dataset was used to create JUnit test cases for this study. JaCoCo was used to collect coverage statistics, and PIT (Pitest) was used for mutation-based testing to gauge an LLM’s capacity to identify test flaws. With 99.05% branch coverage, 90.48% line coverage from 3508 test runs, and 94.32% mutation coverage, we achieved genuinely impressive results. In contrast, under the same circumstances, Randoop, a well-known automated test generation tool, obtained 68.64% branch coverage and 78.84% line coverage. This demonstrates how multi-step LLM refinement works well to advance automated test generation and, in the end, produce software that is more dependable and maintainable.",
    "title_zh": "基于多大模型链式调用的AI驱动单元测试生成：以GPT-4o、Gemini与Claude-3.5为例的案例研究",
    "abstract_zh": "软件测试是软件开发周期中一项至关重要的活动，它能够验证代码的正确性、可靠性和可维护性。单元测试作为软件测试的核心，旨在验证程序中各个独立组件的正确性。然而，手动编写这些测试既耗费资源又耗时。为此，研究人员提出了多种自动化测试生成方法，以缓解这一问题。近年来，大型语言模型（LLMs）在自动生成单元测试方面展现出卓越的效果。尽管单一LLM配置即可生成令人满意的测试结果，但仍存在一些潜在风险，例如特定测试用例的重复、边缘情况覆盖不足，以及完整断言的缺失。\n\n本文提出了一种新颖的方法——LLM链式协作（LLM Chaining），通过多个大模型（即Gemini、GPT-4o和Claude-3.5 Sonnet）之间的协同工作，实现测试用例的迭代优化与增强。我们最初尝试使用单个LLM生成单元测试，但在观察到其表现后，进一步探索了LLM链式协作机制，以提升生成测试的准确性和全面性。在所有测试配置中，Gemini与GPT-4o组合表现最为稳定可靠。在此配置下，系统首先由Gemini生成初始的JUnit测试用例，随后将生成结果传递给GPT-4o进行准确性与深度的整合与优化。\n\n本研究基于HumanEval数据集生成JUnit测试用例，并采用JaCoCo收集覆盖率统计数据，同时利用PIT（Pitest）进行基于突变的测试，以评估LLM识别测试缺陷的能力。实验结果显示，在3508次测试运行中，取得了99.05%的分支覆盖率、90.48%的行覆盖率以及94.32%的突变覆盖率，成果极为显著。相比之下，在相同条件下，知名自动化测试生成工具Randoop仅达到68.64%的分支覆盖率和78.84%的行覆盖率。\n\n这一结果充分证明，多步骤的LLM精炼机制在推动自动化测试生成方面具有显著优势，最终能够生成更加可靠、可维护性更高的软件产品。"
  },
  {
    "date": "2025-11-26",
    "title": "HybridMoE: LoRA-Based LLMs Fine-Tune With Hybrid Mixture of Experts",
    "authors": "Shengchuan Lin, Yufei Ma, Sen Liu, Junhua Shi, Linbo Jin, Dehong Gao, Shanqing Yu, Qi Xuan, Xiaoyan Cai, Libin Yang",
    "publish": "IEEE Transactions on Audio, Speech and Language Processing",
    "url": "https://doi.org/10.1109/taslpro.2025.3637555",
    "source": "IEEE",
    "abstract": "<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">P</b>arameter-<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">E</b>fficient <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">F</b>ine-<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">T</b>uning (PEFT) of <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</b>arge <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">L</b>anguage <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">M</b>odels (LLMs) in multitask scenarios is cutting-edge research currently. This paper introduces an innovative PEFT approach, named HybridMoE, by applying the <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Lo</b>w-<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">R</b>ank <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</b>daptation (LoRA) network into the <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">M</b>ixture-<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">o</b>f-<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">E</b>xpert (MoE) architecture. Considering the similarity and diversity between multiple tasks, we introduce the Multi-A MoE and Multi-B MoE architectures to allow experts of MoE to share either the decomposition <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$A$</tex-math></inline-formula> matrix or <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$B$</tex-math></inline-formula> matrix among themselves. HybridMoE integrates the Multi-A and Multi-B architectures, and the multiple tasks automatically learn to choose either of them through a gating network. Extensive experiments show that HybridMoE outperforms MoLoRA in performance on the HuggingFace Open LLM benchmark with various open-sourced LLMs, including LLaMA-2, Baichuan2, Qwen, and Yi. By sharing the parameters, HybridMoE requires fewer trainable parameters and is more efficient. The experimental results demonstrate that HybridMoE provides a novel solution for fine-tuning LLMs, striking a balance between performance and resource allocation, and opening up possibilities for broader research directions.",
    "title_zh": "HybridMoE：基于LoRA的LLM微调与混合专家模型",
    "abstract_zh": "参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）在多任务场景下对大型语言模型（Large Language Models, LLMs）的研究目前处于前沿。本文提出了一种创新的PEFT方法——HybridMoE，通过将低秩适配（Low-Rank Adaptation, LoRA）网络引入到混合专家（Mixture-of-Experts, MoE）架构中。考虑到多个任务之间的相似性与差异性，我们设计了Multi-A MoE和Multi-B MoE两种架构，使MoE中的专家能够共享分解矩阵A或B。HybridMoE融合了Multi-A与Multi-B架构，并通过一个门控网络自动学习选择适合各任务的架构。大量实验表明，相较于MoLoRA，在HuggingFace开源LLM基准测试中，HybridMoE在多种开源大模型（包括LLaMA-2、Baichuan2、Qwen和Yi）上均展现出更优的性能表现。由于参数共享机制，HybridMoE所需的可训练参数更少，效率更高。实验结果证明，HybridMoE为大模型微调提供了一种新颖的解决方案，在性能与资源分配之间取得了良好平衡，也为未来更广泛的研究方向开辟了可能性。"
  },
  {
    "date": "2025-11-26",
    "title": "Retraction Notice: Deep Learning in Object Oriented Software Engineering: A Case Of Stability",
    "authors": "Abdullah, Kanta Prasad Sharma Gupta",
    "publish": "2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)",
    "url": "https://doi.org/10.1109/icacite53722.2022.11269050",
    "source": "IEEE",
    "abstract": "The software engineering stages assisted by means of deep learning to know due to the variety of SE responsibilities, it is vital to perceive the present SE duties assisted through deep learning, given that it supports practitioners locate the plausible to leverage deep learning knowledge of in their personal glitches. A Stability estimation framework for OO blueprint maintainability angle has been deliberate on this paper using deep learning. The evolved framework correlates the item orientated blueprint traits with maintainability and additionally correlates maintainability with Stability. No such framework has been current in the literature overview that quantifies software program Stability of OO blueprint through taking maintainability into attention using deep learning. The framework bridges the gap amongst item orientated blueprint traits, maintainability and Stability. Framework measures and improves the maintainability of software program blueprint on the blueprint degree of software program improvement lifestyles cycle, maximum massive to a dependable cease product. Stability and maintainability estimation fashions were proposed through the use of the evolved framework. Stability estimation version has been formulated which takes item orientated blueprint traits into attention and proposed Stability estimation fashions take maintainability in attention for enumerating stability of object oriented blueprint.",
    "title_zh": "撤稿声明：面向对象软件工程中的深度学习：以稳定性为例",
    "abstract_zh": "由于软件工程（SE）职责的多样性，了解深度学习在哪些阶段能够提供支持至关重要。鉴于此，明确当前由深度学习辅助的软件工程任务，有助于从业者识别如何在其自身项目中应用深度学习知识来解决实际问题。本文提出了一种基于深度学习的面向对象设计可维护性评估框架。该框架将面向对象设计特性与可维护性相关联，并进一步将可维护性与系统稳定性相关联。在现有文献综述中，尚未出现通过深度学习方法，综合考虑可维护性来量化面向对象设计稳定性的框架。本框架填补了面向对象设计特性、可维护性与系统稳定性之间的研究空白。该框架在软件开发生命周期的设计阶段对软件设计的可维护性进行度量和改进，从而为最终交付可靠的产品奠定基础。基于所提出的框架，本文还构建了稳定性与可维护性评估模型。其中，稳定性评估模型充分考虑了面向对象设计特征；而所提出的稳定性评估模型则进一步引入可维护性因素，用于估算面向对象设计的稳定性。"
  },
  {
    "date": "2025-11-26",
    "title": "Aims and Scope",
    "authors": "N/A",
    "publish": "2025 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)",
    "url": "https://doi.org/10.1109/sped67700.2025.11252046",
    "source": "IEEE",
    "abstract": null,
    "title_zh": "宗旨与范围",
    "abstract_zh": "None"
  }
]